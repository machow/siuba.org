[
  {
    "objectID": "guide/basics-column-ops.html#lazy-functions",
    "href": "guide/basics-column-ops.html#lazy-functions",
    "title": "Column operations",
    "section": "Lazy functions",
    "text": "Lazy functions"
  },
  {
    "objectID": "guide/basics-column-ops.html#calling-external-functions",
    "href": "guide/basics-column-ops.html#calling-external-functions",
    "title": "Column operations",
    "section": "Calling external functions",
    "text": "Calling external functions\n\nimport pandas as pd\nfrom siuba import _, mutate\nfrom siuba.siu import call\n\nmy_dates = pd.DataFrame({\"date\": [\"2021-01-01\", \"2021-01-02\"]})\n\npd.to_datetime(my_dates.date)\n\n0   2021-01-01\n1   2021-01-02\nName: date, dtype: datetime64[ns]\n\n\n\nmy_dates >> mutate(parsed = _.date) >> _.parsed\n\n0    2021-01-01\n1    2021-01-02\nName: parsed, dtype: object\n\n\n\nmy_dates >> mutate(parsed = call(pd.to_datetime, _.date))\n\n\n\n\n\n  \n    \n      \n      date\n      parsed\n    \n  \n  \n    \n      0\n      2021-01-01\n      2021-01-01\n    \n    \n      1\n      2021-01-02\n      2021-01-02\n    \n  \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou may be familiar with the pd.Series.pipe() method, which could handle the situation using _.date.pipe(...):\n\nmy_dates >> mutate(parsed = _.date.pipe(pd.to_datetime))\n\nBe careful with this approach, since it will work in situations involving pandas DataFrames, but call() works in any situation!"
  },
  {
    "objectID": "guide/basics-examples.html",
    "href": "guide/basics-examples.html",
    "title": "Examples",
    "section": "",
    "text": "This page contains examples of some of the situations siuba really shines in."
  },
  {
    "objectID": "guide/basics-lazy-expressions.html",
    "href": "guide/basics-lazy-expressions.html",
    "title": "Lazy expressions",
    "section": "",
    "text": "A siu expression is a way of specifying what action you want to perform. This allows siuba verbs to decide how to execute the action, depending on whether your data is a local DataFrame or remote table.\nNotice how the output represents each step in our lazy expression, with these pieces:"
  },
  {
    "objectID": "guide/basics-lazy-expressions.html#method-translation",
    "href": "guide/basics-lazy-expressions.html#method-translation",
    "title": "Lazy expressions",
    "section": "Method translation",
    "text": "Method translation\nYou can include method calls like .isin() in a lazy expression.\n\nfrom siuba import _, filter\nfrom siuba.data import mtcars\n\nexpr = _.cyl.isin([2,4])\n\nexpr\n\n‚ñà‚îÄ'__call__'\n‚îú‚îÄ‚ñà‚îÄ.\n‚îÇ ‚îú‚îÄ‚ñà‚îÄ.\n‚îÇ ‚îÇ ‚îú‚îÄ_\n‚îÇ ‚îÇ ‚îî‚îÄ'cyl'\n‚îÇ ‚îî‚îÄ'isin'\n‚îî‚îÄ[2, 4]\n\n\nWhen used in a verb like filter() it will call it over the underlying data. So when you call it on a pandas Series, the Series.isin() method gets called.\n\n# call our expr, which uses .isin\nmtcars >> filter(expr)\n\n# equivalent to...\nmtcars >> filter(_.cyl.isin([2, 4]))\n\n# or in pandas\nmtcars[lambda d: d.cyl.isin([2, 4])]\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      2\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      7\n      24.4\n      4\n      146.7\n      62\n      3.69\n      3.190\n      20.00\n      1\n      0\n      4\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      27\n      30.4\n      4\n      95.1\n      113\n      3.77\n      1.513\n      16.90\n      1\n      1\n      5\n      2\n    \n    \n      31\n      21.4\n      4\n      121.0\n      109\n      4.11\n      2.780\n      18.60\n      1\n      1\n      4\n      2\n    \n  \n\n11 rows √ó 11 columns\n\n\n\nSee the pandas.Series API documentation for detailed documentation on all the different methods available."
  },
  {
    "objectID": "guide/basics-lazy-expressions.html#use-in-pipes",
    "href": "guide/basics-lazy-expressions.html#use-in-pipes",
    "title": "Lazy expressions",
    "section": "Use in pipes",
    "text": "Use in pipes\nSometimes it is helpful to use Pandas DataFrame methods, in addition to siuba verbs. This can be done by piping the data to _.<some_method>().\nHere is an example using the siuba verb count(), with the pandas method .sort_values().\n\nfrom siuba import _, count\nfrom siuba.data import mtcars\n\n(mtcars\n    >> count(_.cyl)         # this is a siuba verb\n    >> _.sort_values(\"n\")   # this is a pandas method\n)\n\n\n\n\n\n  \n    \n      \n      cyl\n      n\n    \n  \n  \n    \n      1\n      6\n      7\n    \n    \n      0\n      4\n      11\n    \n    \n      2\n      8\n      14\n    \n  \n\n\n\n\nHere is another example, using the DataFrame .shape attribute.\n\n\n\n# siuba pipe\nmtcars >> _.shape[0]\n\n32\n\n\n\n# regular pandas\nmtcars.shape[0]"
  },
  {
    "objectID": "guide/basics-lazy-expressions.html#call-external-functions",
    "href": "guide/basics-lazy-expressions.html#call-external-functions",
    "title": "Lazy expressions",
    "section": "Call external functions",
    "text": "Call external functions\n\nimport pandas as pd\nfrom siuba import _, mutate\nfrom siuba.siu import call\n\nmy_dates = pd.DataFrame({\"date\": [\"2021-01-01\", \"2021-01-02\"]})\n\npd.to_datetime(my_dates.date)\n\n0   2021-01-01\n1   2021-01-02\nName: date, dtype: datetime64[ns]\n\n\n\nmy_dates >> mutate(parsed = _.date) >> _.parsed\n\n0    2021-01-01\n1    2021-01-02\nName: parsed, dtype: object\n\n\n\nmy_dates >> mutate(parsed = call(pd.to_datetime, _.date))\n\n\n\n\n\n  \n    \n      \n      date\n      parsed\n    \n  \n  \n    \n      0\n      2021-01-01\n      2021-01-01\n    \n    \n      1\n      2021-01-02\n      2021-01-02"
  },
  {
    "objectID": "guide/basics-lazy-expressions.html#common-challenges",
    "href": "guide/basics-lazy-expressions.html#common-challenges",
    "title": "Lazy expressions",
    "section": "Common challenges",
    "text": "Common challenges\n\nReserved words (_.class)\nMost column names can be referred to using _.some_name syntax. However, python reserved words like class can‚Äôt be used in this way.\nUse indexing (e.g.¬†_[\"some_name\"]) to refer to any column by name.\n\n# bad: raises a SyntaxError\n_.class\n\n# good\n_[\"class\"]\n\nMoreover, pandas reserves names for its methods (e.g.¬†_.shape or _.mean). This is also solved by indexing.\n\ndf = pd.DataFrame({\"mean\": [1,2,3]})\n\n# bad: is accessing the mean method\ndf.mean + 1\n\n# good (pandas)\ndf[\"mean\"]\n\n# good (siuba)\n_[\"mean\"]\n\n\n\nLogical keywords: and, or, in\nIn python libraries like pandas (and numpy), logical comparisons are done using special operators.\nBelow is some example data, along with the operators for logical operations.\n\nimport pandas as pd\n\ndf = pd.DataFrame({\"x\": [2, 3, 4, 5]})\n\n\n\n\npython keyword\npandas\nexample\n\n\n\n\nor\n|\n(df.x < 3) | (df.x > 4)\n\n\nand\n&\n(df.x > 3) & (df.x < 4)\n\n\nin\n.isin()\ndf.x.isin([3, 4, 5])\n\n\n\n\n\nGoogle colab overrides _\nGoogle colab uses very old versions of the library ipykernel, which has a bug in it. This causes it to continuously overwrite the _ variable.\nTo fix this, rename the _ variable imported from siuba.\n\nfrom siuba import _ as D, filter\nfrom siuba.data import mtcars\n\nmtcars >> filter(D.mpg > 30)\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      17\n      32.4\n      4\n      78.7\n      66\n      4.08\n      2.200\n      19.47\n      1\n      1\n      4\n      1\n    \n    \n      18\n      30.4\n      4\n      75.7\n      52\n      4.93\n      1.615\n      18.52\n      1\n      1\n      4\n      2\n    \n    \n      19\n      33.9\n      4\n      71.1\n      65\n      4.22\n      1.835\n      19.90\n      1\n      1\n      4\n      1\n    \n    \n      27\n      30.4\n      4\n      95.1\n      113\n      3.77\n      1.513\n      16.90\n      1\n      1\n      5\n      2"
  },
  {
    "objectID": "guide/basics-sql.html",
    "href": "guide/basics-sql.html",
    "title": "SQL basics",
    "section": "",
    "text": "Up to this point we‚Äôve covered lazy expressions (_), and using table verbs. A major benefit of these two approaches is that they allow us to change how siuba behaves depending on the data source on which it is operating."
  },
  {
    "objectID": "guide/basics-sql.html#setup",
    "href": "guide/basics-sql.html#setup",
    "title": "SQL basics",
    "section": "Setup",
    "text": "Setup\nFor these examples we first set up a sqlite database, with an mtcars table.\n\nfrom sqlalchemy import create_engine\nfrom siuba.sql import LazyTbl\nfrom siuba import _, group_by, summarize, show_query, collect \nfrom siuba.data import mtcars\n\n# copy in to sqlite, using the pandas .to_sql() method\nengine = create_engine(\"sqlite:///:memory:\")\nmtcars.to_sql(\"mtcars\", engine, if_exists = \"replace\")\n\n32"
  },
  {
    "objectID": "guide/basics-sql.html#accessing-tables",
    "href": "guide/basics-sql.html#accessing-tables",
    "title": "SQL basics",
    "section": "Accessing tables",
    "text": "Accessing tables\nUse the LazyTbl class to connect to a SQL table. Printing the table will show a preview of the first few rows.\n\n# Create a lazy SQL DataFrame\ntbl_mtcars = LazyTbl(engine, \"mtcars\")\ntbl_mtcars\n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n  \n    \n      \n      index\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      0\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      1\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      2\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      3\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      4\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n# .. may have more rows\n\n\nNotice that we defined the variable tbl_mtcars to refer to the mtcars table in the database. When we print tbl_mtcars it shows a preview of the underlying data, along with some notes about the database being used: # DB Conn: Engine(sqlite:///:memory:)."
  },
  {
    "objectID": "guide/basics-sql.html#basic-analysis",
    "href": "guide/basics-sql.html#basic-analysis",
    "title": "SQL basics",
    "section": "Basic analysis",
    "text": "Basic analysis\nYou don‚Äôt need to change your analysis code to run it on a SQL table. For example, the code below groups and summarizes the data.\n\n# connect with siuba\n\ntbl_query = (tbl_mtcars\n  >> group_by(_.cyl)\n  >> summarize(avg_hp = _.hp.mean())\n  )\n\ntbl_query\n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n  \n    \n      \n      cyl\n      avg_hp\n    \n  \n  \n    \n      0\n      4\n      82.636364\n    \n    \n      1\n      6\n      122.285714\n    \n    \n      2\n      8\n      209.214286\n    \n  \n\n# .. may have more rows\n\n\nUnder the hood, functions like summarize know how to convert the lazy expressions like _.hp.mean() shown in the code above to SQL."
  },
  {
    "objectID": "guide/basics-sql.html#show-query",
    "href": "guide/basics-sql.html#show-query",
    "title": "SQL basics",
    "section": "Show query",
    "text": "Show query\nBy default, printing out a LazyTbl shows a preview of the data. Use show_query() to see the actual SQL query siuba will generate.\n\nq = tbl_query >> show_query()\n\nSELECT mtcars.cyl, avg(mtcars.hp) AS avg_hp \nFROM mtcars GROUP BY mtcars.cyl"
  },
  {
    "objectID": "guide/basics-sql.html#collect-to-dataframe",
    "href": "guide/basics-sql.html#collect-to-dataframe",
    "title": "SQL basics",
    "section": "Collect to DataFrame",
    "text": "Collect to DataFrame\nUse collect() to fetch the full query results as a pandas DataFrame.\n\ntbl_query >> collect()\n\n\n\n\n\n  \n    \n      \n      cyl\n      avg_hp\n    \n  \n  \n    \n      0\n      4\n      82.636364\n    \n    \n      1\n      6\n      122.285714\n    \n    \n      2\n      8\n      209.214286"
  },
  {
    "objectID": "guide/basics-table-verbs.html",
    "href": "guide/basics-table-verbs.html",
    "title": "Table verbs",
    "section": "",
    "text": "Table verbs take one or more tables as input, and return a table as output."
  },
  {
    "objectID": "guide/basics-table-verbs.html#syntax",
    "href": "guide/basics-table-verbs.html#syntax",
    "title": "Table verbs",
    "section": "Syntax",
    "text": "Syntax\n\n\n\n# preferred: pipe data to verb\nmtcars >> count(_.cyl)\n\n\n\n\n\n  \n    \n      \n      cyl\n      n\n    \n  \n  \n    \n      0\n      4\n      11\n    \n    \n      1\n      6\n      7\n    \n    \n      2\n      8\n      14\n    \n  \n\n\n\n\n\n# call directly\ncount(mtcars, _.cyl)"
  },
  {
    "objectID": "guide/basics-table-verbs.html#verbs-using-tidyselection",
    "href": "guide/basics-table-verbs.html#verbs-using-tidyselection",
    "title": "Table verbs",
    "section": "Verbs using tidyselection",
    "text": "Verbs using tidyselection\nSome verbs‚Äîlike select() for keeping specific columns‚Äîuse a special syntax called tidyselection. This syntax can be thought of as a mini-language for specifying a set of columns, either by inclusion or exclusion.\n\nSiubaPandas\n\n\n\n# keep cyl column\nmtcars >> select(_.cyl)\n\n# keep all *except* cyl column\nmtcars >> select(-_.cyl)\n\n# complex select, plus rename cyl to cylinder\nmtcars >> select(_.cylinder == _.cyl, _.startswith(\"m\"))\n\n\n\n\n# keep cyl column\nmtcars[[\"cyl\"]]\n\n# keep all *except* cyl column\nmtcars.drop([\"cyl\"], axis=1)\n\n# complex select, plus rename cyl to cylinder\ncols = mtcars.columns\nmtcars.loc[:, (cols == \"cyl\") | cols.str.startswith(\"m\")] \\\n      .rename({\"cyl\": \"cylinder\"})\n\n\n\n\nMore options for tidyselection exist, such as matching patterns, or slicing. See the select columns page for a discussion of all tidyselect options."
  },
  {
    "objectID": "guide/basics-verbs-ops-expr.html",
    "href": "guide/basics-verbs-ops-expr.html",
    "title": "Verbs and Column Operations",
    "section": "",
    "text": "Table verbs take one or more tables as input, and return a table as output."
  },
  {
    "objectID": "guide/basics-verbs-ops-expr.html#syntax",
    "href": "guide/basics-verbs-ops-expr.html#syntax",
    "title": "Verbs and Column Operations",
    "section": "Syntax",
    "text": "Syntax\n\n\n\n# preferred: pipe data to verb\nmtcars >> count(_.cyl)\n\n\n\n\n\n  \n    \n      \n      cyl\n      n\n    \n  \n  \n    \n      0\n      4\n      11\n    \n    \n      1\n      6\n      7\n    \n    \n      2\n      8\n      14\n    \n  \n\n\n\n\n\n# call directly\ncount(mtcars, _.cyl)"
  },
  {
    "objectID": "guide/basics-verbs-ops-expr.html#verbs-using-tidyselection",
    "href": "guide/basics-verbs-ops-expr.html#verbs-using-tidyselection",
    "title": "Verbs and Column Operations",
    "section": "Verbs using tidyselection",
    "text": "Verbs using tidyselection\nSome verbs‚Äîlike select() for keeping specific columns‚Äîuse a special syntax called tidyselection. This syntax can be thought of as a mini-language for specifying a set of columns, either by inclusion or exclusion.\n\nSiubaPandas\n\n\n\n# keep cyl column\nmtcars >> select(_.cyl)\n\n# keep all *except* cyl column\nmtcars >> select(-_.cyl)\n\n# complex select, plus rename cyl to cylinder\nmtcars >> select(_.cylinder == _.cyl, _.startswith(\"m\"))\n\n\n\n\n# keep cyl column\nmtcars[[\"cyl\"]]\n\n# keep all *except* cyl column\nmtcars.drop([\"cyl\"], axis=1)\n\n# complex select, plus rename cyl to cylinder\ncols = mtcars.columns\nmtcars.loc[:, (cols == \"cyl\") | cols.str.startswith(\"m\")] \\\n      .rename({\"cyl\": \"cylinder\"})\n\n\n\n\nMore options for tidyselection exist, such as matching patterns, or slicing. See the select columns page for a discussion of all tidyselect options."
  },
  {
    "objectID": "guide/basics-verbs-ops-expr.html#pipe-to-dataframe-methods",
    "href": "guide/basics-verbs-ops-expr.html#pipe-to-dataframe-methods",
    "title": "Verbs and Column Operations",
    "section": "Pipe to DataFrame methods",
    "text": "Pipe to DataFrame methods\nSometimes it is helpful to use Pandas DataFrame methods, in addition to siuba verbs. This can be done by piping the data to _.<some_method>().\nHere is an example using the siuba verb count(), with the pandas method .sort_values().\n\n(mtcars\n    >> count(_.cyl)         # this is a siuba verb\n    >> _.sort_values(\"n\")   # this is a pandas method\n)\n\n\n\n\n\n  \n    \n      \n      cyl\n      n\n    \n  \n  \n    \n      1\n      6\n      7\n    \n    \n      0\n      4\n      11\n    \n    \n      2\n      8\n      14\n    \n  \n\n\n\n\nHere is another example, using the DataFrame .shape attribute.\n\n\n\n# siuba pipe\nmtcars >> _.shape[0]\n\n32\n\n\n\n# regular pandas\nmtcars.shape[0]"
  },
  {
    "objectID": "guide/basics-verbs-ops-expr.html#call-external-functions",
    "href": "guide/basics-verbs-ops-expr.html#call-external-functions",
    "title": "Verbs and Column Operations",
    "section": "Call external functions",
    "text": "Call external functions\nA major advantage of using the pipe approach is that you can pipe any object (e.g.¬†a DataFrame) to any function, using call().\nThe example below pipes to the seaborn‚Äôs barplot function.\n\nfrom siuba.siu import call\nimport seaborn as sns\n\nmtcars >> count(_.cyl) >> call(sns.barplot, x=\"cyl\", y=\"n\", data=_)\n\n<AxesSubplot:xlabel='cyl', ylabel='n'>\n\n\n\n\n\nNote that sns.barplot() expects the data as a named argument, so we pass data=_, where _ is a placeholder for the data.\ncall() can also take a single function to call the data on.\n\n\n\n# piping\nmtcars >> call(len)\n\n32\n\n\n\n# regular function call\nlen(mtcars)\n\n32"
  },
  {
    "objectID": "guide/basics-verbs-ops-expr.html#learning-more",
    "href": "guide/basics-verbs-ops-expr.html#learning-more",
    "title": "Verbs and Column Operations",
    "section": "Learning more",
    "text": "Learning more\n\ncommon table verbs section\ncustom verbs\nflexible pipes"
  },
  {
    "objectID": "guide/core-concepts.html",
    "href": "guide/core-concepts.html",
    "title": "Core concepts",
    "section": "",
    "text": "core\n\nsiu expression\nverb\ncolumn operation\n\ncomposition\n\npipe\nsingledispatch\ntranslator\ndata backend\nnested data\n\nprogramming\n\nacross\nover\n\nextension\n\nverb_dispatch\nop_dispatch"
  },
  {
    "objectID": "guide/extra-r-to-python.html",
    "href": "guide/extra-r-to-python.html",
    "title": "R to Python",
    "section": "",
    "text": "Pandas allows you to slice all strings in a Series, but does not allow you to apply custom slices to each string (a la stringr::str_sub). This means there is no easy equivalent to using results from stringr::str_locate to subset strings.\nWhile most Pandas string methods are under the .str accessor, the ones for ordering are not. To stringr::str_order() and stringr::str_sort(), use .argsort() and .sort_values().\nstringr has an *_all() variant on several functions (e.g.¬†str_replace, str_locate, str_extract, str_match). Pandas generally has equivalent behavior, but it is sometimes specified by using an alternative method (e.g.¬†str.extractall()), and sometimes by using an argument (e.g.¬†str_replace(..., n = 1)).\nPandas string methods are modeled after python str object methods AND stringr (This is mentioned in the .str accessor source code). However, it‚Äôs not always clear what accepts a regex (similar to stringr) and what does not (similr to str object methods).\nFor example, .str.count() only accepts a regex. str.startswith() does not. Other methods like str.contains() accept a regex by default, but this can be disabled using the regex argument.\nThis is not a big issue in practice, but warrants some caution / teaching strategy."
  },
  {
    "objectID": "guide/ops-autocomplete.html",
    "href": "guide/ops-autocomplete.html",
    "title": "Siuba",
    "section": "",
    "text": "import pandas as pd\n\npd.set_option(\"display.max_rows\", 5)\n\nfrom siuba.data import penguins\nfrom siuba import _, summarize, group_by"
  },
  {
    "objectID": "guide/ops-case-when.html",
    "href": "guide/ops-case-when.html",
    "title": "Conditionals (if_else)",
    "section": "",
    "text": "from siuba.data import penguins\nfrom siuba import _, summarize, group_by, if_else, transmute, case_when\n\npenguins\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      male\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      female\n      2007\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      342\n      Chinstrap\n      Dream\n      50.8\n      19.0\n      210.0\n      4100.0\n      male\n      2009\n    \n    \n      343\n      Chinstrap\n      Dream\n      50.2\n      18.7\n      198.0\n      3775.0\n      female\n      2009\n    \n  \n\n344 rows √ó 8 columns"
  },
  {
    "objectID": "guide/ops-case-when.html#if_else-for-two-cases",
    "href": "guide/ops-case-when.html#if_else-for-two-cases",
    "title": "Conditionals (if_else)",
    "section": "if_else for two cases",
    "text": "if_else for two cases\nUse the if_else() when values depend only on two cases‚Äîlike whether some condition is True or False. This is similar to a Python if else statement, but applies to each value in a column.\n\nBasics\n\nif_else(penguins.bill_length_mm > 40, \"long\", \"short\")\n\n0      short\n1      short\n       ...  \n342     long\n343     long\nLength: 344, dtype: object\n\n\n\n\nUse in a verb\n\ntransmute(\n    penguins,\n    bill_length = if_else(_.bill_length_mm > 40, \"long\", \"short\")\n)\n\n\n\n\n\n  \n    \n      \n      bill_length\n    \n  \n  \n    \n      0\n      short\n    \n    \n      1\n      short\n    \n    \n      ...\n      ...\n    \n    \n      342\n      long\n    \n    \n      343\n      long\n    \n  \n\n344 rows √ó 1 columns"
  },
  {
    "objectID": "guide/ops-case-when.html#case_when-for-many-cases",
    "href": "guide/ops-case-when.html#case_when-for-many-cases",
    "title": "Conditionals (if_else)",
    "section": "case_when for many cases",
    "text": "case_when for many cases\nThe case_when() function is a more general version of if_else(). It lets you check as many cases as you want, and map them to resulting values.\n\nBasics\n\ncase_when(penguins, {\n    _.bill_depth_mm <= 18: \"short\",\n    _.bill_depth_mm <= 19: \"medium\",\n    _.bill_depth_mm > 19: \"long\"\n})\n\n0      medium\n1       short\n        ...  \n342    medium\n343    medium\nLength: 344, dtype: object\n\n\n\n\nUse in a verb\n\n# also works\npenguins >> case_when({ ... })\n\n\n\nSet default when no match\nUse a True as the final case, in order to set a value when no other cases match.\n\ncase_when(penguins, {\n    _.bill_depth_mm.between(18, 19): \"medium\",\n    True: \"OTHER\"\n})\n\n0      medium\n1       OTHER\n        ...  \n342    medium\n343    medium\nLength: 344, dtype: object\n\n\nNote that this works because‚Äîfor each value‚Äîcase_when checks for the first matching condition. The final True condition guarantees that it will always be a match."
  },
  {
    "objectID": "guide/ops-categoricals.html",
    "href": "guide/ops-categoricals.html",
    "title": "Categoricals (forcats)",
    "section": "",
    "text": "Categoricals are a way of representing columns of data, that provide:\nWhile codes were historically important for representing large columns of data, the big value of categoricals today is as a tool for customizing order in plots. This might seem like a small job, but as it turns out it is very important in data analysis.\nThis page will discuss the pandas.Categorical class for creating categoricals, as well as a helper submodule siuba.dply.forcats with helper functions for working with this kind of data."
  },
  {
    "objectID": "guide/ops-categoricals.html#required-datasets",
    "href": "guide/ops-categoricals.html#required-datasets",
    "title": "Categoricals (forcats)",
    "section": "Required datasets",
    "text": "Required datasets\nThis page uses the nycflights13 dataset, which can be installed using pip:\npip install nycflights13"
  },
  {
    "objectID": "guide/ops-categoricals.html#overview",
    "href": "guide/ops-categoricals.html#overview",
    "title": "Categoricals (forcats)",
    "section": "Overview",
    "text": "Overview\nHere is a simple pd.Categorical representing a column with 3 values.\n\nx = [\"front\", \"middle\", \"back\"]\n\na_cat = pd.Categorical(x)\na_cat\n\n['front', 'middle', 'back']\nCategories (3, object): ['back', 'front', 'middle']\n\n\nNotice that the bottom line of the print out shows the categories ordered as ['back', 'front', 'middle']. By default, pd.Categorical categories are in (roughly) alphabetical order. Ideally, we‚Äôd have them in an order like front, middle, back!\nOne way to do this, is to use fct_inorder() to order by first observed first, second observed second, etc..\n\nfct_inorder(x)\n\n['front', 'middle', 'back']\nCategories (3, object): ['front', 'middle', 'back']\n\n\nThe remaining sections will focus on two kinds of categorical helper functions:\n\nfunctions for ordering category levels.\nfunctions for grouping categories together.\n\nHowever, before we do that, let‚Äôs go through a few useful ways to interact with categoricals.\n\nCore attributes\n\na_cat.codes\n\narray([1, 2, 0], dtype=int8)\n\n\n\na_cat.categories\n\nIndex(['back', 'front', 'middle'], dtype='object')\n\n\n\na_cat.ordered\n\nFalse\n\n\n\n\nWrapping in pd.Series\npandas often wraps categoricals in a Series object.\n\na_cat2 = pd.Categorical([\"b\", \"a\", \"c\"])\nser = pd.Series(a_cat2)\nser\n\n0    b\n1    a\n2    c\ndtype: category\nCategories (3, object): ['a', 'b', 'c']\n\n\nFor example, any time you create a DataFrame column out of a categorical, it gets wrapped in a pd.Series.\n\ndf = pd.DataFrame({\"some_cat\": a_cat2})\n\nprint(type(df.some_cat))\ndf.some_cat\n\n<class 'pandas.core.series.Series'>\n\n\n0    b\n1    a\n2    c\nName: some_cat, dtype: category\nCategories (3, object): ['a', 'b', 'c']\n\n\n\n\n\n\n\n\nNote\n\n\n\n99% of the time when doing data analysis, your categorical is wrapped in a Series.\n\n\nNote that accessor methods like .str.upper() are available on the series, and the underlying category attributes are availble using the .cat accessor.\n\nser.str.upper()\n\n0    B\n1    A\n2    C\ndtype: object\n\n\n\nser.cat.codes\n\n0    1\n1    0\n2    2\ndtype: int8\n\n\nYou can get the underlying categorical out using the .array property.\n\nser.array\n\n['b', 'a', 'c']\nCategories (3, object): ['a', 'b', 'c']\n\n\n\n\nUsing in verbs\nThe functions in siuba.dplyr.forcats can be used with lazy expressions.\n\nfct_inorder(_.species)\n\n‚ñà‚îÄ'__call__'\n‚îú‚îÄ‚ñà‚îÄ'__custom_func__'\n‚îÇ ‚îî‚îÄ<function fct_inorder at 0x7f3ec9698550>\n‚îî‚îÄ‚ñà‚îÄ.\n  ‚îú‚îÄ_\n  ‚îî‚îÄ'species'\n\n\nNote how the above output is a lazy expression, which can be used inside verbs like mutate():\n\n(penguins\n    >> mutate(\n        species_cat = fct_inorder(_.species),\n        species_cat2 = _.species.astype(\"category\"),\n    )\n)\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n      species_cat\n      species_cat2\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      male\n      2007\n      Adelie\n      Adelie\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      female\n      2007\n      Adelie\n      Adelie\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      342\n      Chinstrap\n      Dream\n      50.8\n      19.0\n      210.0\n      4100.0\n      male\n      2009\n      Chinstrap\n      Chinstrap\n    \n    \n      343\n      Chinstrap\n      Dream\n      50.2\n      18.7\n      198.0\n      3775.0\n      female\n      2009\n      Chinstrap\n      Chinstrap\n    \n  \n\n344 rows √ó 10 columns"
  },
  {
    "objectID": "guide/ops-categoricals.html#order-categories-by-counts-with-fct_infreq",
    "href": "guide/ops-categoricals.html#order-categories-by-counts-with-fct_infreq",
    "title": "Categoricals (forcats)",
    "section": "Order categories by counts with fct_infreq()",
    "text": "Order categories by counts with fct_infreq()\nUse fct_infreq to order category levels by their frequency in the data.\n\nfct_infreq(penguins.species)\n\n0         Adelie\n1         Adelie\n         ...    \n342    Chinstrap\n343    Chinstrap\nLength: 344, dtype: category\nCategories (3, object): ['Adelie', 'Gentoo', 'Chinstrap']\n\n\nIn the output above, the category ordering shows us that ‚ÄúAdelie‚Äù is most frequent in the data, followed by ‚ÄúGentoo‚Äù, and then ‚ÄúChinstrap‚Äù.\nWe can verify this explicitly by using the verb count() to tally up each species.\n\ntbl_species_count = penguins >> count(_.species)\n\ntbl_species_count\n\n\n\n\n\n  \n    \n      \n      species\n      n\n    \n  \n  \n    \n      0\n      Adelie\n      152\n    \n    \n      1\n      Chinstrap\n      68\n    \n    \n      2\n      Gentoo\n      124\n    \n  \n\n\n\n\nOrdering by frequency is helpful for giving viewers a rough sense for which groups have less data in your plots.\nFor example, the code below plots each species on the x-axis, bill_depth_mm on the y-axis. It orders the categories of species by frequency, so those with the most data are shown on the left.\n\nfrom plotnine import ggplot, aes, geom_point, position_jitter\n\n(penguins\n    >> mutate(species = fct_infreq(_.species))\n    >> ggplot(aes(\"species\", \"bill_depth_mm\"))\n    + geom_point(position=position_jitter(width=.1, height=0))\n)\n\n/opt/hostedtoolcache/Python/3.10.6/x64/lib/python3.10/site-packages/plotnine/layer.py:412: PlotnineWarning: geom_point : Removed 2 rows containing missing values.\n\n\n\n\n\n<ggplot: (8744203161576)>\n\n\nNote that the position_jitter(width=.1, height=0) tells the plot to randomly adjust the width of each point between +-.1 (where the distance between each species label is 1)."
  },
  {
    "objectID": "guide/ops-categoricals.html#general-reordering-with-fct_reorder",
    "href": "guide/ops-categoricals.html#general-reordering-with-fct_reorder",
    "title": "Categoricals (forcats)",
    "section": "General reordering with fct_reorder()",
    "text": "General reordering with fct_reorder()\nUse fct_reorder() to reorder the categories of a column, based on another column.\nThis function takes 3 main arguments:\n\nA column to copy and return with reordered categories.\nA column used to calculate the new ordering.\nAn optional function that performs a calculation (defaults to calculating the median).\n\nFor example, the code below reorders the categories of the species column.\n\nfct_reorder(penguins.species, penguins.bill_depth_mm, \"mean\")\n\n0         Adelie\n1         Adelie\n         ...    \n342    Chinstrap\n343    Chinstrap\nLength: 344, dtype: category\nCategories (3, object): ['Gentoo', 'Adelie', 'Chinstrap']\n\n\nNote that it reorders species based on the mean of bill_depth_mm within each category.\n\n\n\n\n\n\nNote\n\n\n\nCurrently, the easiest way to specify a calculation is by passing a string, like \"mean\". Under the hood, fct_reorder() calls pd.Series.agg(), so you could also pass a lambda or function directly.\n\n\n\nBasic example\nThe code below reorders species using the default function (‚Äúmedian‚Äù) over bill_depth_mm. This results in boxplots are ordered from lowest to highest median.\n\nfrom plotnine import ggplot, aes, geom_boxplot\n\n(penguins\n    >> mutate(species = fct_reorder(_.species, _.bill_depth_mm))\n    >> ggplot(aes(\"species\", \"bill_depth_mm\"))\n    + geom_boxplot()\n)\n\n/opt/hostedtoolcache/Python/3.10.6/x64/lib/python3.10/site-packages/plotnine/layer.py:334: PlotnineWarning: stat_boxplot : Removed 2 rows containing non-finite values.\n\n\n\n\n\n<ggplot: (8744200111675)>\n\n\n\n\nUsed with count\nA common use for fct_reorder is to reorder a rolled up count.\nFor example, the code below counts the number of rows per species.\n\ntbl_penguin_species = penguins >> count(_.species)\ntbl_penguin_species\n\n\n\n\n\n  \n    \n      \n      species\n      n\n    \n  \n  \n    \n      0\n      Adelie\n      152\n    \n    \n      1\n      Chinstrap\n      68\n    \n    \n      2\n      Gentoo\n      124\n    \n  \n\n\n\n\nSuppose we had a table like this one, we might want to reorder the categories based on the n column.\n\nfct_reorder(tbl_penguin_species.species, tbl_penguin_species.n, desc=True)\n\n0       Adelie\n1    Chinstrap\n2       Gentoo\ndtype: category\nCategories (3, object): ['Adelie', 'Gentoo', 'Chinstrap']\n\n\nNote that above we used the desc=True argument to put the categories in descending order. Because there is only entry per category level, the default function (‚Äúmedian‚Äù) just returns that value or n. This results in categories ordered by n.\nHere is the same calculation used to reorder the bars on a plot.\n\nfrom plotnine import ggplot, aes, geom_col\n\n(tbl_penguin_species\n    >> mutate(species = fct_reorder(_.species, _.n, desc=True))\n    >> ggplot(aes(\"species\", \"n\"))\n    + geom_col()\n)\n\n\n\n\n<ggplot: (8744203008073)>"
  },
  {
    "objectID": "guide/ops-categoricals.html#binning-categories-with-fct_lump",
    "href": "guide/ops-categoricals.html#binning-categories-with-fct_lump",
    "title": "Categoricals (forcats)",
    "section": "Binning categories with fct_lump()",
    "text": "Binning categories with fct_lump()\nWhile functions like fct_infreq() and fct_reorder() change the order of categories, functions like fct_lump() reduce the number of categories.\nUse fct_lump() to lump categories with fewer observations into a single category (e.g.¬†‚ÄúOther‚Äù).\n\nBasic example\nFor example, let‚Äôs look at the nycflights13 table flights.\n\nfrom nycflights13 import flights\n\nflights\n\n\n\n\n\n  \n    \n      \n      year\n      month\n      day\n      dep_time\n      sched_dep_time\n      dep_delay\n      arr_time\n      sched_arr_time\n      arr_delay\n      carrier\n      flight\n      tailnum\n      origin\n      dest\n      air_time\n      distance\n      hour\n      minute\n      time_hour\n    \n  \n  \n    \n      0\n      2013\n      1\n      1\n      517.0\n      515\n      2.0\n      830.0\n      819\n      11.0\n      UA\n      1545\n      N14228\n      EWR\n      IAH\n      227.0\n      1400\n      5\n      15\n      2013-01-01T10:00:00Z\n    \n    \n      1\n      2013\n      1\n      1\n      533.0\n      529\n      4.0\n      850.0\n      830\n      20.0\n      UA\n      1714\n      N24211\n      LGA\n      IAH\n      227.0\n      1416\n      5\n      29\n      2013-01-01T10:00:00Z\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      336774\n      2013\n      9\n      30\n      NaN\n      1159\n      NaN\n      NaN\n      1344\n      NaN\n      MQ\n      3572\n      N511MQ\n      LGA\n      CLE\n      NaN\n      419\n      11\n      59\n      2013-09-30T15:00:00Z\n    \n    \n      336775\n      2013\n      9\n      30\n      NaN\n      840\n      NaN\n      NaN\n      1020\n      NaN\n      MQ\n      3531\n      N839MQ\n      LGA\n      RDU\n      NaN\n      431\n      8\n      40\n      2013-09-30T12:00:00Z\n    \n  \n\n336776 rows √ó 19 columns\n\n\n\nThis table has a column for carrier that lists each agency running flights. We can use the verb count() to quickly see how many unique carriers there are, and get a feel for how many flights each has run.\n\ntbl_carrier_counts = flights >> count(_.carrier, sort=True)\ntbl_carrier_counts\n\n\n\n\n\n  \n    \n      \n      carrier\n      n\n    \n  \n  \n    \n      0\n      UA\n      58665\n    \n    \n      1\n      B6\n      54635\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      14\n      HA\n      342\n    \n    \n      15\n      OO\n      32\n    \n  \n\n16 rows √ó 2 columns\n\n\n\nNotice two pieces:\n\nThere are 16 rows, so 16 carriers\nThere is a big difference between the number of \"UA\" and \"OO\" flights (58,665 vs 32).\n\nLet‚Äôs use fct_lump() to keep only the 7 biggest carriers, and relable the rest to ‚ÄúOther‚Äù.\n\nfct_lump(tbl_carrier_counts.carrier, w=tbl_carrier_counts.n, n=7)\n\n0        UA\n1        B6\n      ...  \n14    Other\n15    Other\nLength: 16, dtype: category\nCategories (8, object): ['AA', 'B6', 'DL', 'EV', 'MQ', 'UA', 'US', 'Other']\n\n\nIn the code above, we told fct_lump() to lump categories for carrier, weighted by the n column, and resulting in n=7 of the original groups.\nHere‚Äôs an example using the above code to order a barchart.\n\nfrom plotnine import ggplot, aes, geom_col\n\n(tbl_carrier_counts\n    >> mutate(binned = fct_lump(_.carrier, w=_.n, n=7))\n    >> ggplot(aes(\"binned\", \"n\", fill=\"carrier\")) \n    + geom_col()\n)\n\n\n\n\n<ggplot: (8744203008160)>\n\n\nNotice that all of the smaller carriers are grouped into the ‚ÄúOther‚Äù bar.\nThis plot looks okay, but there are two limitations:\n\nThe first bar on the left is ‚ÄúAA‚Äù, but the color legend is in alphabetical order, so starts with ‚Äú9E‚Äù. It would be nice if the legend were in the same order as the bars.\nThe bars themselves are not ordered by frequency.\n\nWe‚Äôll tackle these pieces in the section below.\n\n\nRespecting category order\nfct_lump() preserves existing category order. This enables you to order categories before collapsing them down.\n\nfrom plotnine import ggplot, aes, geom_col\n\n(tbl_carrier_counts\n    >> mutate(carrier = fct_inorder(_.carrier))\n    >> mutate(binned = fct_lump(_.carrier, w=_.n, n=7))\n    >> ggplot(aes(\"binned\", \"n\", fill=\"carrier\")) \n    + geom_col()\n)\n\n\n\n\n<ggplot: (8744200293151)>"
  },
  {
    "objectID": "guide/ops-datetime.html",
    "href": "guide/ops-datetime.html",
    "title": "Datetime operations (.dt) üìù",
    "section": "",
    "text": "Warning\n\n\n\nThis section is in the draft phase."
  },
  {
    "objectID": "guide/ops-datetime.html#overview",
    "href": "guide/ops-datetime.html#overview",
    "title": "Datetime operations (.dt) üìù",
    "section": "Overview",
    "text": "Overview\nThis page covers how to work with dates and times in siuba. siuba works by using pandas methods, either by calling them directly, or translating them to SQL.\n\nimport pandas as pd\n\nfrom siuba import _, count, mutate\nfrom siuba.siu import call\n\n\nCreating datetime columns\nThere are roughly two ways to create a datetime column in pandas:\n\npd.to_datetime() which takes a range of inputs.\npd.Series.astype(\"datetime64[ns]\") method call.\n\nThe pd.to_datetime() function is flexible, and can also take a list of datetimes or a Series.\n\ndt_index = pd.to_datetime([\"2021-01-02 01:02:03\", \"2022-02-03 04:05:06\"])\ndt_index\n\nDatetimeIndex(['2021-01-02 01:02:03', '2022-02-03 04:05:06'], dtype='datetime64[ns]', freq=None)\n\n\nNote that sometimes the result is not a Series. For example, the above object is a DatetimeIndex. Generally, everything is easier after wrapping it in a pandas Series.\n\nser_times = pd.Series(dt_index)\n\nOn the other hand, the .astype() method is a simple way to convert a series to a datetime.\n\npd.Series([\"2021-01-02 03:04:05\"]).astype(\"datetime64[ns]\")\n\n0   2021-01-02 03:04:05\ndtype: datetime64[ns]\n\n\nThe pandas time series docs discuss in exquisite detail the intricacies of different datetime objects, and how they‚Äôre created! The rest of this page will just use pandas Series to look at datetime operations.\n\n\nUsing datetime methods (.dt)\nsiuba uses Pandas methods, so can use any of the datetime methods it makes available, like .dt.month_name().\n\nser_times.dt.month_name()\n\n0     January\n1    February\ndtype: object\n\n\n\n\nUse in a verb\n\ndf = pd.DataFrame({\n    \"times\": ser_times,\n    \"raw\": [\"2023-04-05 06:07:08\", \"2024-05-06 07:08:09\"],\n})\n\n\ndf >> count(month = _.times.dt.month_name())\n\n\n\n\n\n  \n    \n      \n      month\n      n\n    \n  \n  \n    \n      0\n      February\n      1\n    \n    \n      1\n      January\n      1\n    \n  \n\n\n\n\nYou can call functions like pd.to_datetime() using siuba‚Äôs call().\n\nres = df >> mutate(parsed = call(pd.to_datetime, _.raw))\n\nres.parsed\n\n0   2023-04-05 06:07:08\n1   2024-05-06 07:08:09\nName: parsed, dtype: datetime64[ns]\n\n\nNotice that this creates a new datetime column by calling pd.to_datetime(df.raw)."
  },
  {
    "objectID": "guide/ops-datetime.html#change-granularity-with-floor_date",
    "href": "guide/ops-datetime.html#change-granularity-with-floor_date",
    "title": "Datetime operations (.dt) üìù",
    "section": "Change granularity with floor_date()",
    "text": "Change granularity with floor_date()\nSiuba has an experimental function called floor_date() for rounding down to a specific unit of time (e.g.¬†the week, the day, or the hour).\n\nimport pandas as pd\n\nfrom siuba.experimental.datetime import floor_date, ceil_date\n\ndates = pd.to_datetime([\"2021-01-01 01:02:03\", \"2021-01-08 01:02:03\"])\n\n\nBasics\nThe floor_date() functions takes two arguments:\n\nA column to round down (e.g.¬†a pandas Series).\nA datetime unit to round to (e.g.¬†‚ÄúMS‚Äù for ‚ÄúMonth Start‚Äù; see the pandas unit alias doc)\n\nFor example, the code below rounds dates down to the nearest week.\n\nfloor_date(dates, \"W\")\n\nDatetimeIndex(['2021-01-01', '2021-01-08'], dtype='datetime64[ns]', freq=None)\n\n\n\n\nDatetime unit options\nThere are a lot of useful time units, such as ‚ÄúMS‚Äù for the start of a month. Below is a table of some of the most useful ones.\n\n\n\nhuman speak\npandas alias\n\n\n\n\nsecond\nS\n\n\nminute\nM\n\n\nhour\nH\n\n\nday\nD\n\n\nweek\nW\n\n\nmonth\nM\n\n\nbimonth\n2M\n\n\nquarter\nQ\n\n\nseason\n\n\n\nhalfyear\n\n\n\nyear\nY\n\n\n\n\n# month start\nfloor_date(dates, \"MS\")\n\nDatetimeIndex(['2021-01-01', '2021-01-01'], dtype='datetime64[ns]', freq=None)\n\n\n\n\nRound up with ceil_date()\nThe counterpart function ceil_date() rounds up to the specified unit of time.\n\n# round up to month end\nceil_date(dates, \"M\")\n\nDatetimeIndex(['2021-01-31', '2021-01-31'], dtype='datetime64[ns]', freq=None)\n\n\n\n\nPreserving input type\nNote that pandas has multiple formats for representing datetime:\n\nTimestamp: directly representing points in time.\nPeriod: representing time as number of spans of some time unit from a reference point (e.g.¬†120 months from Jan, 1970).\n\n\nx = [\"2021-01-01 01:02:03\", \"2021-02-03 01:02:03\"]\n\ndt_index = pd.DatetimeIndex(x)\nfloor_date(dt_index, \"W\")\n\nDatetimeIndex(['2021-01-01', '2021-02-03'], dtype='datetime64[ns]', freq=None)\n\n\n\n# note freq=\"m\" refers to minute frequency\nper_index = pd.PeriodIndex(x, freq=\"m\")\nfloor_date(per_index, \"W\")\n\nPeriodIndex(['2020-12-28/2021-01-03', '2021-02-01/2021-02-07'], dtype='period[W-SUN]')\n\n\nNote that the \"W\" stands for week.\n\n\nUnit start vs unit end\nNote that the units we discussed here all referred to ‚Äúunit start‚Äù. This is a bit tricky, so might be explained best in an example using month units.\n\nmonth start: ‚Äú2021-02-03‚Äù becomes ‚Äú2021-02-01‚Äù\nmonth end: ‚Äú2021-02-03‚Äù becomes ‚Äú2021-01-31‚Äù\n\nFor most time units pandas has a unit end version, such as:\n\n‚ÄúM‚Äù: month end (vs ‚ÄúMS‚Äù for month start)\n‚ÄúY‚Äù: year end (vs ‚ÄúYS‚Äù for year start)\n‚ÄúQ‚Äù: quarter end (vs ‚ÄúQS‚Äù for quarter start)\n\nIt‚Äôs a bit confusing that ‚ÄúM‚Äù stands for month end, but ‚ÄúD‚Äù stands for day start. In general, time units at the day level or finer grain only do unit start, so be careful with units!"
  },
  {
    "objectID": "guide/ops-datetime.html#translating-to-sql",
    "href": "guide/ops-datetime.html#translating-to-sql",
    "title": "Datetime operations (.dt) üìù",
    "section": "Translating to SQL",
    "text": "Translating to SQL\n\nfrom siuba import _, count\nfrom siuba.data import cars_sql\n\nüöß TODO: add a basic SQL dataset with time columns"
  },
  {
    "objectID": "guide/ops-datetime.html#learning-more",
    "href": "guide/ops-datetime.html#learning-more",
    "title": "Datetime operations (.dt) üìù",
    "section": "Learning more",
    "text": "Learning more\n\nPandas user guide on timeseries\nPandas Series API on .dt accessor methods"
  },
  {
    "objectID": "guide/ops-siu-expr.html",
    "href": "guide/ops-siu-expr.html",
    "title": "Lazy functions",
    "section": "",
    "text": "A siu expression is a way of specifying what action you want to perform. This allows siuba verbs to decide how to execute the action, depending on whether your data is a local DataFrame or remote table.\nNotice how the output represents each step in our lazy expression, with these pieces:"
  },
  {
    "objectID": "guide/ops-siu-expr.html#lazy-functions",
    "href": "guide/ops-siu-expr.html#lazy-functions",
    "title": "Lazy functions",
    "section": "Lazy functions",
    "text": "Lazy functions\n\nfrom siuba import ops\n\nexpr_n = ops.add(_, _)\nexpr_n\n\n‚ñà‚îÄ'__call__'\n‚îú‚îÄ‚ñà‚îÄ'__custom_func__'\n‚îÇ ‚îî‚îÄ<function singledispatch.<locals>.wrapper at 0x7fd01d1bbf40>\n‚îú‚îÄ_\n‚îî‚îÄ_"
  },
  {
    "objectID": "guide/ops-siu-expr.html#lazy-methods",
    "href": "guide/ops-siu-expr.html#lazy-methods",
    "title": "Lazy functions",
    "section": "Lazy methods",
    "text": "Lazy methods\nThe simplest lazy operation is called a method, which\n\nimport operator as op\n\n_.__getitem__(\"a\")\nop.getitem(_, \"a\")\n_[\"a\"]\n\n‚ñà‚îÄ[\n‚îú‚îÄ_\n‚îî‚îÄ‚ñà‚îÄ'__siu_slice__'\n  ‚îî‚îÄ'a'"
  },
  {
    "objectID": "guide/ops-siu-expr.html#as-a-lambda-shorthand",
    "href": "guide/ops-siu-expr.html#as-a-lambda-shorthand",
    "title": "Lazy functions",
    "section": "As a lambda shorthand",
    "text": "As a lambda shorthand\nWe can use siu expressions like lambda functions. For example, to keep specific rows of a pandas DataFrame.\n\nfrom siuba.data import mtcars\n\n# old approach: repeat name\nmtcars[mtcars.cyl == 4]\n\n# old approach: lambda\nmtcars[lambda _: _.cyl == 4]\n\n# siu approach\nmtcars[_.cyl == 4]\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      2\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      7\n      24.4\n      4\n      146.7\n      62\n      3.69\n      3.190\n      20.00\n      1\n      0\n      4\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      27\n      30.4\n      4\n      95.1\n      113\n      3.77\n      1.513\n      16.90\n      1\n      1\n      5\n      2\n    \n    \n      31\n      21.4\n      4\n      121.0\n      109\n      4.11\n      2.780\n      18.60\n      1\n      1\n      4\n      2\n    \n  \n\n11 rows √ó 11 columns"
  },
  {
    "objectID": "guide/ops-strings.html",
    "href": "guide/ops-strings.html",
    "title": "String operations (str) üìù",
    "section": "",
    "text": "String operations allow you to perform actions like:\n\nMatch: detect when a string matches a pattern.\nTransform: e.g.¬†convert something from mIxED to lower case, or replace part of it.\nExtract: grab specific parts of string value (e.g.¬†a matching pattern).\n\nThis page will cover different methods for performing these actions, but will ultimately focus on str.contains(), str.replace(), and str.extract() for common match, transform, and extract tasks.\n\nfrom siuba.data import penguins\nfrom siuba import _, mutate, summarize, group_by, filter\n\nfruits = pd.Series([\n        \"apple\",\n        \"apricot\",\n        \"avocado\",\n        \"banana\",\n        \"bell pepper\"\n])\n\ndf_fruits = pd.DataFrame({\"name\": fruits})\n\n\n\nsiuba uses Pandas methods, so can use any of the string methods it makes available, like .str.upper().\n\nfruits.str.upper()\n\n0          APPLE\n1        APRICOT\n2        AVOCADO\n3         BANANA\n4    BELL PEPPER\ndtype: object\n\n\nNote that most string methods use .str.<method_name>() syntax. These are called ‚Äústring accessor methods‚Äù, since they are accessed from a special place (.str).\n\n\n\nUse string methods as you would any other methods inside verbs.\n\nmutate(df_fruits, loud = _.name.str.upper())\n\n\n\n\n\n  \n    \n      \n      name\n      loud\n    \n  \n  \n    \n      0\n      apple\n      APPLE\n    \n    \n      1\n      apricot\n      APRICOT\n    \n    \n      2\n      avocado\n      AVOCADO\n    \n    \n      3\n      banana\n      BANANA\n    \n    \n      4\n      bell pepper\n      BELL PEPPER"
  },
  {
    "objectID": "guide/ops-strings.html#matching-patterns",
    "href": "guide/ops-strings.html#matching-patterns",
    "title": "String operations (str) üìù",
    "section": "Matching patterns",
    "text": "Matching patterns\n\nFixed text\nThere are three common approaches for simple string matches:\n\nAn exact match with ==.\nA match from an anchor point, using str.startswith() or str.endswith().\nA match from any point, using str.contains()\n\n\n# exact match\nfruits == \"banana\"\n\n# starts with \"ap\"\nfruits.str.startswith(\"ap\")\n\n# ends with \"cado\"\nfruits.str.endswith(\"cado\")\n\n# has an \"e\" anywhere\nfruits.str.contains(\"e\", regex=False)\n\n0     True\n1    False\n2    False\n3    False\n4     True\ndtype: bool\n\n\nAll these operations return a boolean Series, so can be used to filter rows.\n\nfilter(df_fruits, _.name.str.startswith(\"ap\"))\n\n\n\n\n\n  \n    \n      \n      name\n    \n  \n  \n    \n      0\n      apple\n    \n    \n      1\n      apricot\n    \n  \n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that for str.contains() we set the regex=False argument. This is because‚Äîunlike operations like str.startswith()‚Äîpandas by default assumes you are passing something called a regular expression to str.contains().\n\n\n\n\nstr.contains() patterns\nUse str.contains(...) to perform matches with regular expressions‚Äîa special string syntax for specifying patterns to match.\nFor example, you can use \"^\" or \"$\" to match the start or end of a string, respectively.\n\n# check if starts with \"ap\" ----\n\npenguins.species.str.contains(\"^ap\")\n\n0      False\n1      False\n       ...  \n342    False\n343    False\nName: species, Length: 344, dtype: bool\n\n\n\n# check if endswith with \"a\" ----\n\npenguins.species.str.contains(\"a$\")\n\n0      False\n1      False\n       ...  \n342    False\n343    False\nName: species, Length: 344, dtype: bool\n\n\nNote that \"$\" and \"^\" are called anchor points."
  },
  {
    "objectID": "guide/ops-strings.html#transforming-strings",
    "href": "guide/ops-strings.html#transforming-strings",
    "title": "String operations (str) üìù",
    "section": "Transforming strings",
    "text": "Transforming strings\nString transformations take a string and return a new, changed version. For example, by converting all the letters to lower, upper, or title case.\n\nSimple transformations\n\nfruits.str.lower()\n\nfruits.str.upper()\n\n0          APPLE\n1        APRICOT\n2        AVOCADO\n3         BANANA\n4    BELL PEPPER\ndtype: object\n\n\n\n\nstr.replace() patterns\nUse .str.replace(..., regex=True) with regular expressions to replace patterns in strings.\nFor example, the code below uses \"p.\", where . is called a wildcard‚Äìwhich matches any character.\n\nfruits.str.replace(\"p.\", \"XX\", regex=True)\n\n0          aXXle\n1        aXXicot\n2        avocado\n3         banana\n4    bell XXXXer\ndtype: object"
  },
  {
    "objectID": "guide/ops-strings.html#extracting-parts",
    "href": "guide/ops-strings.html#extracting-parts",
    "title": "String operations (str) üìù",
    "section": "Extracting parts",
    "text": "Extracting parts\n\n.str[] to slice\n\n\n\n\n\n\nWarning\n\n\n\nIt is currently not possible to apply a sequence of slices to .str. You can only apply the same slice to every string in the Series.\n\n\n\n\n.str.extract() patterns\nUse str.extract() with a regular expression to pull out a matching piece of text.\nFor example the regular expression ‚Äú^(.*) ‚Äù contains the following pieces:\n\na matches the literal letter ‚Äúa‚Äù\n.* has a . which matches anything, and * which modifies it to apply 0 or more times.\n\n\nfruits.str.extract(\"a(.*)\")\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      0\n      pple\n    \n    \n      1\n      pricot\n    \n    \n      2\n      vocado\n    \n    \n      3\n      nana\n    \n    \n      4\n      NaN"
  },
  {
    "objectID": "guide/ops-strings.html#split-and-flatten",
    "href": "guide/ops-strings.html#split-and-flatten",
    "title": "String operations (str) üìù",
    "section": "Split and flatten",
    "text": "Split and flatten\n\n.str.split() into list-entries\nUse .str.split() to split each entry on a character, producing a list per row of split strings.\n\nfruits.str.split(\"pp\")\n\n0          [a, le]\n1        [apricot]\n2        [avocado]\n3         [banana]\n4    [bell pe, er]\ndtype: object\n\n\nSeeing each entry be a list may surprising, and is fairly rare in pandas.\n\n\n.str.join() is the inverse of split\n\npenguins.species.str.split(\"e\").str.join(\"e\")\n\n0         Adelie\n1         Adelie\n         ...    \n342    Chinstrap\n343    Chinstrap\nName: species, Length: 344, dtype: object\n\n\n\n\n.explode() to unnest entries\nUse .str.explode() to take a column with list-entries (like those returned by .str.split()) and unnest each entry, so there is 1 row per each element in each list.\n\nsplits = fruits.str.split(\"pp\")\nsplits\n\n0          [a, le]\n1        [apricot]\n2        [avocado]\n3         [banana]\n4    [bell pe, er]\ndtype: object\n\n\nNotice that the result above has 4 list-entries (rows). The first and last rows are the splits [\"a\", \"le\"] and [\"bell pe\", \"er\"], so there are 7 elements total.\nThe .explode() method makes each of the 7 elements its own row.\n\nsplits.explode()\n\n0          a\n0         le\n      ...   \n4    bell pe\n4         er\nLength: 7, dtype: object\n\n\nBe careful to note that it‚Äôs .explode() and not .str.explode(), since it can be used on lists of other things as well!\n\n\n.str.findall() for advanced splitting\nFor example, the code below uses \"pp?\", where ? means the preceding character (‚Äúp‚Äù) is optional for matching:\n\nfruits.str.findall(\"pp?\")\n\n0       [pp]\n1        [p]\n2         []\n3         []\n4    [p, pp]\ndtype: object"
  },
  {
    "objectID": "guide/ops-strings.html#templates-with-str_glue",
    "href": "guide/ops-strings.html#templates-with-str_glue",
    "title": "String operations (str) üìù",
    "section": "Templates with str_glue()",
    "text": "Templates with str_glue()\n\n\n\n\n\n\nWarning\n\n\n\nstr_glue() does not exist. This section is ‚ú®aspirational‚ú®."
  },
  {
    "objectID": "guide/ops-strings.html#more-regular-expressions",
    "href": "guide/ops-strings.html#more-regular-expressions",
    "title": "String operations (str) üìù",
    "section": "More regular expressions",
    "text": "More regular expressions\n\nAnchor points\n\n^ - matches the beginning of a string.\n$ - matches the end of a string.\n\n\n\nRepetition qualifiers\n\n* - matches 0 or more\n+ - matches 1 or more\n? - matches 0 or 1\n\n\n\nGrouping\n\n()\n{}\n[]\n\n\n\nAlternatives\n\n|"
  },
  {
    "objectID": "guide/overview.html",
    "href": "guide/overview.html",
    "title": "Overview",
    "section": "",
    "text": "Warning\n\n\n\nThese docs are a work in progress. Note that install directions point to a development branch of siuba.\npip install -e git+https://github.com/machow/siuba.git@dev-docs#egg=siuba\nThese markers are used to indicate how complete a page is:\n\nüöß Undrafted: may not have any content.\nüìù Refining: sections and examples are complete, sometimes narrative is there (with a few spots to fill in).\nSiuba is a tool for concise, flexible data-analysis over multiple data sources. It currently supports pandas DataFrames and SQL tables."
  },
  {
    "objectID": "guide/overview.html#installing",
    "href": "guide/overview.html#installing",
    "title": "Overview",
    "section": "Installing",
    "text": "Installing\npip install -e git+https://github.com/machow/siuba.git@dev-docs#egg=siuba"
  },
  {
    "objectID": "guide/overview.html#basic-use",
    "href": "guide/overview.html#basic-use",
    "title": "Overview",
    "section": "Basic use",
    "text": "Basic use\nThe code below uses the example DataFrame mtcars, to get the average horsepower (hp) per cylinder.\n\nfrom siuba import _, group_by, summarize\nfrom siuba.data import mtcars\n\n(mtcars\n  >> group_by(_.cyl)\n  >> summarize(avg_hp = _.hp.mean())\n  )\n\n\n\n\n\n  \n    \n      \n      cyl\n      avg_hp\n    \n  \n  \n    \n      0\n      4\n      82.636364\n    \n    \n      1\n      6\n      122.285714\n    \n    \n      2\n      8\n      209.214286\n    \n  \n\n\n\n\nThere are three key concepts in this example:\n\n\n\n\n\n\n\n\nconcept\nexample\nmeaning\n\n\n\n\nverb\ngroup_by(...)\na function that operates on a table, like a DataFrame or SQL table\n\n\nlazy expression\n_.hp.mean()\nan expression created with siuba._, that represents actions you want to perform\n\n\npipe\nmtcars >> group_by(...)\na syntax that allows you to chain verbs with the >> operator"
  },
  {
    "objectID": "guide/overview.html#lazy-expressions-_",
    "href": "guide/overview.html#lazy-expressions-_",
    "title": "Overview",
    "section": "Lazy expressions (_)",
    "text": "Lazy expressions (_)\nA siu expression is a way of specifying what action you want to perform. This allows siuba verbs to decide how to execute the action, depending on whether your data is a local DataFrame or remote table.\n\nfrom siuba import _\n\n_.cyl == 4\n\n‚ñà‚îÄ==\n‚îú‚îÄ‚ñà‚îÄ.\n‚îÇ ‚îú‚îÄ_\n‚îÇ ‚îî‚îÄ'cyl'\n‚îî‚îÄ4\n\n\nNotice how the output represents each step in our lazy expression, with these pieces:\n\nblack box ‚ñà - a method like checking equality (==) or getting an attribute (.).\nunderscore (_) - a placeholder for a table of data.\n\nWe can use these expressions like lambda functions. For example, to keep specific rows of a pandas DataFrame.\n\n# old approach: repeat name\nmtcars[mtcars.cyl == 4]\n\n# old approach: lambda\nmtcars[lambda _: _.cyl == 4]\n\n# siu approach\nmtcars[_.cyl == 4]\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      2\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      7\n      24.4\n      4\n      146.7\n      62\n      3.69\n      3.190\n      20.00\n      1\n      0\n      4\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      27\n      30.4\n      4\n      95.1\n      113\n      3.77\n      1.513\n      16.90\n      1\n      1\n      5\n      2\n    \n    \n      31\n      21.4\n      4\n      121.0\n      109\n      4.11\n      2.780\n      18.60\n      1\n      1\n      4\n      2\n    \n  \n\n11 rows √ó 11 columns\n\n\n\nNote that like the lambda function, siuba avoids typing the same (potentially_very_long) name twice, while also being a bit shorter."
  },
  {
    "objectID": "guide/overview.html#table-verbs",
    "href": "guide/overview.html#table-verbs",
    "title": "Overview",
    "section": "Table verbs",
    "text": "Table verbs\nVerbs are functions that operate on a table of data. They can be combined using a pipe with the >> operator.\n\nfrom siuba import _, mutate, filter, group_by, summarize\nfrom siuba.data import mtcars\n\n\nMutate\nThe previous example can be re-written in siuba as the following.\n\n(mtcars\n  >> group_by(_.cyl)\n  >> mutate(demeaned = _.hp - _.hp.mean())\n  )\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n      demeaned\n    \n  \n  \n    \n      0\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n      -12.285714\n    \n    \n      1\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n      -12.285714\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      15.0\n      8\n      301.0\n      335\n      3.54\n      3.570\n      14.60\n      0\n      1\n      5\n      8\n      125.785714\n    \n    \n      31\n      21.4\n      4\n      121.0\n      109\n      4.11\n      2.780\n      18.60\n      1\n      1\n      4\n      2\n      26.363636\n    \n  \n\n32 rows √ó 12 columns\n\n\n\nNote that there is a key difference: mutate returned a pandas DataFrame with the new column (demeaned) at the end. This is a core feature of siuba verbs‚Äìtables in and tables out.\n\n\nFilter\nBelow are examples of keeping certain rows with filter, and calculating a single number per group with summarize.\n\ng_cyl = group_by(mtcars, _.cyl)\n\n# keep lowest hp per group\ng_cyl >> filter(_.hp == _.hp.min())\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      5\n      18.1\n      6\n      225.0\n      105\n      2.76\n      3.460\n      20.22\n      1\n      0\n      3\n      1\n    \n    \n      18\n      30.4\n      4\n      75.7\n      52\n      4.93\n      1.615\n      18.52\n      1\n      1\n      4\n      2\n    \n    \n      21\n      15.5\n      8\n      318.0\n      150\n      2.76\n      3.520\n      16.87\n      0\n      0\n      3\n      2\n    \n    \n      22\n      15.2\n      8\n      304.0\n      150\n      3.15\n      3.435\n      17.30\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n\nSummarize\n\ng_cyl >> summarize(avg_hp = _.hp.mean())\n\n\n\n\n\n  \n    \n      \n      cyl\n      avg_hp\n    \n  \n  \n    \n      0\n      4\n      82.636364\n    \n    \n      1\n      6\n      122.285714\n    \n    \n      2\n      8\n      209.214286"
  },
  {
    "objectID": "guide/overview.html#column-operations",
    "href": "guide/overview.html#column-operations",
    "title": "Overview",
    "section": "Column operations",
    "text": "Column operations\nThe verbs above received a few different calculations as arguments:\n\n_.hp.mean()\n_.hp.min()\n\nYou can use any methods from the underlying pandas objects as methods.\n\n# outside\nmtcars.shape[0] + 1\n\n# inside mutate\nmtcars >> mutate(res = _.shape[0] + 1)\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n      res\n    \n  \n  \n    \n      0\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n      33\n    \n    \n      1\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n      33\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      15.0\n      8\n      301.0\n      335\n      3.54\n      3.570\n      14.60\n      0\n      1\n      5\n      8\n      33\n    \n    \n      31\n      21.4\n      4\n      121.0\n      109\n      4.11\n      2.780\n      18.60\n      1\n      1\n      4\n      2\n      33\n    \n  \n\n32 rows √ó 12 columns\n\n\n\nThis includes the str and dt attribute accessor methods:\n\nimport pandas as pd\n\ndf = pd.DataFrame({\"x\": [\"apple\", \"banana\"]})\n\n# outside\ndf.x.str.contains(\"a\")\n\n# inside mutate\ndf >> mutate(res = _.x.str.contains(\"a\"))\n\n\n\n\n\n  \n    \n      \n      x\n      res\n    \n  \n  \n    \n      0\n      apple\n      True\n    \n    \n      1\n      banana\n      True"
  },
  {
    "objectID": "guide/overview.html#using-with-plotnine",
    "href": "guide/overview.html#using-with-plotnine",
    "title": "Overview",
    "section": "Using with plotnine",
    "text": "Using with plotnine\nFortnuately, plotnine supports siuba‚Äôs style of piping, so is easy to plug in to!\n\nfrom siuba import mutate, _\nfrom plotnine import ggplot, aes, geom_point\n\n(mtcars\n  >> mutate(hp_per_cyl = _.hp / _.cyl)\n  >> ggplot(aes(\"cyl\", \"hp_per_cyl\"))\n   + geom_point()\n)\n\n\n\n\n<ggplot: (8770583005803)>"
  },
  {
    "objectID": "guide/overview.html#next-steps",
    "href": "guide/overview.html#next-steps",
    "title": "Overview",
    "section": "Next steps",
    "text": "Next steps\nTODO"
  },
  {
    "objectID": "guide/programming-adv-sql.html",
    "href": "guide/programming-adv-sql.html",
    "title": "Advanced SQL üöß",
    "section": "",
    "text": "Single code-chunk review of basics: show_query, collect, preview\nLazyTbl from query string (in dbcooper currently?)\nsimplify query (show_query(simplify=True))\nsiuba.sql.sql_raw\nLazyTbl core properties\n\nlast_op\n\nCustom over clauses\nTranslate function"
  },
  {
    "objectID": "guide/programming-new-ops.html",
    "href": "guide/programming-new-ops.html",
    "title": "Custom column ops üìù",
    "section": "",
    "text": "Use symbolic_dispatch() to create new functions for operating on columns.\nThis function creates what are called single generic functions‚Äî which let you register different ways to handle different types of data."
  },
  {
    "objectID": "guide/programming-new-ops.html#define-a-new-function",
    "href": "guide/programming-new-ops.html#define-a-new-function",
    "title": "Custom column ops üìù",
    "section": "Define a new function",
    "text": "Define a new function\n\nfrom siuba.siu import symbolic_dispatch\nfrom pandas.core.groupby import SeriesGroupBy\nfrom pandas import Series\n\n@symbolic_dispatch\ndef like(col, text):\n    raise NotImplementedError(f\"Not implemented for class {type(col)}\")\n\n@like.register\ndef _like_ser(col: Series, text: str) -> Series:\n    \"\"\"Return transformation. Checks whether text is in each entry of col.\"\"\"\n    return col.str.contains(text)"
  },
  {
    "objectID": "guide/programming-new-ops.html#check-for-a-translation",
    "href": "guide/programming-new-ops.html#check-for-a-translation",
    "title": "Custom column ops üìù",
    "section": "Check for a translation",
    "text": "Check for a translation\n\nlike.dispatch(Series)\n\n<function __main__._like_ser(col: pandas.core.series.Series, text: str) -> pandas.core.series.Series>"
  },
  {
    "objectID": "guide/programming-new-ops.html#register-an-error-with-functionlookupbound",
    "href": "guide/programming-new-ops.html#register-an-error-with-functionlookupbound",
    "title": "Custom column ops üìù",
    "section": "Register an error with FunctionLookupBound",
    "text": "Register an error with FunctionLookupBound\n\nfrom siuba.siu import FunctionLookupBound\n\n@symbolic_dispatch\ndef some_func(x):\n    return 1\n\nsome_func.register(Series, FunctionLookupBound(\"Not implemented\"))\n\n<siuba.siu.visitors.FunctionLookupBound at 0x7f7b519c5360>\n\n\n\nf_concrete = some_func.dispatch(Series)\n\n# indicates that a function is *not* implemented\nisinstance(f_concrete, FunctionLookupBound)\n\nTrue"
  },
  {
    "objectID": "guide/programming-new-ops.html#registering-sql-translation",
    "href": "guide/programming-new-ops.html#registering-sql-translation",
    "title": "Custom column ops üìù",
    "section": "Registering SQL translation",
    "text": "Registering SQL translation\n\nfrom sqlalchemy import sql\n\nfrom siuba.sql.dialects.postgresql import PostgresqlColumn, PostgresqlColumnAgg\n\n@like.register\ndef _like_pg(codata: PostgresqlColumn, col: sql.ClauseElement, text: str) -> sql.ClauseElement:\n    return col.like(text)\n\n\nexpr = like(PostgresqlColumn(), sql.column(\"a\"), \"yo\")\nprint(expr)\n\na LIKE :a_1"
  },
  {
    "objectID": "guide/programming-new-ops.html#register-a-sql-window-function",
    "href": "guide/programming-new-ops.html#register-a-sql-window-function",
    "title": "Custom column ops üìù",
    "section": "Register a SQL window function",
    "text": "Register a SQL window function"
  },
  {
    "objectID": "guide/programming-new-ops.html#extending-existing-column-op",
    "href": "guide/programming-new-ops.html#extending-existing-column-op",
    "title": "Custom column ops üìù",
    "section": "Extending existing column op",
    "text": "Extending existing column op"
  },
  {
    "objectID": "guide/programming-new-verbs.html",
    "href": "guide/programming-new-verbs.html",
    "title": "Custom table verbs üöß",
    "section": "",
    "text": "You may be wondering how a siuba function, like mutate(), could work on a SQL database. This is because these functions are defined using a technique called single dispatch. This approach allows you to define class-specific versions of a function."
  },
  {
    "objectID": "guide/programming-new-verbs.html#create-a-new-verb-with-verb_dispatch",
    "href": "guide/programming-new-verbs.html#create-a-new-verb-with-verb_dispatch",
    "title": "Custom table verbs üöß",
    "section": "Create a new verb with verb_dispatch()",
    "text": "Create a new verb with verb_dispatch()\nUse the verb_dispatch() decorator to create a new table verb function.\nThe code below creates a function called head(), with an implementation that works specifically on a DataFrame.\n\n# DataFrame version of function ---\n\n@verb_dispatch(pd.DataFrame)\ndef head(__data, n = 5):\n    return __data.head(n)\n\nhead(mtcars, 2)\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      21.0\n      6\n      160.0\n      110\n      3.9\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      21.0\n      6\n      160.0\n      110\n      3.9\n      2.875\n      17.02\n      0\n      1\n      4\n      4"
  },
  {
    "objectID": "guide/programming-new-verbs.html#register-a-sql-translation",
    "href": "guide/programming-new-verbs.html#register-a-sql-translation",
    "title": "Custom table verbs üöß",
    "section": "Register a SQL translation",
    "text": "Register a SQL translation\nWe can define a SQL specific version, that acts on a SqlAlchemy Table by registering a new translation of our function head, with the @head.register decorator.\n\nfrom sqlalchemy import Table, Column, MetaData\n\n@head.register\ndef _head_sql(__data: Table, n = 5):\n    return __data.select().limit(n)\n\ntable = Table(\"some_table\", MetaData(), Column('a'), Column('b'))\nprint(\n    head(table, 2)\n)\n\nSELECT some_table.a, some_table.b \nFROM some_table\n LIMIT :param_1"
  },
  {
    "objectID": "guide/programming-new-verbs.html#why-not-use-methods",
    "href": "guide/programming-new-verbs.html#why-not-use-methods",
    "title": "Custom table verbs üöß",
    "section": "Why not use methods?",
    "text": "Why not use methods?\nWhy use singledispatch rather than create a class method like mtcars.head()?\nThere are two big benefits:\n\nAnyone can define and package a function. Using it is just a matter of importing it. With a method, you need to somehow put it onto the class representing your data. You end up with 300+ methods on a class.\nYour function might do something that is not the class‚Äôs core responsibility. In this case, it should not be part of the class definition.\n\nSee the post Single dispatch for democratizing data science tools for more."
  },
  {
    "objectID": "guide/programming-new-verbs.html#grouped-data",
    "href": "guide/programming-new-verbs.html#grouped-data",
    "title": "Custom table verbs üöß",
    "section": "Grouped data",
    "text": "Grouped data\nSince single dispatch functions define how to execute an action for a specific class of data, it allows siuba to handle grouped data in two ways:\n\npandas - register dispatchers for its special grouped data classes (DataFrameGroupBy, SeriesGroupBy).\nSQL - use a single class for grouped and ungrouped data, with grouping info as an attribute (siuba.sql.LazyTbl).\n\nFor example, here is a simple verb that calculates the number of rows in a grouped DataFrame.\n\nfrom pandas.core.groupby import DataFrameGroupBy\n@verb_dispatch(DataFrameGroupBy)\ndef size(__data):\n    return __data.size()\nsize(mtcars.groupby('cyl'))\n\ncyl\n4    11\n6     7\n8    14\ndtype: int64"
  },
  {
    "objectID": "guide/programming-new-verbs.html#handling-indexes",
    "href": "guide/programming-new-verbs.html#handling-indexes",
    "title": "Custom table verbs üöß",
    "section": "Handling indexes",
    "text": "Handling indexes\nMost siuba table verbs take a DataFrame, and return a DataFrame. Moreover, they don‚Äôt stick columns onto the index. This means you don‚Äôt need to call reset_index all the time.\nA common place where reset_index is called is after a pandas grouped aggregation.\n\nfrom siuba.data import mtcars\nfrom siuba import summarize\ng_cyl = mtcars.groupby(\"cyl\")\nagg_res = g_cyl[[\"hp\", \"mpg\"]].agg(\"mean\")\n# nooooo\nagg_res\n\n\n\n\n\n  \n    \n      \n      hp\n      mpg\n    \n    \n      cyl\n      \n      \n    \n  \n  \n    \n      4\n      82.636364\n      26.663636\n    \n    \n      6\n      122.285714\n      19.742857\n    \n    \n      8\n      209.214286\n      15.100000"
  },
  {
    "objectID": "guide/programming-new-verbs.html#verb_dispatch",
    "href": "guide/programming-new-verbs.html#verb_dispatch",
    "title": "Custom table verbs üöß",
    "section": "verb_dispatch()",
    "text": "verb_dispatch()\nOne thing to note is that siuba‚Äôs singledispatch implementation is called singledispatch2. This function (whose name will likely change!) is a very light wrapper around python‚Äôs built in functools.singledispatch that does two things:\n\nAllow verbs to be piped using data >> verb1() >> verb2() syntax.\nStrip out the symbolic part of lazy expressions.\n\nThese two concepts are covered in the next two sections."
  },
  {
    "objectID": "guide/programming-pipes.html",
    "href": "guide/programming-pipes.html",
    "title": "Flexible pipes üìù",
    "section": "",
    "text": "A commonly used feature of siuba is the >> operator for piping the result of one verb into another. This feature may seem silly compared to method chaining. However, it makes it easy for other packages to contribute new verbs, and lazy pipes can be combined together.\nThis page will focus on three pieces of piping:"
  },
  {
    "objectID": "guide/programming-pipes.html#basic-pipe",
    "href": "guide/programming-pipes.html#basic-pipe",
    "title": "Flexible pipes üìù",
    "section": "Basic pipe",
    "text": "Basic pipe\nUse the >> operator to pass the result of one verb into another.\n\n(mtcars\n    >> group_by(_.cyl)\n    >> summarize(res = _.hp.mean())\n)\n\n\n\n\n\n  \n    \n      \n      cyl\n      res\n    \n  \n  \n    \n      0\n      4\n      82.636364\n    \n    \n      1\n      6\n      122.285714\n    \n    \n      2\n      8\n      209.214286\n    \n  \n\n\n\n\nNote that this is equivalent to the code below, but much more readable.\n\nsummarize(\n    group_by(\n        mtcars,\n        _.cyl\n    ),\n    res = _.hp.mean()\n)\n\n\n\n\n\n  \n    \n      \n      cyl\n      res\n    \n  \n  \n    \n      0\n      4\n      82.636364\n    \n    \n      1\n      6\n      122.285714\n    \n    \n      2\n      8\n      209.214286"
  },
  {
    "objectID": "guide/programming-pipes.html#lazy-pipes",
    "href": "guide/programming-pipes.html#lazy-pipes",
    "title": "Flexible pipes üìù",
    "section": "Lazy pipes",
    "text": "Lazy pipes\nStart a pipe with _ to create a lazy pipes. Lazy pipes are a lot like functions‚Äîthey run once data is passed to them.\n\nfilter_cyl = _ >> filter(_.cyl.isin([4, 8]))\n\nmtcars >> filter_cyl >> head()\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      2\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.32\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      4\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.44\n      17.02\n      0\n      0\n      3\n      2\n    \n    \n      6\n      14.3\n      8\n      360.0\n      245\n      3.21\n      3.57\n      15.84\n      0\n      0\n      3\n      4\n    \n    \n      7\n      24.4\n      4\n      146.7\n      62\n      3.69\n      3.19\n      20.00\n      1\n      0\n      4\n      2\n    \n    \n      8\n      22.8\n      4\n      140.8\n      95\n      3.92\n      3.15\n      22.90\n      1\n      0\n      4\n      2\n    \n  \n\n\n\n\nMultiple lazy pipes can be combined in a pipe.\n\nhead_5 = _ >> head(n=5)\n\nmtcars >> filter_cyl >> head_5\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      2\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.32\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      4\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.44\n      17.02\n      0\n      0\n      3\n      2\n    \n    \n      6\n      14.3\n      8\n      360.0\n      245\n      3.21\n      3.57\n      15.84\n      0\n      0\n      3\n      4\n    \n    \n      7\n      24.4\n      4\n      146.7\n      62\n      3.69\n      3.19\n      20.00\n      1\n      0\n      4\n      2\n    \n    \n      8\n      22.8\n      4\n      140.8\n      95\n      3.92\n      3.15\n      22.90\n      1\n      0\n      4\n      2\n    \n  \n\n\n\n\nThis allows them to work as building blocks during an analysis!"
  },
  {
    "objectID": "guide/programming-pipes.html#how-do-verbs-work",
    "href": "guide/programming-pipes.html#how-do-verbs-work",
    "title": "Flexible pipes üìù",
    "section": "How do verbs work?",
    "text": "How do verbs work?\nThe key to using verbs in a pipe is they have two modes, depending on what they receive as their first argument:\n\nverb(DataFrame, ...): execute the verb right away.\nverb(...): delay execution, and return a Call, which can be used in a pipe.\n\nFor example, here is a summarize being executed directly.\n\nsummarize(mtcars, avg = _.mpg.mean())\n\n\n\n\n\n  \n    \n      \n      avg\n    \n  \n  \n    \n      0\n      20.090625\n    \n  \n\n\n\n\nHere is an example of a group_by call that could be used in a pipe.\n\ngroup_cyl = group_by(_.cyl)\n\ntype(group_cyl)\n\nsiuba.siu.calls.Call\n\n\n\nmtcars >> group_cyl >> summarize(res = _.hp.mean())\n\n\n\n\n\n  \n    \n      \n      cyl\n      res\n    \n  \n  \n    \n      0\n      4\n      82.636364\n    \n    \n      1\n      6\n      122.285714\n    \n    \n      2\n      8\n      209.214286\n    \n  \n\n\n\n\n\nExplicit use of verbs in a pipe\nUse _ as the first argument to a verb, in order to return a Call.\n\n# eagerly evaluated ----\ngroup_by(mtcars, _.cyl)\n\n# lazy: both of these can be used in a pipe ----\n\n# implicit\ngroup_by(_.cyl)\n\n# explicit\ngroup_by(_, _.cyl)\n\n<function group_by at 0x7f1957e59fc0>(_,_.cyl())\n\n\nThis is much more explicit, but also a bit more clunky looking."
  },
  {
    "objectID": "guide/programming-pipes.html#call-two-table-verbs",
    "href": "guide/programming-pipes.html#call-two-table-verbs",
    "title": "Flexible pipes üìù",
    "section": "Call two-table verbs",
    "text": "Call two-table verbs\nSome verbs take two tables of data. For example, inner_join() merges two tables of data based on some condition matching them up.\nFor two-table verbs, use _ as the first argument, to indicate it is being used in a pipe.\n\ntbl_labels = pd.DataFrame({\"cyl\": [4, 6, 8], \"label\": [\"four\", \"six\", \"eight\"]})\n\n# executed right away\ninner_join(mtcars, tbl_labels, \"cyl\")\n\n# piping approach\nmtcars >> inner_join(_, tbl_labels, \"cyl\")\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n      label\n    \n  \n  \n    \n      0\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n      six\n    \n    \n      1\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n      six\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      15.8\n      8\n      351.0\n      264\n      4.22\n      3.170\n      14.50\n      0\n      1\n      5\n      4\n      eight\n    \n    \n      31\n      15.0\n      8\n      301.0\n      335\n      3.54\n      3.570\n      14.60\n      0\n      1\n      5\n      8\n      eight\n    \n  \n\n32 rows √ó 12 columns"
  },
  {
    "objectID": "guide/programming-pipes.html#call-dataframe-methods",
    "href": "guide/programming-pipes.html#call-dataframe-methods",
    "title": "Flexible pipes üìù",
    "section": "Call DataFrame methods",
    "text": "Call DataFrame methods\nSometimes it is helpful to use Pandas DataFrame methods, in addition to siuba verbs. This can be done by piping the data to _.<some_method>().\nHere is an example using the siuba verb count(), with the pandas method .sort_values().\n\nfrom siuba import _, count\nfrom siuba.data import mtcars\n\n(mtcars\n    >> count(_.cyl)         # this is a siuba verb\n    >> _.sort_values(\"n\")   # this is a pandas method\n)\n\n\n\n\n\n  \n    \n      \n      cyl\n      n\n    \n  \n  \n    \n      1\n      6\n      7\n    \n    \n      0\n      4\n      11\n    \n    \n      2\n      8\n      14\n    \n  \n\n\n\n\nHere is another example, using the DataFrame .shape attribute.\n\n\n\n# siuba pipe\nmtcars >> _.shape[0]\n\n32\n\n\n\n# regular pandas\nmtcars.shape[0]"
  },
  {
    "objectID": "guide/programming-pipes.html#call-external-functions",
    "href": "guide/programming-pipes.html#call-external-functions",
    "title": "Flexible pipes üìù",
    "section": "Call external functions",
    "text": "Call external functions\nUse call() to pipe data into any function call.\nThe example below pipes to the seaborn‚Äôs barplot function.\n\nfrom siuba.siu import call\nimport seaborn as sns\n\n(mtcars\n    >> count(_.cyl)\n    >> call(sns.barplot, x=\"cyl\", y=\"n\", data=_)\n)\n\n<AxesSubplot:xlabel='cyl', ylabel='n'>\n\n\n\n\n\nNote that sns.barplot() expects the data as a named argument, so we pass data=_, where _ is a placeholder for the data.\ncall() can also take a single function to call the data on.\n\n\n\n# piping\nmtcars >> call(len)\n\n32\n\n\n\n# regular function call\nlen(mtcars)\n\n32"
  },
  {
    "objectID": "guide/programming-pipes.html#pipe-as-an-alternative-to",
    "href": "guide/programming-pipes.html#pipe-as-an-alternative-to",
    "title": "Flexible pipes üìù",
    "section": "pipe() as an alternative to >>",
    "text": "pipe() as an alternative to >>\n\n\n\n(\n    mtcars\n    >> group_by(_.cyl, _.gear)\n    >> summarize(res = _.hp.mean())\n    >> call(print, \"Printed output -\\n\", _)\n)\n\nPrinted output -\n     cyl  gear         res\n0     4     3   97.000000\n1     4     4   76.000000\n..  ...   ...         ...\n6     8     3  194.166667\n7     8     5  299.500000\n\n[8 rows x 3 columns]\n\n\n\npipe(\n    mtcars\n    , group_by(_.cyl, _.gear)\n    , summarize(res = _.hp.mean())\n    , call(print, \"Printed output -\\n\", _)\n)"
  },
  {
    "objectID": "guide/verb-arrange.html",
    "href": "guide/verb-arrange.html",
    "title": "Arrange rows",
    "section": "",
    "text": "choosing columns to arrange by\nspecifying an order (ascending or descending)\n\nBelow, we‚Äôll illustrate this function with a single variable, multiple variables, and more general expressions.\n\nfrom siuba import _, arrange, select\nfrom siuba.data import mtcars\n\nsmall_mtcars = mtcars >> select(_.cyl, _.mpg, _.hp)\n\nsmall_mtcars\n\n\n\n\n\n  \n    \n      \n      cyl\n      mpg\n      hp\n    \n  \n  \n    \n      0\n      6\n      21.0\n      110\n    \n    \n      1\n      6\n      21.0\n      110\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      8\n      15.0\n      335\n    \n    \n      31\n      4\n      21.4\n      109\n    \n  \n\n32 rows √ó 3 columns\n\n\n\n\nBasics\nThe simplest way to use arrange is to specify a column name. The arrange function uses pandas.sort_values under the hood, and arranges rows in ascending order.\nFor example, the code below arranges the rows from least to greatest horsepower (hp).\n\n# simple arrange of 1 var\nsmall_mtcars >> arrange(_.hp)\n\n\n\n\n\n  \n    \n      \n      cyl\n      mpg\n      hp\n    \n  \n  \n    \n      18\n      4\n      30.4\n      52\n    \n    \n      7\n      4\n      24.4\n      62\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      28\n      8\n      15.8\n      264\n    \n    \n      30\n      8\n      15.0\n      335\n    \n  \n\n32 rows √ó 3 columns\n\n\n\n\n\nSort in descending order\nIf you add a - before a column or expression, arrange will sort the rows in descending order. This applies to all types of columns, including arrays of strings and categories!\n\nsmall_mtcars >> arrange(-_.hp)\n\n\n\n\n\n  \n    \n      \n      cyl\n      mpg\n      hp\n    \n  \n  \n    \n      30\n      8\n      15.0\n      335\n    \n    \n      28\n      8\n      15.8\n      264\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      7\n      4\n      24.4\n      62\n    \n    \n      18\n      4\n      30.4\n      52\n    \n  \n\n32 rows √ó 3 columns\n\n\n\n\n\nArrange by multiple variables\nWhen arrange receives multiple arguments, it sorts so that the one specified first changes the slowest, followed by the second, and so on.\n\nsmall_mtcars >> arrange(_.cyl, -_.mpg)\n\n\n\n\n\n  \n    \n      \n      cyl\n      mpg\n      hp\n    \n  \n  \n    \n      19\n      4\n      33.9\n      65\n    \n    \n      17\n      4\n      32.4\n      66\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      14\n      8\n      10.4\n      205\n    \n    \n      15\n      8\n      10.4\n      215\n    \n  \n\n32 rows √ó 3 columns\n\n\n\nNotice that in the result above, cyl values are sorted first. In other words, all of the 4‚Äôs are bunched together, with mpg sorted in descending order within each bunch.\n\n\nUsing expressions\nYou can also arrange the rows of your data using more complex expressions, similar to those you would use in a mutate.\nFor example, the code below sorts by horsepower (hp) per cylinder (cyl).\n\nsmall_mtcars >> arrange(_.hp / _.cyl)\n\n\n\n\n\n  \n    \n      \n      cyl\n      mpg\n      hp\n    \n  \n  \n    \n      18\n      4\n      30.4\n      52\n    \n    \n      7\n      4\n      24.4\n      62\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      28\n      8\n      15.8\n      264\n    \n    \n      30\n      8\n      15.0\n      335\n    \n  \n\n32 rows √ó 3 columns\n\n\n\n\n\nCategorical series behavior\nArrange uses pd.sort_values() behind the scenes, which sorts pd.Categorical series by their category order.\n\nser = pd.Categorical([\"a\", \"z\"], categories=[\"z\", \"a\"])\n\nser\n\n['a', 'z']\nCategories (2, object): ['z', 'a']\n\n\n\nser.sort_values()\n\n['z', 'a']\nCategories (2, object): ['z', 'a']\n\n\nSiuba contains a submodule called forcats that make it easy to change the category order.\n\nfrom siuba.dply.forcats import fct_rev\n\n# reverse the category order\nfct_rev(ser)\n\n['a', 'z']\nCategories (2, object): ['a', 'z']\n\n\nYou can learn more in the siuba forcats docs."
  },
  {
    "objectID": "guide/verb-filter.html",
    "href": "guide/verb-filter.html",
    "title": "Filter rows",
    "section": "",
    "text": "The filter() function keeps rows of data that meet all specified conditions."
  },
  {
    "objectID": "guide/verb-filter.html#what-counts-as-na",
    "href": "guide/verb-filter.html#what-counts-as-na",
    "title": "Filter rows",
    "section": "What counts as NA?",
    "text": "What counts as NA?\nUse pandas.isna() to determine whether a value is considered to be NA.\n\ndf = pd.DataFrame({\n    \"x\": [True, False, None],\n    })\n\ndf.x\n\n0     True\n1    False\n2     None\nName: x, dtype: object\n\n\nNotice in the code above that the last value is None. We can confirm pandas sees this as an NA with the code below.\n\npd.isna(df.x)\n\n0    False\n1    False\n2     True\nName: x, dtype: bool\n\n\nSince None is considered an NA, its row gets removed in the filter below.\n\ndf >> filter(_.x)\n\n\n\n\n\n  \n    \n      \n      x\n    \n  \n  \n    \n      0\n      True"
  },
  {
    "objectID": "guide/verb-filter.html#drop-only-by-na",
    "href": "guide/verb-filter.html#drop-only-by-na",
    "title": "Filter rows",
    "section": "Drop only by NA",
    "text": "Drop only by NA\nIf you want to remove only by NA values from your data, use the pandas .notna() method.\nThis effectively says, ‚Äúkeep any values of x that are not NA‚Äù.\n\ndf >> filter(_.x.notna())\n\n\n\n\n\n  \n    \n      \n      x\n    \n  \n  \n    \n      0\n      True\n    \n    \n      1\n      False"
  },
  {
    "objectID": "guide/verb-group-by.html",
    "href": "guide/verb-group-by.html",
    "title": "Group by",
    "section": "",
    "text": "This function is used to specify groups in your data for verbs‚Äîlike mutate(), filter(), and summarize()‚Äîto perform operations over.\nFor example, in the mtcars dataset, there are 3 possible values for cylinders (cyl). You could use group_by to say that you want to perform operations separately for each of these 3 groups of values.\nAn important compliment to group_by() is ungroup(), which removes all current groupings."
  },
  {
    "objectID": "guide/verb-group-by.html#group-by-column",
    "href": "guide/verb-group-by.html#group-by-column",
    "title": "Group by",
    "section": "Group by column",
    "text": "Group by column\nThe simplest way to use group by is to specify your grouping column directly. This is shown below, by grouping mtcars according to its 3 groups of cylinder values (4, 6, or 8 cylinders).\n\ng_cyl = small_cars >> group_by(_.cyl)\n\ng_cyl\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      cyl\n      gear\n      hp\n    \n  \n  \n    \n      0\n      6\n      4\n      110\n    \n    \n      1\n      6\n      4\n      110\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      8\n      5\n      335\n    \n    \n      31\n      4\n      4\n      109\n    \n  \n\n32 rows √ó 3 columns\n\n\n\nNote that the result is simply a pandas GroupedDataFrame, which is what is returned if you use mtcars.groupby('cyl'). Normally, a GroupedDataFrame doesn‚Äôt print out a preview of itself, but siuba modifies it to do so, since this is very handy.\nThe group_by function is most often used with filter, mutate, and summarize.\n\nFilter\n\n# keep rows where hp is greater than mean hp within cyl group\ng_cyl >> filter(_.hp > _.hp.mean())\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      cyl\n      gear\n      hp\n    \n  \n  \n    \n      2\n      4\n      4\n      93\n    \n    \n      6\n      8\n      3\n      245\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      8\n      5\n      335\n    \n    \n      31\n      4\n      4\n      109\n    \n  \n\n15 rows √ó 3 columns\n\n\n\n\n\nMutate\n\ng_cyl >> mutate(avg_hp = _.hp.mean())\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      cyl\n      gear\n      hp\n      avg_hp\n    \n  \n  \n    \n      0\n      6\n      4\n      110\n      122.285714\n    \n    \n      1\n      6\n      4\n      110\n      122.285714\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      8\n      5\n      335\n      209.214286\n    \n    \n      31\n      4\n      4\n      109\n      82.636364\n    \n  \n\n32 rows √ó 4 columns\n\n\n\n\n\nSummarize\n\ng_cyl >> summarize(avg_hp = _.hp.mean())\n\n\n\n\n\n  \n    \n      \n      cyl\n      avg_hp\n    \n  \n  \n    \n      0\n      4\n      82.636364\n    \n    \n      1\n      6\n      122.285714\n    \n    \n      2\n      8\n      209.214286"
  },
  {
    "objectID": "guide/verb-group-by.html#group-by-multiple-columns",
    "href": "guide/verb-group-by.html#group-by-multiple-columns",
    "title": "Group by",
    "section": "Group by multiple columns",
    "text": "Group by multiple columns\nIn order to group by multiple columns, simply specify them all as arguments to group_by.\n\nsmall_cars >> group_by(_.cyl, _.gear)\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      cyl\n      gear\n      hp\n    \n  \n  \n    \n      0\n      6\n      4\n      110\n    \n    \n      1\n      6\n      4\n      110\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      8\n      5\n      335\n    \n    \n      31\n      4\n      4\n      109\n    \n  \n\n32 rows √ó 3 columns"
  },
  {
    "objectID": "guide/verb-group-by.html#group-by-an-expression",
    "href": "guide/verb-group-by.html#group-by-an-expression",
    "title": "Group by",
    "section": "Group by an expression",
    "text": "Group by an expression\n\nsmall_cars >> group_by(high_hp = _.hp > 300)\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      cyl\n      gear\n      hp\n      high_hp\n    \n  \n  \n    \n      0\n      6\n      4\n      110\n      False\n    \n    \n      1\n      6\n      4\n      110\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      8\n      5\n      335\n      True\n    \n    \n      31\n      4\n      4\n      109\n      False\n    \n  \n\n32 rows √ó 4 columns"
  },
  {
    "objectID": "guide/verb-group-by.html#count-rows",
    "href": "guide/verb-group-by.html#count-rows",
    "title": "Group by",
    "section": "Count rows",
    "text": "Count rows\n\nfrom siuba import _, group_by, count\n\n# count number of rows per group\nmtcars >> group_by(_.cyl, _.gear) >> summarize(n = _.shape[0])\n\n# equivalent\nmtcars >> count(_.cyl, _.gear)\n\n\n\n\n\n  \n    \n      \n      cyl\n      gear\n      n\n    \n  \n  \n    \n      0\n      4\n      3\n      1\n    \n    \n      1\n      4\n      4\n      8\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      6\n      8\n      3\n      12\n    \n    \n      7\n      8\n      5\n      2\n    \n  \n\n8 rows √ó 3 columns"
  },
  {
    "objectID": "guide/verb-group-by.html#ungroup",
    "href": "guide/verb-group-by.html#ungroup",
    "title": "Group by",
    "section": "Ungroup",
    "text": "Ungroup\n\nsmall_cars >> group_by(_.cyl) >> ungroup()\n\n\n\n\n\n  \n    \n      \n      cyl\n      gear\n      hp\n    \n  \n  \n    \n      0\n      6\n      4\n      110\n    \n    \n      1\n      6\n      4\n      110\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      8\n      5\n      335\n    \n    \n      31\n      4\n      4\n      109\n    \n  \n\n32 rows √ó 3 columns"
  },
  {
    "objectID": "guide/verb-mutate.html",
    "href": "guide/verb-mutate.html",
    "title": "Mutate to transform",
    "section": "",
    "text": "The mutate() function creates a new column of data, or overwrite an existing one.\nWe‚Äôll use a subset of the mtcars dataset for examples."
  },
  {
    "objectID": "guide/verb-mutate.html#basics",
    "href": "guide/verb-mutate.html#basics",
    "title": "Mutate to transform",
    "section": "Basics",
    "text": "Basics\n\nsmall_cars >> mutate(mpg_per_cyl = _.mpg / _.cyl)\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      hp\n      mpg_per_cyl\n    \n  \n  \n    \n      0\n      21.0\n      6\n      110\n      3.500\n    \n    \n      1\n      21.0\n      6\n      110\n      3.500\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      15.0\n      8\n      335\n      1.875\n    \n    \n      31\n      21.4\n      4\n      109\n      5.350\n    \n  \n\n32 rows √ó 4 columns"
  },
  {
    "objectID": "guide/verb-mutate.html#replacing-columns",
    "href": "guide/verb-mutate.html#replacing-columns",
    "title": "Mutate to transform",
    "section": "Replacing columns",
    "text": "Replacing columns\nWhen a created column is given the same name as an existing column, it replaces that column in the data.\n\nsmall_cars >> mutate(mpg = _.mpg - _.mpg.mean(), new_column = 1)\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      hp\n      new_column\n    \n  \n  \n    \n      0\n      0.909375\n      6\n      110\n      1\n    \n    \n      1\n      0.909375\n      6\n      110\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      -5.090625\n      8\n      335\n      1\n    \n    \n      31\n      1.309375\n      4\n      109\n      1\n    \n  \n\n32 rows √ó 4 columns\n\n\n\nNote that replacement columns are put in the same position as the original columns. For example, in the result above, the mpg column is still in the first position on the left."
  },
  {
    "objectID": "guide/verb-mutate.html#using-previous-arguments",
    "href": "guide/verb-mutate.html#using-previous-arguments",
    "title": "Mutate to transform",
    "section": "Using previous arguments",
    "text": "Using previous arguments\nArguments can refer to columns that were created in earlier arguments.\n\nsmall_cars >> mutate(cyl2 = _.cyl * 2, cyl4 = _.cyl2 * 2)\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      hp\n      cyl2\n      cyl4\n    \n  \n  \n    \n      0\n      21.0\n      6\n      110\n      12\n      24\n    \n    \n      1\n      21.0\n      6\n      110\n      12\n      24\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      15.0\n      8\n      335\n      16\n      32\n    \n    \n      31\n      21.4\n      4\n      109\n      8\n      16\n    \n  \n\n32 rows √ó 5 columns\n\n\n\nIn the code above, cyl4 uses the earlier argument cyl2."
  },
  {
    "objectID": "guide/verb-mutate.html#grouped-mutates",
    "href": "guide/verb-mutate.html#grouped-mutates",
    "title": "Mutate to transform",
    "section": "Grouped mutates",
    "text": "Grouped mutates\n\n(small_cars\n  >> group_by(_.cyl)\n  >> mutate(\n       hp_mean = _.hp.mean(),\n       demeaned_hp = _.hp - _.hp_mean\n     )\n  )\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      mpg\n      cyl\n      hp\n      hp_mean\n      demeaned_hp\n    \n  \n  \n    \n      0\n      21.0\n      6\n      110\n      122.285714\n      -12.285714\n    \n    \n      1\n      21.0\n      6\n      110\n      122.285714\n      -12.285714\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      15.0\n      8\n      335\n      209.214286\n      125.785714\n    \n    \n      31\n      21.4\n      4\n      109\n      82.636364\n      26.363636\n    \n  \n\n32 rows √ó 5 columns\n\n\n\n\n(small_cars\n  >> group_by(_.cyl)\n  >> mutate(\n       hp_per_cyl = _.hp / _.cyl,\n       diff = _.hp_per_cyl - _.hp_per_cyl.shift(1)\n     )\n  )\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      mpg\n      cyl\n      hp\n      hp_per_cyl\n      diff\n    \n  \n  \n    \n      0\n      21.0\n      6\n      110\n      18.333333\n      NaN\n    \n    \n      1\n      21.0\n      6\n      110\n      18.333333\n      0.000\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      15.0\n      8\n      335\n      41.875000\n      8.875\n    \n    \n      31\n      21.4\n      4\n      109\n      27.250000\n      -1.000\n    \n  \n\n32 rows √ó 5 columns"
  },
  {
    "objectID": "guide/verb-select.html",
    "href": "guide/verb-select.html",
    "title": "Select columns",
    "section": "",
    "text": "This function lets you select specific columns of your data to keep.\nThere are three different building blocks that can used in a selection:"
  },
  {
    "objectID": "guide/verb-select.html#select-by-name-or-position",
    "href": "guide/verb-select.html#select-by-name-or-position",
    "title": "Select columns",
    "section": "Select by name or position",
    "text": "Select by name or position\nThe simplest way to select a column to keep is to refer to it by name or position.\n\nselect(penguins, _.species, _.island, 6, -1)\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      sex\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      male\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      female\n      2007\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      342\n      Chinstrap\n      Dream\n      male\n      2009\n    \n    \n      343\n      Chinstrap\n      Dream\n      female\n      2009\n    \n  \n\n344 rows √ó 4 columns\n\n\n\nThe code above does the following:\n\nselects by name the species and island columns.\nselects by position the index 6 and -1 columns (the last item).\n\nSelecting by position should produce the same results as indexing a list of names.\npenguins.columns[6]       # \"sex\"\npenguins.columns[-1]      # \"year\""
  },
  {
    "objectID": "guide/verb-select.html#excluding-columns",
    "href": "guide/verb-select.html#excluding-columns",
    "title": "Select columns",
    "section": "Excluding columns",
    "text": "Excluding columns\nYou can remove a column from the data by putting a tilde operator (~) in front of it.\n\npenguins >> select(~_.body_mass_g, ~_.sex, ~_.year)\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      342\n      Chinstrap\n      Dream\n      50.8\n      19.0\n      210.0\n    \n    \n      343\n      Chinstrap\n      Dream\n      50.2\n      18.7\n      198.0\n    \n  \n\n344 rows √ó 5 columns\n\n\n\nThe code above keeps all columns except body_mass_g, sex, and year.\nNote that the ~ operator flips the value of True and False in pandas, and is called the ‚Äúinvert operator‚Äù.\n\n~pd.Series([True, False])\n\n0    False\n1     True\ndtype: bool"
  },
  {
    "objectID": "guide/verb-select.html#renaming-columns",
    "href": "guide/verb-select.html#renaming-columns",
    "title": "Select columns",
    "section": "Renaming columns",
    "text": "Renaming columns\nYou can rename a specified column by using the equality operator (==). This operation takes the following form.\n\n_.new_name == _.old_name\n\n\npenguins >> select(_.species_name == _.species)\n\n\n\n\n\n  \n    \n      \n      species_name\n    \n  \n  \n    \n      0\n      Adelie\n    \n    \n      1\n      Adelie\n    \n    \n      ...\n      ...\n    \n    \n      342\n      Chinstrap\n    \n    \n      343\n      Chinstrap\n    \n  \n\n344 rows √ó 1 columns\n\n\n\nNote that expressing the new column name on the left is similar to how creating a python dictionary works. For example‚Ä¶\n\nselect(_.a == _.x, _.b == _.y)\ndict(a = \"x\", b = \"y\")\n\nboth create new entries named ‚Äúa‚Äù and ‚Äúb‚Äù."
  },
  {
    "objectID": "guide/verb-select.html#select-by-slice",
    "href": "guide/verb-select.html#select-by-slice",
    "title": "Select columns",
    "section": "Select by slice",
    "text": "Select by slice\nWhen the columns are adjacent to each other, you can select them using _[\"start_col\":\"end_col\"].\n\npenguins >> select(_.species, _[\"bill_length_mm\":\"body_mass_g\"])\n\n\n\n\n\n  \n    \n      \n      species\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n    \n  \n  \n    \n      0\n      Adelie\n      39.1\n      18.7\n      181.0\n      3750.0\n    \n    \n      1\n      Adelie\n      39.5\n      17.4\n      186.0\n      3800.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      342\n      Chinstrap\n      50.8\n      19.0\n      210.0\n      4100.0\n    \n    \n      343\n      Chinstrap\n      50.2\n      18.7\n      198.0\n      3775.0\n    \n  \n\n344 rows √ó 5 columns\n\n\n\nYou can use three methods to specify a column in a slice:\n\n_.some_col\n\"some_col\"\na position number\n\n\nExclusion\nYou can exclude slice selections using the ~ operator.\n\npenguins >> select(~_[\"bill_length_mm\":\"body_mass_g\"])\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      sex\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      male\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      female\n      2007\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      342\n      Chinstrap\n      Dream\n      male\n      2009\n    \n    \n      343\n      Chinstrap\n      Dream\n      female\n      2009\n    \n  \n\n344 rows √ó 4 columns\n\n\n\n\n\nPosition number\nNote that when position number is used to slice columns, the end position is not included in the selection.\n\n# these are equivalent\n\npenguins >> select(0, 1)\npenguins >> select(_[0:2])\n\n\n\n\n\n  \n    \n      \n      species\n      island\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n    \n    \n      1\n      Adelie\n      Torgersen\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      342\n      Chinstrap\n      Dream\n    \n    \n      343\n      Chinstrap\n      Dream\n    \n  \n\n344 rows √ó 2 columns"
  },
  {
    "objectID": "guide/verb-select.html#select-by-pattern-e.g.-endswith",
    "href": "guide/verb-select.html#select-by-pattern-e.g.-endswith",
    "title": "Select columns",
    "section": "Select by pattern (e.g.¬†endswith)",
    "text": "Select by pattern (e.g.¬†endswith)\n\npenguins >> select(_.species, _.endswith(\"mm\"))\n\n\n\n\n\n  \n    \n      \n      species\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n    \n  \n  \n    \n      0\n      Adelie\n      39.1\n      18.7\n      181.0\n    \n    \n      1\n      Adelie\n      39.5\n      17.4\n      186.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      342\n      Chinstrap\n      50.8\n      19.0\n      210.0\n    \n    \n      343\n      Chinstrap\n      50.2\n      18.7\n      198.0\n    \n  \n\n344 rows √ó 4 columns\n\n\n\n\npenguins >> select(_.contains(\"length\"))\n\n\n\n\n\n  \n    \n      \n      bill_length_mm\n      flipper_length_mm\n    \n  \n  \n    \n      0\n      39.1\n      181.0\n    \n    \n      1\n      39.5\n      186.0\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      342\n      50.8\n      210.0\n    \n    \n      343\n      50.2\n      198.0\n    \n  \n\n344 rows √ó 2 columns"
  },
  {
    "objectID": "guide/verb-select.html#pandas-comparison",
    "href": "guide/verb-select.html#pandas-comparison",
    "title": "Select columns",
    "section": "Pandas comparison",
    "text": "Pandas comparison\n\nimport pandas as pd\n\nfrom siuba.data import mtcars\nfrom siuba import select, _\n\nClick between tabs to compare code across siuba and pandas.\n\nSiubaPandas\n\n\n\n# keep cyl column\nmtcars >> select(_.cyl)\n\n# keep all *except* cyl column\nmtcars >> select(-_.cyl)\n\n# complex select, plus rename cyl to cylinder\nmtcars >> select(_.cylinder == _.cyl, _.startswith(\"m\"))\n\n\n\n\n# keep cyl column\nmtcars[[\"cyl\"]]\n\n# keep all *except* cyl column\nmtcars.drop([\"cyl\"], axis=1)\n\n# complex select, plus rename cyl to cylinder\ncols = mtcars.columns\nmtcars.loc[:, (cols == \"cyl\") | cols.str.startswith(\"m\")] \\\n      .rename({\"cyl\": \"cylinder\"})"
  },
  {
    "objectID": "guide/verb-summarize.html",
    "href": "guide/verb-summarize.html",
    "title": "Summarize to aggregate",
    "section": "",
    "text": "The summarize() creates new columns in your table, based on an aggregation. Aggregations take data and reduces it to a single number. When applied to grouped data, this function returns one row per grouping."
  },
  {
    "objectID": "guide/verb-summarize.html#summarize-over-all-rows",
    "href": "guide/verb-summarize.html#summarize-over-all-rows",
    "title": "Summarize to aggregate",
    "section": "Summarize over all rows",
    "text": "Summarize over all rows\n\nmtcars >> summarize(avg_mpg = _.mpg.mean())\nmtcars\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      15.0\n      8\n      301.0\n      335\n      3.54\n      3.570\n      14.60\n      0\n      1\n      5\n      8\n    \n    \n      31\n      21.4\n      4\n      121.0\n      109\n      4.11\n      2.780\n      18.60\n      1\n      1\n      4\n      2\n    \n  \n\n32 rows √ó 11 columns"
  },
  {
    "objectID": "guide/verb-summarize.html#summarize-over-groups",
    "href": "guide/verb-summarize.html#summarize-over-groups",
    "title": "Summarize to aggregate",
    "section": "Summarize over groups",
    "text": "Summarize over groups\nUse group_by() to split the data up, apply some aggregation, and then combine results.\n\n(mtcars\n  >> group_by(_.cyl)\n  >> summarize(\n       avg = _.mpg.mean(),\n       range = _.mpg.max() - _.mpg.min(),\n       avg_per_cyl = (_.mpg / _.cyl).mean()\n  )\n)\n\n\n\n\n\n  \n    \n      \n      cyl\n      avg\n      range\n      avg_per_cyl\n    \n  \n  \n    \n      0\n      4\n      26.663636\n      12.5\n      6.665909\n    \n    \n      1\n      6\n      19.742857\n      3.6\n      3.290476\n    \n    \n      2\n      8\n      15.100000\n      8.8\n      1.887500\n    \n  \n\n\n\n\nNote there are 3 unique groupings for cyl (4, 6, and 8), so the resulting table has 3 rows."
  },
  {
    "objectID": "guide/workflows-backends.html",
    "href": "guide/workflows-backends.html",
    "title": "Backend Examples üöß",
    "section": "",
    "text": "from siuba import _, group_by, filter, show_query\nfrom siuba.sql import LazyTbl\nfrom siuba.data import mtcars\n\nfrom sqlalchemy import create_engine"
  },
  {
    "objectID": "guide/workflows-backends.html#duckdb",
    "href": "guide/workflows-backends.html#duckdb",
    "title": "Backend Examples üöß",
    "section": "duckdb",
    "text": "duckdb\n\nengine = create_engine(\"duckdb:///:memory:\")\nengine.execute(\"register\", (\"mtcars\", mtcars))\nengine.execute(\"SHOW TABLES\").fetchall()\n\n[('mtcars',)]\n\n\n\ntbl_mtcars_duckdb = LazyTbl(engine, \"mtcars\")\n\ntbl_filtered = tbl_mtcars_duckdb >> filter_mpg\ntbl_filtered\n\n\n# Source: lazy query\n# DB Conn: Engine(duckdb:///:memory:)\n# Preview:\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      14.3\n      8\n      360.0\n      245\n      3.21\n      3.570\n      15.84\n      0\n      0\n      3\n      4\n    \n    \n      1\n      10.4\n      8\n      472.0\n      205\n      2.93\n      5.250\n      17.98\n      0\n      0\n      3\n      4\n    \n    \n      2\n      10.4\n      8\n      460.0\n      215\n      3.00\n      5.424\n      17.82\n      0\n      0\n      3\n      4\n    \n    \n      3\n      14.7\n      8\n      440.0\n      230\n      3.23\n      5.345\n      17.42\n      0\n      0\n      3\n      4\n    \n    \n      4\n      13.3\n      8\n      350.0\n      245\n      3.73\n      3.840\n      15.41\n      0\n      0\n      3\n      4\n    \n  \n\n# .. may have more rows\n\n\n\nq = tbl_filtered >> show_query(simplify=True)\n\nSELECT mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb \nFROM (SELECT *, mpg < avg(mpg) OVER (PARTITION BY cyl) AS win1 \nFROM (SELECT * \nFROM mtcars) AS anon_2) AS anon_1 \nWHERE win1"
  },
  {
    "objectID": "guide/wrangle-helpers.html",
    "href": "guide/wrangle-helpers.html",
    "title": "Helpers: count, separate, complete",
    "section": "",
    "text": "Some combinations of verbs and column operations get used so frequently that they earn their own helper verbs. These helpers make things a little quicker or concise to type.\nThis page discusses 3 helper functions that will super charge your workflow:"
  },
  {
    "objectID": "guide/wrangle-helpers.html#count-values",
    "href": "guide/wrangle-helpers.html#count-values",
    "title": "Helpers: count, separate, complete",
    "section": "Count values",
    "text": "Count values\nUse count() to quickly tally up the number of rows per grouping.\n\nmtcars >> count(_.cyl, _.gear)\n\n\n\n\n\n  \n    \n      \n      cyl\n      gear\n      n\n    \n  \n  \n    \n      0\n      4\n      3\n      1\n    \n    \n      1\n      4\n      4\n      8\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      6\n      8\n      3\n      12\n    \n    \n      7\n      8\n      5\n      2\n    \n  \n\n8 rows √ó 3 columns\n\n\n\nThe output above has 8 rows‚Äîone for each grouping‚Äîand a column named n with the number of observations in each grouping.\nNote that count() is a shorthand for combining group_by() and summarize().\n\nfrom siuba import group_by, summarize\n\nmtcars >> group_by(_.cyl, _.gear) >> summarize(n = _.shape[0])\n\n\n\n\n\n  \n    \n      \n      cyl\n      gear\n      n\n    \n  \n  \n    \n      0\n      4\n      3\n      1\n    \n    \n      1\n      4\n      4\n      8\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      6\n      8\n      3\n      12\n    \n    \n      7\n      8\n      5\n      2\n    \n  \n\n8 rows √ó 3 columns\n\n\n\n\nSorting results\nUse the sort argument to sort results by number of observations, in descending order.\n\nmtcars >> count(_.cyl, sort=True)\n\n\n\n\n\n  \n    \n      \n      cyl\n      n\n    \n  \n  \n    \n      0\n      8\n      14\n    \n    \n      1\n      4\n      11\n    \n    \n      2\n      6\n      7\n    \n  \n\n\n\n\n\n\nWith expressions\n\nmtcars >> count(_.cyl, high_mpg = _.mpg > 30)\n\n\n\n\n\n  \n    \n      \n      cyl\n      high_mpg\n      n\n    \n  \n  \n    \n      0\n      4\n      False\n      7\n    \n    \n      1\n      4\n      True\n      4\n    \n    \n      2\n      6\n      False\n      7\n    \n    \n      3\n      8\n      False\n      14\n    \n  \n\n\n\n\nHere‚Äôs a somewhat advanced example that puts mpg in different bins.\n\nfrom siuba import count\nfrom siuba.siu import call\nfrom siuba.data import mtcars\n\ncar_counts = mtcars >> count(_.cyl, mpg = call(pd.cut, _.mpg, bins = 3))\ncar_counts\n\n\n\n\n\n  \n    \n      \n      cyl\n      mpg\n      n\n    \n  \n  \n    \n      0\n      4\n      (10.376, 18.233]\n      0\n    \n    \n      1\n      4\n      (18.233, 26.067]\n      6\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      7\n      8\n      (18.233, 26.067]\n      2\n    \n    \n      8\n      8\n      (26.067, 33.9]\n      0\n    \n  \n\n9 rows √ó 3 columns\n\n\n\nNote these important pieces:\n\ncall(pd.cut, ...) is used to lazily call pd.cut(...).\npd.cut splits the data into bins, that count then uses as a grouping.\n\nHere‚Äôs a barchart of this data in plotnine.\n\nfrom plotnine import ggplot, aes, geom_col, facet_wrap, theme, element_text\n\n(car_counts\n    >> ggplot(aes(\"mpg\", \"n\", fill=\"mpg\"))\n    + geom_col()\n    + facet_wrap(\"~cyl\")\n    + theme(axis_text_x = element_text(angle=45, hjust=1))\n)\n\n\n\n\n<ggplot: (8790596394486)>\n\n\nAs vehicles increase in cylinders, they have fewer low mpg vehicles. For example, no 8 cylinder vehicles are in the highest mpg bin (above 26 mpg).\n\n\nWeighted counts\nUse the wt argument to do a weighted count (i.e.¬†sum the weight values per grouping).\nThis is useful if you have data that already contains a count, such as the count of cyl and gear below.\n\ntbl_n_cyl_gear = mtcars >> count(_.cyl, _.gear)\n\nThe wt argument lets us roll this result up to count observations per cyl.\n\ntbl_n_cyl_gear >> count(_.cyl, wt=_.n)\n\n\n\n\n\n  \n    \n      \n      cyl\n      nn\n    \n  \n  \n    \n      0\n      4\n      11\n    \n    \n      1\n      6\n      7\n    \n    \n      2\n      8\n      14\n    \n  \n\n\n\n\nNotice that this is equivalent to counting cyl on the raw data.\n\nmtcars >> count(_.cyl)\n\n\n\n\n\n  \n    \n      \n      cyl\n      n\n    \n  \n  \n    \n      0\n      4\n      11\n    \n    \n      1\n      6\n      7\n    \n    \n      2\n      8\n      14"
  },
  {
    "objectID": "guide/wrangle-helpers.html#separate-strings-into-columns",
    "href": "guide/wrangle-helpers.html#separate-strings-into-columns",
    "title": "Helpers: count, separate, complete",
    "section": "Separate strings into columns",
    "text": "Separate strings into columns\nUse separate() to split a column using a pattern, and produce new columns.\nBy default, it splits strings on any non-alphanumeric character, so is helpful for quickly splitting in common situations where values are seperated by dashes (e.g.¬†‚Äúa-1‚Äù).\nFor example, here is some data where the condition column could be split on \"-\".\n\nmeasures = pd.DataFrame({\n    \"condition\": [\"a-1\", \"a-2\", \"b-1\", \"b-2\"],\n    \"value\": [1, 2, 3, 4]\n})\n\nmeasures\n\n\n\n\n\n  \n    \n      \n      condition\n      value\n    \n  \n  \n    \n      0\n      a-1\n      1\n    \n    \n      1\n      a-2\n      2\n    \n    \n      2\n      b-1\n      3\n    \n    \n      3\n      b-2\n      4\n    \n  \n\n\n\n\nSeparate takes the column we want to split, the names of the new columns. It produces a new table with the new columns in the place of the old one.\n\nmeasures >> separate(_.condition, [\"name\", \"number\"])\n\n\n\n\n\n  \n    \n      \n      value\n      name\n      number\n    \n  \n  \n    \n      0\n      1\n      a\n      1\n    \n    \n      1\n      2\n      a\n      2\n    \n    \n      2\n      3\n      b\n      1\n    \n    \n      3\n      4\n      b\n      2\n    \n  \n\n\n\n\nNotice that the condition column got split into name and number columns."
  },
  {
    "objectID": "guide/wrangle-helpers.html#complete-combinations-of-data",
    "href": "guide/wrangle-helpers.html#complete-combinations-of-data",
    "title": "Helpers: count, separate, complete",
    "section": "Complete combinations of data",
    "text": "Complete combinations of data\nUse complete() to fill in missing combinations in the data.\nFor example, the code below counts penguins across species and island, but not all species are on each island.\n\ntbl_n_penguins = penguins >> count(_.species, _.island)\ntbl_n_penguins\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      n\n    \n  \n  \n    \n      0\n      Adelie\n      Biscoe\n      44\n    \n    \n      1\n      Adelie\n      Dream\n      56\n    \n    \n      2\n      Adelie\n      Torgersen\n      52\n    \n    \n      3\n      Chinstrap\n      Dream\n      68\n    \n    \n      4\n      Gentoo\n      Biscoe\n      124\n    \n  \n\n\n\n\nNotice the following in the count:\n\nAdelie penguins are on three islands, BUT\nChinstrap are only on Dream.\nGentoo are only on Biscoe.\n\nWe can use complete to add rows with 0 counts for islands Chinstrap and Gentoo aren‚Äôt on.\n\ntbl_n_penguins >> complete(_.species, _.island, fill = {\"n\": 0})\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      n\n    \n  \n  \n    \n      0\n      Adelie\n      Biscoe\n      44.0\n    \n    \n      1\n      Adelie\n      Dream\n      56.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      7\n      Gentoo\n      Dream\n      0.0\n    \n    \n      8\n      Gentoo\n      Torgersen\n      0.0\n    \n  \n\n9 rows √ó 3 columns\n\n\n\nNow the data has 9 rows, one for each combination of species and island. The fill = {\"n\": 0} argument allowed us to set a default value of 0 for the previously missing rows."
  },
  {
    "objectID": "guide/wrangle-joins.html",
    "href": "guide/wrangle-joins.html",
    "title": "Join tables üìù",
    "section": "",
    "text": "Warning\n\n\n\nThis page is at the refinement stage. The structure, examples, and figures are there, but it lacks narrative structure / needs refining."
  },
  {
    "objectID": "guide/wrangle-joins.html#syntax",
    "href": "guide/wrangle-joins.html#syntax",
    "title": "Join tables üìù",
    "section": "Syntax",
    "text": "Syntax\nLike other siuba verbs, joins can be used in two ways: directly passing both data as arguments, or by piping.\n\n# directly passing data\ninner_join(lhs, rhs, on=\"id\")\n\n# piping\nlhs >> inner_join(_, rhs, on=\"id\")\n\n\n\n\n\n  \n    \n      \n      id\n      val_x\n      val_y\n    \n  \n  \n    \n      0\n      1\n      lhs.1\n      rhs.1\n    \n    \n      1\n      2\n      lhs.2\n      rhs.2\n    \n  \n\n\n\n\nNote that when used in a pipe, the first argument to the join must be _, to represent the data."
  },
  {
    "objectID": "guide/wrangle-joins.html#mutating-joins",
    "href": "guide/wrangle-joins.html#mutating-joins",
    "title": "Join tables üìù",
    "section": "Mutating joins",
    "text": "Mutating joins\n\n\nInner join\n\ninner_join(lhs, rhs, on=\"id\")\n\n\n\n\n\n  \n    \n      \n      id\n      val_x\n      val_y\n    \n  \n  \n    \n      0\n      1\n      lhs.1\n      rhs.1\n    \n    \n      1\n      2\n      lhs.2\n      rhs.2\n    \n  \n\n\n\n\n\n\nOuter joins\n\nleft_join(lhs, rhs, on=\"id\")\n\n\n\n\n\n  \n    \n      \n      id\n      val_x\n      val_y\n    \n  \n  \n    \n      0\n      1\n      lhs.1\n      rhs.1\n    \n    \n      1\n      2\n      lhs.2\n      rhs.2\n    \n    \n      2\n      3\n      lhs.3\n      NaN\n    \n  \n\n\n\n\n\nfull_join(lhs, rhs, on=\"id\")\n\n\n\n\n\n  \n    \n      \n      id\n      val_x\n      val_y\n    \n  \n  \n    \n      0\n      1\n      lhs.1\n      rhs.1\n    \n    \n      1\n      2\n      lhs.2\n      rhs.2\n    \n    \n      2\n      3\n      lhs.3\n      NaN\n    \n    \n      3\n      4\n      NaN\n      rhs.3"
  },
  {
    "objectID": "guide/wrangle-joins.html#filtering-joins",
    "href": "guide/wrangle-joins.html#filtering-joins",
    "title": "Join tables üìù",
    "section": "Filtering joins",
    "text": "Filtering joins\n\nsemi_join(lhs, rhs, on=\"id\")\n\n\n\n\n\n  \n    \n      \n      id\n      val\n    \n  \n  \n    \n      0\n      1\n      lhs.1\n    \n    \n      1\n      2\n      lhs.2\n    \n  \n\n\n\n\n\nanti_join(lhs, rhs, on=\"id\")\n\n\n\n\n\n  \n    \n      \n      id\n      val\n    \n  \n  \n    \n      2\n      3\n      lhs.3"
  },
  {
    "objectID": "guide/wrangle-joins.html#duplicate-matches",
    "href": "guide/wrangle-joins.html#duplicate-matches",
    "title": "Join tables üìù",
    "section": "Duplicate matches",
    "text": "Duplicate matches\n\n\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nlhs_dupes = pd.DataFrame({\n    \"id\": [1, 2, 2, 3], \n    \"val\": [\"lhs.1\", \"lhs.2\", \"lhs.3\", \"lhs.4\"]\n})\n\nrhs_dupes = pd.DataFrame({\n    \"id\": [1, 2, 2, 4],\n    \"val\": [\"rhs.1\", \"rhs.2\", \"rhs.3\", \"rhs.4\"]\n})\n\n\n\n\nlhs_dupes\n\n\n\n\n\n  \n    \n      \n      id\n      val\n    \n  \n  \n    \n      0\n      1\n      lhs.1\n    \n    \n      1\n      2\n      lhs.2\n    \n    \n      2\n      2\n      lhs.3\n    \n    \n      3\n      3\n      lhs.4\n    \n  \n\n\n\n\n\nrhs_dupes\n\n\n\n\n\n  \n    \n      \n      id\n      val\n    \n  \n  \n    \n      0\n      1\n      rhs.1\n    \n    \n      1\n      2\n      rhs.2\n    \n    \n      2\n      2\n      rhs.3\n    \n    \n      3\n      4\n      rhs.4\n    \n  \n\n\n\n\n\n\n\ninner_join(lhs_dupes, rhs_dupes, on=\"id\")\n\n\n\n\n\n  \n    \n      \n      id\n      val_x\n      val_y\n    \n  \n  \n    \n      0\n      1\n      lhs.1\n      rhs.1\n    \n    \n      1\n      2\n      lhs.2\n      rhs.2\n    \n    \n      2\n      2\n      lhs.2\n      rhs.3\n    \n    \n      3\n      2\n      lhs.3\n      rhs.2\n    \n    \n      4\n      2\n      lhs.3\n      rhs.3"
  },
  {
    "objectID": "guide/wrangle-joins.html#na-handling",
    "href": "guide/wrangle-joins.html#na-handling",
    "title": "Join tables üìù",
    "section": "NA handling",
    "text": "NA handling\n\n\n\n\n\n\n\n\n\n\n\nSame as dplyr\n\nimport pandas as pd\nlhs_na = pd.DataFrame({\"id\": [1, pd.NA, 3]})\nrhs_na = pd.DataFrame({\"id\": [1, pd.NA, 2]})\nleft_join(lhs_na, rhs_na, on=\"id\")\n\n\n\n\n\n  \n    \n      \n      id\n    \n  \n  \n    \n      0\n      1\n    \n    \n      1\n      <NA>\n    \n    \n      2\n      3"
  },
  {
    "objectID": "guide/wrangle-joins.html#match-on-multiple-columns",
    "href": "guide/wrangle-joins.html#match-on-multiple-columns",
    "title": "Join tables üìù",
    "section": "Match on multiple columns",
    "text": "Match on multiple columns\n\nlhs_multi = pd.DataFrame({\n    \"source\": [\"a\", \"a\", \"b\"],\n    \"id\": [1, 2, 1],\n    \"val\": [\"lhs.1\", \"lhs.2\", \"lhs.3\"]\n})\n\nrhs_multi = pd.DataFrame({\n    \"source\": [\"a\", \"b\", \"c\"],\n    \"id\": [1, 1, 1],\n    \"val\": [\"lhs.1\", \"lhs.2\", \"lhs.3\"]\n})\n\ninner_join(lhs_multi, rhs_multi, on=[\"source\", \"id\"])\n\n\n\n\n\n  \n    \n      \n      source\n      id\n      val_x\n      val_y\n    \n  \n  \n    \n      0\n      a\n      1\n      lhs.1\n      lhs.1\n    \n    \n      1\n      b\n      1\n      lhs.3\n      lhs.2\n    \n  \n\n\n\n\n\ninner_join(lhs_multi, rhs_multi, on={\"source\": \"source\", \"id\": \"id\"})\n\n\n\n\n\n  \n    \n      \n      source\n      id\n      val_x\n      val_y\n    \n  \n  \n    \n      0\n      a\n      1\n      lhs.1\n      lhs.1\n    \n    \n      1\n      b\n      1\n      lhs.3\n      lhs.2"
  },
  {
    "objectID": "guide/wrangle-joins.html#match-on-expressions",
    "href": "guide/wrangle-joins.html#match-on-expressions",
    "title": "Join tables üìù",
    "section": "Match on expressions",
    "text": "Match on expressions\n\n\n\n\n\n\n\n\n\n\n\nSQL backends can join by expressions.\n\nfrom sqlalchemy import create_engine\nfrom siuba.sql import LazyTbl\n\nengine = create_engine(\"sqlite:///:memory:\")\n\nlhs.to_sql(\"lhs\", engine, index=False)\nrhs.to_sql(\"rhs\", engine, index=False)\n\ntbl_sql_lhs = LazyTbl(engine, \"lhs\")\ntbl_sql_rhs = LazyTbl(engine, \"rhs\")\n\ninner_join(\n    tbl_sql_lhs,\n    tbl_sql_rhs,\n    sql_on = lambda lhs, rhs: lhs.val <= rhs.val\n)\n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n  \n    \n      \n      id_x\n      val_x\n      val_y\n      id_y\n    \n  \n  \n    \n      0\n      1\n      lhs.1\n      rhs.1\n      1\n    \n    \n      1\n      1\n      lhs.1\n      rhs.2\n      2\n    \n    \n      2\n      1\n      lhs.1\n      rhs.3\n      4\n    \n    \n      3\n      2\n      lhs.2\n      rhs.1\n      1\n    \n    \n      4\n      2\n      lhs.2\n      rhs.2\n      2\n    \n  \n\n# .. may have more rows"
  },
  {
    "objectID": "guide/wrangle-reshape.html",
    "href": "guide/wrangle-reshape.html",
    "title": "Reshape tables üìù",
    "section": "",
    "text": "import pandas as pd\nfrom siuba import _, spread, gather\n\ncosts = pd.DataFrame({\n    'id': [1,2],\n    'price_x': [.1, .2],\n    'price_y': [.4, .5],\n    'price_z': [.7, .8]\n})\n\ncosts\n\n\n\n\n\n  \n    \n      \n      id\n      price_x\n      price_y\n      price_z\n    \n  \n  \n    \n      0\n      1\n      0.1\n      0.4\n      0.7\n    \n    \n      1\n      2\n      0.2\n      0.5\n      0.8"
  },
  {
    "objectID": "guide/wrangle-reshape.html#gather-data-long",
    "href": "guide/wrangle-reshape.html#gather-data-long",
    "title": "Reshape tables üìù",
    "section": "Gather data long",
    "text": "Gather data long\n\n# selecting each variable manually\ncosts >> gather('measure', 'value', _.price_x, _.price_y, _.price_z)\n\n# selecting variables using a slice\ncosts >> gather('measure', 'value', _[\"price_x\":\"price_z\"])\n\n# selecting by excluding id\ncosts >> gather('measure', 'value', -_.id)\n\n\n\n\n\n  \n    \n      \n      id\n      measure\n      value\n    \n  \n  \n    \n      0\n      1\n      price_x\n      0.1\n    \n    \n      1\n      2\n      price_x\n      0.2\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      4\n      1\n      price_z\n      0.7\n    \n    \n      5\n      2\n      price_z\n      0.8\n    \n  \n\n6 rows √ó 3 columns"
  },
  {
    "objectID": "guide/wrangle-reshape.html#spread-data-wide",
    "href": "guide/wrangle-reshape.html#spread-data-wide",
    "title": "Reshape tables üìù",
    "section": "Spread data wide",
    "text": "Spread data wide\n\n(costs\n    >> gather('measure', 'value', -_.id)\n    >> spread('measure', 'value')\n)\n\n\n\n\n\n  \n    \n      \n      id\n      price_x\n      price_y\n      price_z\n    \n  \n  \n    \n      0\n      1\n      0.1\n      0.4\n      0.7\n    \n    \n      1\n      2\n      0.2\n      0.5\n      0.8"
  },
  {
    "objectID": "guide/wrangle-reshape.html#pivot-wider-and-aggregate",
    "href": "guide/wrangle-reshape.html#pivot-wider-and-aggregate",
    "title": "Reshape tables üìù",
    "section": "Pivot wider and aggregate",
    "text": "Pivot wider and aggregate\nIf there would be multiple entries per cell in the spread wide data, then the spread() function raises an error.\nThis is shown below, where there are duplicate entries where id=1 and measure=\"a\".\n\ndf = pd.DataFrame({\n    \"id\": [1, 1, 2],\n    \"measure\": [\"a\", \"a\", \"b\"],\n    \"value\": [8, 9, 10]\n})\n\ndf >> spread(\"measure\", \"value\")\n\nValueError: Index contains duplicate entries, cannot reshape\n\n\nUse the pandas pivot_table function to deal with this situation.\n\ndf.pivot_table(columns=\"measure\", values=\"value\", index=\"id\", aggfunc=list)\n\n\n\n\n\n  \n    \n      measure\n      a\n      b\n    \n    \n      id\n      \n      \n    \n  \n  \n    \n      1\n      [8, 9]\n      NaN\n    \n    \n      2\n      NaN\n      [10]\n    \n  \n\n\n\n\nNotice that the top-left entry is a list of two values, [8, 9]. The aggfunc argument is able to reduce those values down to one.\nFor example, by taking the mean.\n\ndf.pivot_table(columns=\"measure\", values=\"value\", index=\"id\", aggfunc=\"mean\")\n\n\n\n\n\n  \n    \n      measure\n      a\n      b\n    \n    \n      id\n      \n      \n    \n  \n  \n    \n      1\n      8.5\n      NaN\n    \n    \n      2\n      NaN\n      10.0\n    \n  \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nsiuba can pipe to the pandas DataFrame‚Äôs pivot_table method as follows.\n\n(df\n    >> _.pivot_table(...)\n)\n\nWhere you would replace ... with your arguments. See flexible piping for more details."
  },
  {
    "objectID": "api/index.html",
    "href": "api/index.html",
    "title": "API",
    "section": "",
    "text": "This page will diagram out key concepts"
  },
  {
    "objectID": "examples/index.html",
    "href": "examples/index.html",
    "title": "Examples",
    "section": "",
    "text": "This page will diagram out key concepts"
  }
]