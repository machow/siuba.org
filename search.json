[
  {
    "objectID": "guide/basics-column-ops.html#lazy-functions",
    "href": "guide/basics-column-ops.html#lazy-functions",
    "title": "Column operations",
    "section": "Lazy functions",
    "text": "Lazy functions"
  },
  {
    "objectID": "guide/basics-column-ops.html#calling-external-functions",
    "href": "guide/basics-column-ops.html#calling-external-functions",
    "title": "Column operations",
    "section": "Calling external functions",
    "text": "Calling external functions\n\nimport pandas as pd\nfrom siuba import _, mutate\nfrom siuba.siu import call\n\nmy_dates = pd.DataFrame({\"date\": [\"2021-01-01\", \"2021-01-02\"]})\n\npd.to_datetime(my_dates.date)\n\n0   2021-01-01\n1   2021-01-02\nName: date, dtype: datetime64[ns]\n\n\n\nmy_dates >> mutate(parsed = _.date) >> _.parsed\n\n0    2021-01-01\n1    2021-01-02\nName: parsed, dtype: object\n\n\n\nmy_dates >> mutate(parsed = call(pd.to_datetime, _.date))\n\n\n\n\n\n  \n    \n      \n      date\n      parsed\n    \n  \n  \n    \n      0\n      2021-01-01\n      2021-01-01\n    \n    \n      1\n      2021-01-02\n      2021-01-02\n    \n  \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou may be familiar with the pd.Series.pipe() method, which could handle the situation using _.date.pipe(...):\n\nmy_dates >> mutate(parsed = _.date.pipe(pd.to_datetime))\n\nBe careful with this approach, since it will work in situations involving pandas DataFrames, but call() works in any situation!"
  },
  {
    "objectID": "guide/basics-examples.html",
    "href": "guide/basics-examples.html",
    "title": "Examples",
    "section": "",
    "text": "This page contains examples of some of the situations siuba really shines in."
  },
  {
    "objectID": "guide/basics-lazy-expressions.html",
    "href": "guide/basics-lazy-expressions.html",
    "title": "Lazy expressions",
    "section": "",
    "text": "A siu expression is a way of specifying what action you want to perform. This allows siuba verbs to decide how to execute the action, depending on whether your data is a local DataFrame or remote table.\nNotice how the output represents each step in our lazy expression, with these pieces:"
  },
  {
    "objectID": "guide/basics-lazy-expressions.html#method-translation",
    "href": "guide/basics-lazy-expressions.html#method-translation",
    "title": "Lazy expressions",
    "section": "Method translation",
    "text": "Method translation\nYou can include method calls like .isin() in a lazy expression.\n\nfrom siuba import _, filter\nfrom siuba.data import mtcars\n\nexpr = _.cyl.isin([2,4])\n\nexpr\n\n‚ñà‚îÄ'__call__'\n‚îú‚îÄ‚ñà‚îÄ.\n‚îÇ ‚îú‚îÄ‚ñà‚îÄ.\n‚îÇ ‚îÇ ‚îú‚îÄ_\n‚îÇ ‚îÇ ‚îî‚îÄ'cyl'\n‚îÇ ‚îî‚îÄ'isin'\n‚îî‚îÄ[2, 4]\n\n\nWhen used in a verb like filter() it will call it over the underlying data. So when you call it on a pandas Series, the Series.isin() method gets called.\n\n# call our expr, which uses .isin\nmtcars >> filter(expr)\n\n# equivalent to...\nmtcars >> filter(_.cyl.isin([2, 4]))\n\n# or in pandas\nmtcars[lambda d: d.cyl.isin([2, 4])]\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      2\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      7\n      24.4\n      4\n      146.7\n      62\n      3.69\n      3.190\n      20.00\n      1\n      0\n      4\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      27\n      30.4\n      4\n      95.1\n      113\n      3.77\n      1.513\n      16.90\n      1\n      1\n      5\n      2\n    \n    \n      31\n      21.4\n      4\n      121.0\n      109\n      4.11\n      2.780\n      18.60\n      1\n      1\n      4\n      2\n    \n  \n\n11 rows √ó 11 columns\n\n\n\nSee the pandas.Series API documentation for detailed documentation on all the different methods available."
  },
  {
    "objectID": "guide/basics-lazy-expressions.html#use-in-pipes",
    "href": "guide/basics-lazy-expressions.html#use-in-pipes",
    "title": "Lazy expressions",
    "section": "Use in pipes",
    "text": "Use in pipes\nSometimes it is helpful to use Pandas DataFrame methods, in addition to siuba verbs. This can be done by piping the data to _.<some_method>().\nHere is an example using the siuba verb count(), with the pandas method .sort_values().\n\nfrom siuba import _, count\nfrom siuba.data import mtcars\n\n(mtcars\n    >> count(_.cyl)         # this is a siuba verb\n    >> _.sort_values(\"n\")   # this is a pandas method\n)\n\n\n\n\n\n  \n    \n      \n      cyl\n      n\n    \n  \n  \n    \n      1\n      6\n      7\n    \n    \n      0\n      4\n      11\n    \n    \n      2\n      8\n      14\n    \n  \n\n\n\n\nHere is another example, using the DataFrame .shape attribute.\n\n\n\n# siuba pipe\nmtcars >> _.shape[0]\n\n32\n\n\n\n# regular pandas\nmtcars.shape[0]"
  },
  {
    "objectID": "guide/basics-lazy-expressions.html#call-external-functions",
    "href": "guide/basics-lazy-expressions.html#call-external-functions",
    "title": "Lazy expressions",
    "section": "Call external functions",
    "text": "Call external functions\n\nimport pandas as pd\nfrom siuba import _, mutate\nfrom siuba.siu import call\n\nmy_dates = pd.DataFrame({\"date\": [\"2021-01-01\", \"2021-01-02\"]})\n\npd.to_datetime(my_dates.date)\n\n0   2021-01-01\n1   2021-01-02\nName: date, dtype: datetime64[ns]\n\n\n\nmy_dates >> mutate(parsed = _.date) >> _.parsed\n\n0    2021-01-01\n1    2021-01-02\nName: parsed, dtype: object\n\n\n\nmy_dates >> mutate(parsed = call(pd.to_datetime, _.date))\n\n\n\n\n\n  \n    \n      \n      date\n      parsed\n    \n  \n  \n    \n      0\n      2021-01-01\n      2021-01-01\n    \n    \n      1\n      2021-01-02\n      2021-01-02"
  },
  {
    "objectID": "guide/basics-lazy-expressions.html#common-challenges",
    "href": "guide/basics-lazy-expressions.html#common-challenges",
    "title": "Lazy expressions",
    "section": "Common challenges",
    "text": "Common challenges\n\nReserved words (_.class)\nMost column names can be referred to using _.some_name syntax. However, python reserved words like class can‚Äôt be used in this way.\nUse indexing (e.g.¬†_[\"some_name\"]) to refer to any column by name.\n\n# bad: raises a SyntaxError\n_.class\n\n# good\n_[\"class\"]\n\nMoreover, pandas reserves names for its methods (e.g.¬†_.shape or _.mean). This is also solved by indexing.\n\ndf = pd.DataFrame({\"mean\": [1,2,3]})\n\n# bad: is accessing the mean method\ndf.mean + 1\n\n# good (pandas)\ndf[\"mean\"]\n\n# good (siuba)\n_[\"mean\"]\n\n\n\nLogical keywords: and, or, in\nIn python libraries like pandas (and numpy), logical comparisons are done using special operators.\nBelow is some example data, along with the operators for logical operations.\n\nimport pandas as pd\n\ndf = pd.DataFrame({\"x\": [2, 3, 4, 5]})\n\n\n\n\npython keyword\npandas\nexample\n\n\n\n\nor\n|\n(df.x < 3) | (df.x > 4)\n\n\nand\n&\n(df.x > 3) & (df.x < 4)\n\n\nin\n.isin()\ndf.x.isin([3, 4, 5])\n\n\n\n\n\nGoogle colab overrides _\nGoogle colab uses very old versions of the library ipykernel, which has a bug in it. This causes it to continuously overwrite the _ variable.\nTo fix this, rename the _ variable imported from siuba.\n\nfrom siuba import _ as D, filter\nfrom siuba.data import mtcars\n\nmtcars >> filter(D.mpg > 30)\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      17\n      32.4\n      4\n      78.7\n      66\n      4.08\n      2.200\n      19.47\n      1\n      1\n      4\n      1\n    \n    \n      18\n      30.4\n      4\n      75.7\n      52\n      4.93\n      1.615\n      18.52\n      1\n      1\n      4\n      2\n    \n    \n      19\n      33.9\n      4\n      71.1\n      65\n      4.22\n      1.835\n      19.90\n      1\n      1\n      4\n      1\n    \n    \n      27\n      30.4\n      4\n      95.1\n      113\n      3.77\n      1.513\n      16.90\n      1\n      1\n      5\n      2"
  },
  {
    "objectID": "guide/basics-sql.html",
    "href": "guide/basics-sql.html",
    "title": "SQL basics",
    "section": "",
    "text": "Up to this point we‚Äôve covered lazy expressions (_), and using table verbs. A major benefit of these two approaches is that they allow us to change how siuba behaves depending on the data source on which it is operating."
  },
  {
    "objectID": "guide/basics-sql.html#setup",
    "href": "guide/basics-sql.html#setup",
    "title": "SQL basics",
    "section": "Setup",
    "text": "Setup\nFor these examples we first set up a sqlite database, with an mtcars table.\n\nfrom sqlalchemy import create_engine\nfrom siuba.sql import LazyTbl\nfrom siuba import _, group_by, summarize, show_query, collect \nfrom siuba.data import mtcars\n\n# copy in to sqlite, using the pandas .to_sql() method\nengine = create_engine(\"sqlite:///:memory:\")\nmtcars.to_sql(\"mtcars\", engine, if_exists = \"replace\")\n\n32"
  },
  {
    "objectID": "guide/basics-sql.html#accessing-tables",
    "href": "guide/basics-sql.html#accessing-tables",
    "title": "SQL basics",
    "section": "Accessing tables",
    "text": "Accessing tables\nUse the LazyTbl class to connect to a SQL table. Printing the table will show a preview of the first few rows.\n\n# Create a lazy SQL DataFrame\ntbl_mtcars = LazyTbl(engine, \"mtcars\")\ntbl_mtcars\n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n  \n    \n      \n      index\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      0\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      1\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      2\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      3\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      4\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n# .. may have more rows\n\n\nNotice that we defined the variable tbl_mtcars to refer to the mtcars table in the database. When we print tbl_mtcars it shows a preview of the underlying data, along with some notes about the database being used: # DB Conn: Engine(sqlite:///:memory:)."
  },
  {
    "objectID": "guide/basics-sql.html#basic-analysis",
    "href": "guide/basics-sql.html#basic-analysis",
    "title": "SQL basics",
    "section": "Basic analysis",
    "text": "Basic analysis\nYou don‚Äôt need to change your analysis code to run it on a SQL table. For example, the code below groups and summarizes the data.\n\n# connect with siuba\n\ntbl_query = (tbl_mtcars\n  >> group_by(_.cyl)\n  >> summarize(avg_hp = _.hp.mean())\n  )\n\ntbl_query\n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n  \n    \n      \n      cyl\n      avg_hp\n    \n  \n  \n    \n      0\n      4\n      82.636364\n    \n    \n      1\n      6\n      122.285714\n    \n    \n      2\n      8\n      209.214286\n    \n  \n\n# .. may have more rows\n\n\nUnder the hood, functions like summarize know how to convert the lazy expressions like _.hp.mean() shown in the code above to SQL."
  },
  {
    "objectID": "guide/basics-sql.html#show-query",
    "href": "guide/basics-sql.html#show-query",
    "title": "SQL basics",
    "section": "Show query",
    "text": "Show query\nBy default, printing out a LazyTbl shows a preview of the data. Use show_query() to see the actual SQL query siuba will generate.\n\nq = tbl_query >> show_query()\n\nSELECT mtcars.cyl, avg(mtcars.hp) AS avg_hp \nFROM mtcars GROUP BY mtcars.cyl"
  },
  {
    "objectID": "guide/basics-sql.html#collect-to-dataframe",
    "href": "guide/basics-sql.html#collect-to-dataframe",
    "title": "SQL basics",
    "section": "Collect to DataFrame",
    "text": "Collect to DataFrame\nUse collect() to fetch the full query results as a pandas DataFrame.\n\ntbl_query >> collect()\n\n\n\n\n\n  \n    \n      \n      cyl\n      avg_hp\n    \n  \n  \n    \n      0\n      4\n      82.636364\n    \n    \n      1\n      6\n      122.285714\n    \n    \n      2\n      8\n      209.214286"
  },
  {
    "objectID": "guide/basics-table-verbs.html",
    "href": "guide/basics-table-verbs.html",
    "title": "Table verbs",
    "section": "",
    "text": "Table verbs take one or more tables as input, and return a table as output."
  },
  {
    "objectID": "guide/basics-table-verbs.html#syntax",
    "href": "guide/basics-table-verbs.html#syntax",
    "title": "Table verbs",
    "section": "Syntax",
    "text": "Syntax\n\n\n\n# preferred: pipe data to verb\nmtcars >> count(_.cyl)\n\n\n\n\n\n  \n    \n      \n      cyl\n      n\n    \n  \n  \n    \n      0\n      4\n      11\n    \n    \n      1\n      6\n      7\n    \n    \n      2\n      8\n      14\n    \n  \n\n\n\n\n\n# call directly\ncount(mtcars, _.cyl)"
  },
  {
    "objectID": "guide/basics-table-verbs.html#verbs-using-tidyselection",
    "href": "guide/basics-table-verbs.html#verbs-using-tidyselection",
    "title": "Table verbs",
    "section": "Verbs using tidyselection",
    "text": "Verbs using tidyselection\nSome verbs‚Äîlike select() for keeping specific columns‚Äîuse a special syntax called tidyselection. This syntax can be thought of as a mini-language for specifying a set of columns, either by inclusion or exclusion.\n\nSiubaPandas\n\n\n\n# keep cyl column\nmtcars >> select(_.cyl)\n\n# keep all *except* cyl column\nmtcars >> select(-_.cyl)\n\n# complex select, plus rename cyl to cylinder\nmtcars >> select(_.cylinder == _.cyl, _.startswith(\"m\"))\n\n\n\n\n# keep cyl column\nmtcars[[\"cyl\"]]\n\n# keep all *except* cyl column\nmtcars.drop([\"cyl\"], axis=1)\n\n# complex select, plus rename cyl to cylinder\ncols = mtcars.columns\nmtcars.loc[:, (cols == \"cyl\") | cols.str.startswith(\"m\")] \\\n      .rename({\"cyl\": \"cylinder\"})\n\n\n\n\nMore options for tidyselection exist, such as matching patterns, or slicing. See the select columns page for a discussion of all tidyselect options."
  },
  {
    "objectID": "guide/basics-verbs-ops-expr.html",
    "href": "guide/basics-verbs-ops-expr.html",
    "title": "Verbs and Column Operations",
    "section": "",
    "text": "Table verbs take one or more tables as input, and return a table as output."
  },
  {
    "objectID": "guide/basics-verbs-ops-expr.html#syntax",
    "href": "guide/basics-verbs-ops-expr.html#syntax",
    "title": "Verbs and Column Operations",
    "section": "Syntax",
    "text": "Syntax\n\n\n\n# preferred: pipe data to verb\nmtcars >> count(_.cyl)\n\n\n\n\n\n  \n    \n      \n      cyl\n      n\n    \n  \n  \n    \n      0\n      4\n      11\n    \n    \n      1\n      6\n      7\n    \n    \n      2\n      8\n      14\n    \n  \n\n\n\n\n\n# call directly\ncount(mtcars, _.cyl)"
  },
  {
    "objectID": "guide/basics-verbs-ops-expr.html#verbs-using-tidyselection",
    "href": "guide/basics-verbs-ops-expr.html#verbs-using-tidyselection",
    "title": "Verbs and Column Operations",
    "section": "Verbs using tidyselection",
    "text": "Verbs using tidyselection\nSome verbs‚Äîlike select() for keeping specific columns‚Äîuse a special syntax called tidyselection. This syntax can be thought of as a mini-language for specifying a set of columns, either by inclusion or exclusion.\n\nSiubaPandas\n\n\n\n# keep cyl column\nmtcars >> select(_.cyl)\n\n# keep all *except* cyl column\nmtcars >> select(-_.cyl)\n\n# complex select, plus rename cyl to cylinder\nmtcars >> select(_.cylinder == _.cyl, _.startswith(\"m\"))\n\n\n\n\n# keep cyl column\nmtcars[[\"cyl\"]]\n\n# keep all *except* cyl column\nmtcars.drop([\"cyl\"], axis=1)\n\n# complex select, plus rename cyl to cylinder\ncols = mtcars.columns\nmtcars.loc[:, (cols == \"cyl\") | cols.str.startswith(\"m\")] \\\n      .rename({\"cyl\": \"cylinder\"})\n\n\n\n\nMore options for tidyselection exist, such as matching patterns, or slicing. See the select columns page for a discussion of all tidyselect options."
  },
  {
    "objectID": "guide/basics-verbs-ops-expr.html#pipe-to-dataframe-methods",
    "href": "guide/basics-verbs-ops-expr.html#pipe-to-dataframe-methods",
    "title": "Verbs and Column Operations",
    "section": "Pipe to DataFrame methods",
    "text": "Pipe to DataFrame methods\nSometimes it is helpful to use Pandas DataFrame methods, in addition to siuba verbs. This can be done by piping the data to _.<some_method>().\nHere is an example using the siuba verb count(), with the pandas method .sort_values().\n\n(mtcars\n    >> count(_.cyl)         # this is a siuba verb\n    >> _.sort_values(\"n\")   # this is a pandas method\n)\n\n\n\n\n\n  \n    \n      \n      cyl\n      n\n    \n  \n  \n    \n      1\n      6\n      7\n    \n    \n      0\n      4\n      11\n    \n    \n      2\n      8\n      14\n    \n  \n\n\n\n\nHere is another example, using the DataFrame .shape attribute.\n\n\n\n# siuba pipe\nmtcars >> _.shape[0]\n\n32\n\n\n\n# regular pandas\nmtcars.shape[0]"
  },
  {
    "objectID": "guide/basics-verbs-ops-expr.html#call-external-functions",
    "href": "guide/basics-verbs-ops-expr.html#call-external-functions",
    "title": "Verbs and Column Operations",
    "section": "Call external functions",
    "text": "Call external functions\nA major advantage of using the pipe approach is that you can pipe any object (e.g.¬†a DataFrame) to any function, using call().\nThe example below pipes to the seaborn‚Äôs barplot function.\n\nfrom siuba.siu import call\nimport seaborn as sns\n\nmtcars >> count(_.cyl) >> call(sns.barplot, x=\"cyl\", y=\"n\", data=_)\n\n<AxesSubplot: xlabel='cyl', ylabel='n'>\n\n\n\n\n\nNote that sns.barplot() expects the data as a named argument, so we pass data=_, where _ is a placeholder for the data.\ncall() can also take a single function to call the data on.\n\n\n\n# piping\nmtcars >> call(len)\n\n32\n\n\n\n# regular function call\nlen(mtcars)\n\n32"
  },
  {
    "objectID": "guide/basics-verbs-ops-expr.html#learning-more",
    "href": "guide/basics-verbs-ops-expr.html#learning-more",
    "title": "Verbs and Column Operations",
    "section": "Learning more",
    "text": "Learning more\n\ncommon table verbs section\ncustom verbs\nflexible pipes"
  },
  {
    "objectID": "guide/core-concepts.html",
    "href": "guide/core-concepts.html",
    "title": "Core concepts",
    "section": "",
    "text": "core\n\nsiu expression\nverb\ncolumn operation\n\ncomposition\n\npipe\nsingledispatch\ntranslator\ndata backend\nnested data\n\nprogramming\n\nacross\nover\n\nextension\n\nverb_dispatch\nop_dispatch"
  },
  {
    "objectID": "guide/extra-r-to-python.html",
    "href": "guide/extra-r-to-python.html",
    "title": "R to Python",
    "section": "",
    "text": "Pandas allows you to slice all strings in a Series, but does not allow you to apply custom slices to each string (a la stringr::str_sub). This means there is no easy equivalent to using results from stringr::str_locate to subset strings.\nWhile most Pandas string methods are under the .str accessor, the ones for ordering are not. To stringr::str_order() and stringr::str_sort(), use .argsort() and .sort_values().\nstringr has an *_all() variant on several functions (e.g.¬†str_replace, str_locate, str_extract, str_match). Pandas generally has equivalent behavior, but it is sometimes specified by using an alternative method (e.g.¬†str.extractall()), and sometimes by using an argument (e.g.¬†str_replace(..., n = 1)).\nPandas string methods are modeled after python str object methods AND stringr (This is mentioned in the .str accessor source code). However, it‚Äôs not always clear what accepts a regex (similar to stringr) and what does not (similr to str object methods).\nFor example, .str.count() only accepts a regex. str.startswith() does not. Other methods like str.contains() accept a regex by default, but this can be disabled using the regex argument.\nThis is not a big issue in practice, but warrants some caution / teaching strategy."
  },
  {
    "objectID": "guide/ops-autocomplete.html",
    "href": "guide/ops-autocomplete.html",
    "title": "Siuba",
    "section": "",
    "text": "import pandas as pd\n\npd.set_option(\"display.max_rows\", 5)\n\nfrom siuba.data import penguins\nfrom siuba import _, summarize, group_by"
  },
  {
    "objectID": "guide/ops-case-when.html",
    "href": "guide/ops-case-when.html",
    "title": "Conditionals (if_else)",
    "section": "",
    "text": "from siuba.data import penguins\nfrom siuba import _, summarize, group_by, if_else, transmute, case_when\n\npenguins\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      male\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      female\n      2007\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      342\n      Chinstrap\n      Dream\n      50.8\n      19.0\n      210.0\n      4100.0\n      male\n      2009\n    \n    \n      343\n      Chinstrap\n      Dream\n      50.2\n      18.7\n      198.0\n      3775.0\n      female\n      2009\n    \n  \n\n344 rows √ó 8 columns"
  },
  {
    "objectID": "guide/ops-case-when.html#if_else-for-two-cases",
    "href": "guide/ops-case-when.html#if_else-for-two-cases",
    "title": "Conditionals (if_else)",
    "section": "if_else for two cases",
    "text": "if_else for two cases\nUse the if_else() when values depend only on two cases‚Äîlike whether some condition is True or False. This is similar to a Python if else statement, but applies to each value in a column.\n\nBasics\n\nif_else(penguins.bill_length_mm > 40, \"long\", \"short\")\n\n0      short\n1      short\n       ...  \n342     long\n343     long\nLength: 344, dtype: object\n\n\n\n\nUse in a verb\n\ntransmute(\n    penguins,\n    bill_length = if_else(_.bill_length_mm > 40, \"long\", \"short\")\n)\n\n\n\n\n\n  \n    \n      \n      bill_length\n    \n  \n  \n    \n      0\n      short\n    \n    \n      1\n      short\n    \n    \n      ...\n      ...\n    \n    \n      342\n      long\n    \n    \n      343\n      long\n    \n  \n\n344 rows √ó 1 columns"
  },
  {
    "objectID": "guide/ops-case-when.html#case_when-for-many-cases",
    "href": "guide/ops-case-when.html#case_when-for-many-cases",
    "title": "Conditionals (if_else)",
    "section": "case_when for many cases",
    "text": "case_when for many cases\nThe case_when() function is a more general version of if_else(). It lets you check as many cases as you want, and map them to resulting values.\n\nBasics\n\ncase_when(penguins, {\n    _.bill_depth_mm <= 18: \"short\",\n    _.bill_depth_mm <= 19: \"medium\",\n    _.bill_depth_mm > 19: \"long\"\n})\n\n0      medium\n1       short\n        ...  \n342    medium\n343    medium\nLength: 344, dtype: object\n\n\n\n\nUse in a verb\n\n# also works\npenguins >> case_when({ ... })\n\n\n\nSet default when no match\nUse a True as the final case, in order to set a value when no other cases match.\n\ncase_when(penguins, {\n    _.bill_depth_mm.between(18, 19): \"medium\",\n    True: \"OTHER\"\n})\n\n0      medium\n1       OTHER\n        ...  \n342    medium\n343    medium\nLength: 344, dtype: object\n\n\nNote that this works because‚Äîfor each value‚Äîcase_when checks for the first matching condition. The final True condition guarantees that it will always be a match."
  },
  {
    "objectID": "guide/ops-categoricals.html",
    "href": "guide/ops-categoricals.html",
    "title": "Categoricals (forcats)",
    "section": "",
    "text": "Categoricals are a way of representing columns of data, that provide:\nWhile codes were historically important for representing large columns of data, the big value of categoricals today is as a tool for customizing order in plots. This might seem like a small job, but as it turns out it is very important in data analysis.\nThis page will discuss the pandas.Categorical class for creating categoricals, as well as a helper submodule siuba.dply.forcats with helper functions for working with this kind of data."
  },
  {
    "objectID": "guide/ops-categoricals.html#required-datasets",
    "href": "guide/ops-categoricals.html#required-datasets",
    "title": "Categoricals (forcats)",
    "section": "Required datasets",
    "text": "Required datasets\nThis page uses the nycflights13 dataset, which can be installed using pip:\npip install nycflights13"
  },
  {
    "objectID": "guide/ops-categoricals.html#overview",
    "href": "guide/ops-categoricals.html#overview",
    "title": "Categoricals (forcats)",
    "section": "Overview",
    "text": "Overview\nHere is a simple pd.Categorical representing a column with 3 values.\n\nx = [\"front\", \"middle\", \"back\"]\n\na_cat = pd.Categorical(x)\na_cat\n\n['front', 'middle', 'back']\nCategories (3, object): ['back', 'front', 'middle']\n\n\nNotice that the bottom line of the print out shows the categories ordered as ['back', 'front', 'middle']. By default, pd.Categorical categories are in (roughly) alphabetical order. Ideally, we‚Äôd have them in an order like front, middle, back!\nOne way to do this, is to use fct_inorder() to order by first observed first, second observed second, etc..\n\nfct_inorder(x)\n\n['front', 'middle', 'back']\nCategories (3, object): ['front', 'middle', 'back']\n\n\nThe remaining sections will focus on two kinds of categorical helper functions:\n\nfunctions for ordering category levels.\nfunctions for grouping categories together.\n\nHowever, before we do that, let‚Äôs go through a few useful ways to interact with categoricals.\n\nCore attributes\n\na_cat.codes\n\narray([1, 2, 0], dtype=int8)\n\n\n\na_cat.categories\n\nIndex(['back', 'front', 'middle'], dtype='object')\n\n\n\na_cat.ordered\n\nFalse\n\n\n\n\nWrapping in pd.Series\npandas often wraps categoricals in a Series object.\n\na_cat2 = pd.Categorical([\"b\", \"a\", \"c\"])\nser = pd.Series(a_cat2)\nser\n\n0    b\n1    a\n2    c\ndtype: category\nCategories (3, object): ['a', 'b', 'c']\n\n\nFor example, any time you create a DataFrame column out of a categorical, it gets wrapped in a pd.Series.\n\ndf = pd.DataFrame({\"some_cat\": a_cat2})\n\nprint(type(df.some_cat))\ndf.some_cat\n\n<class 'pandas.core.series.Series'>\n\n\n0    b\n1    a\n2    c\nName: some_cat, dtype: category\nCategories (3, object): ['a', 'b', 'c']\n\n\n\n\n\n\n\n\nNote\n\n\n\n99% of the time when doing data analysis, your categorical is wrapped in a Series.\n\n\nNote that accessor methods like .str.upper() are available on the series, and the underlying category attributes are availble using the .cat accessor.\n\nser.str.upper()\n\n0    B\n1    A\n2    C\ndtype: object\n\n\n\nser.cat.codes\n\n0    1\n1    0\n2    2\ndtype: int8\n\n\nYou can get the underlying categorical out using the .array property.\n\nser.array\n\n['b', 'a', 'c']\nCategories (3, object): ['a', 'b', 'c']\n\n\n\n\nUsing in verbs\nThe functions in siuba.dplyr.forcats can be used with lazy expressions.\n\nfct_inorder(_.species)\n\n‚ñà‚îÄ'__call__'\n‚îú‚îÄ‚ñà‚îÄ'__custom_func__'\n‚îÇ ‚îî‚îÄ<function fct_inorder at 0x7fe0fd961790>\n‚îî‚îÄ‚ñà‚îÄ.\n  ‚îú‚îÄ_\n  ‚îî‚îÄ'species'\n\n\nNote how the above output is a lazy expression, which can be used inside verbs like mutate():\n\n(penguins\n    >> mutate(\n        species_cat = fct_inorder(_.species),\n        species_cat2 = _.species.astype(\"category\"),\n    )\n)\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n      sex\n      year\n      species_cat\n      species_cat2\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n      3750.0\n      male\n      2007\n      Adelie\n      Adelie\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n      3800.0\n      female\n      2007\n      Adelie\n      Adelie\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      342\n      Chinstrap\n      Dream\n      50.8\n      19.0\n      210.0\n      4100.0\n      male\n      2009\n      Chinstrap\n      Chinstrap\n    \n    \n      343\n      Chinstrap\n      Dream\n      50.2\n      18.7\n      198.0\n      3775.0\n      female\n      2009\n      Chinstrap\n      Chinstrap\n    \n  \n\n344 rows √ó 10 columns"
  },
  {
    "objectID": "guide/ops-categoricals.html#order-categories-by-counts-with-fct_infreq",
    "href": "guide/ops-categoricals.html#order-categories-by-counts-with-fct_infreq",
    "title": "Categoricals (forcats)",
    "section": "Order categories by counts with fct_infreq()",
    "text": "Order categories by counts with fct_infreq()\nUse fct_infreq to order category levels by their frequency in the data.\n\nfct_infreq(penguins.species)\n\n0         Adelie\n1         Adelie\n         ...    \n342    Chinstrap\n343    Chinstrap\nLength: 344, dtype: category\nCategories (3, object): ['Adelie', 'Gentoo', 'Chinstrap']\n\n\nIn the output above, the category ordering shows us that ‚ÄúAdelie‚Äù is most frequent in the data, followed by ‚ÄúGentoo‚Äù, and then ‚ÄúChinstrap‚Äù.\nWe can verify this explicitly by using the verb count() to tally up each species.\n\ntbl_species_count = penguins >> count(_.species)\n\ntbl_species_count\n\n\n\n\n\n  \n    \n      \n      species\n      n\n    \n  \n  \n    \n      0\n      Adelie\n      152\n    \n    \n      1\n      Chinstrap\n      68\n    \n    \n      2\n      Gentoo\n      124\n    \n  \n\n\n\n\nOrdering by frequency is helpful for giving viewers a rough sense for which groups have less data in your plots.\nFor example, the code below plots each species on the x-axis, bill_depth_mm on the y-axis. It orders the categories of species by frequency, so those with the most data are shown on the left.\n\nfrom plotnine import ggplot, aes, geom_point, position_jitter\n\n(penguins\n    >> mutate(species = fct_infreq(_.species))\n    >> ggplot(aes(\"species\", \"bill_depth_mm\"))\n    + geom_point(position=position_jitter(width=.1, height=0))\n)\n\n/opt/hostedtoolcache/Python/3.9.15/x64/lib/python3.9/site-packages/plotnine/layer.py:411: PlotnineWarning: geom_point : Removed 2 rows containing missing values.\n\n\n\n\n\n<ggplot: (8787751248261)>\n\n\nNote that the position_jitter(width=.1, height=0) tells the plot to randomly adjust the width of each point between +-.1 (where the distance between each species label is 1)."
  },
  {
    "objectID": "guide/ops-categoricals.html#general-reordering-with-fct_reorder",
    "href": "guide/ops-categoricals.html#general-reordering-with-fct_reorder",
    "title": "Categoricals (forcats)",
    "section": "General reordering with fct_reorder()",
    "text": "General reordering with fct_reorder()\nUse fct_reorder() to reorder the categories of a column, based on another column.\nThis function takes 3 main arguments:\n\nA column to copy and return with reordered categories.\nA column used to calculate the new ordering.\nAn optional function that performs a calculation (defaults to calculating the median).\n\nFor example, the code below reorders the categories of the species column.\n\nfct_reorder(penguins.species, penguins.bill_depth_mm, \"mean\")\n\n0         Adelie\n1         Adelie\n         ...    \n342    Chinstrap\n343    Chinstrap\nLength: 344, dtype: category\nCategories (3, object): ['Gentoo', 'Adelie', 'Chinstrap']\n\n\nNote that it reorders species based on the mean of bill_depth_mm within each category.\n\n\n\n\n\n\nNote\n\n\n\nCurrently, the easiest way to specify a calculation is by passing a string, like \"mean\". Under the hood, fct_reorder() calls pd.Series.agg(), so you could also pass a lambda or function directly.\n\n\n\nBasic example\nThe code below reorders species using the default function (‚Äúmedian‚Äù) over bill_depth_mm. This results in boxplots are ordered from lowest to highest median.\n\nfrom plotnine import ggplot, aes, geom_boxplot\n\n(penguins\n    >> mutate(species = fct_reorder(_.species, _.bill_depth_mm))\n    >> ggplot(aes(\"species\", \"bill_depth_mm\"))\n    + geom_boxplot()\n)\n\n/opt/hostedtoolcache/Python/3.9.15/x64/lib/python3.9/site-packages/plotnine/layer.py:333: PlotnineWarning: stat_boxplot : Removed 2 rows containing non-finite values.\n\n\n\n\n\n<ggplot: (8787748265692)>\n\n\n\n\nUsed with count\nA common use for fct_reorder is to reorder a rolled up count.\nFor example, the code below counts the number of rows per species.\n\ntbl_penguin_species = penguins >> count(_.species)\ntbl_penguin_species\n\n\n\n\n\n  \n    \n      \n      species\n      n\n    \n  \n  \n    \n      0\n      Adelie\n      152\n    \n    \n      1\n      Chinstrap\n      68\n    \n    \n      2\n      Gentoo\n      124\n    \n  \n\n\n\n\nSuppose we had a table like this one, we might want to reorder the categories based on the n column.\n\nfct_reorder(tbl_penguin_species.species, tbl_penguin_species.n, desc=True)\n\n0       Adelie\n1    Chinstrap\n2       Gentoo\ndtype: category\nCategories (3, object): ['Adelie', 'Gentoo', 'Chinstrap']\n\n\nNote that above we used the desc=True argument to put the categories in descending order. Because there is only entry per category level, the default function (‚Äúmedian‚Äù) just returns that value or n. This results in categories ordered by n.\nHere is the same calculation used to reorder the bars on a plot.\n\nfrom plotnine import ggplot, aes, geom_col\n\n(tbl_penguin_species\n    >> mutate(species = fct_reorder(_.species, _.n, desc=True))\n    >> ggplot(aes(\"species\", \"n\"))\n    + geom_col()\n)\n\n\n\n\n<ggplot: (8787751146186)>"
  },
  {
    "objectID": "guide/ops-categoricals.html#binning-categories-with-fct_lump",
    "href": "guide/ops-categoricals.html#binning-categories-with-fct_lump",
    "title": "Categoricals (forcats)",
    "section": "Binning categories with fct_lump()",
    "text": "Binning categories with fct_lump()\nWhile functions like fct_infreq() and fct_reorder() change the order of categories, functions like fct_lump() reduce the number of categories.\nUse fct_lump() to lump categories with fewer observations into a single category (e.g.¬†‚ÄúOther‚Äù).\n\nBasic example\nFor example, let‚Äôs look at the nycflights13 table flights.\n\nfrom nycflights13 import flights\n\nflights\n\n\n\n\n\n  \n    \n      \n      year\n      month\n      day\n      dep_time\n      sched_dep_time\n      dep_delay\n      arr_time\n      sched_arr_time\n      arr_delay\n      carrier\n      flight\n      tailnum\n      origin\n      dest\n      air_time\n      distance\n      hour\n      minute\n      time_hour\n    \n  \n  \n    \n      0\n      2013\n      1\n      1\n      517.0\n      515\n      2.0\n      830.0\n      819\n      11.0\n      UA\n      1545\n      N14228\n      EWR\n      IAH\n      227.0\n      1400\n      5\n      15\n      2013-01-01T10:00:00Z\n    \n    \n      1\n      2013\n      1\n      1\n      533.0\n      529\n      4.0\n      850.0\n      830\n      20.0\n      UA\n      1714\n      N24211\n      LGA\n      IAH\n      227.0\n      1416\n      5\n      29\n      2013-01-01T10:00:00Z\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      336774\n      2013\n      9\n      30\n      NaN\n      1159\n      NaN\n      NaN\n      1344\n      NaN\n      MQ\n      3572\n      N511MQ\n      LGA\n      CLE\n      NaN\n      419\n      11\n      59\n      2013-09-30T15:00:00Z\n    \n    \n      336775\n      2013\n      9\n      30\n      NaN\n      840\n      NaN\n      NaN\n      1020\n      NaN\n      MQ\n      3531\n      N839MQ\n      LGA\n      RDU\n      NaN\n      431\n      8\n      40\n      2013-09-30T12:00:00Z\n    \n  \n\n336776 rows √ó 19 columns\n\n\n\nThis table has a column for carrier that lists each agency running flights. We can use the verb count() to quickly see how many unique carriers there are, and get a feel for how many flights each has run.\n\ntbl_carrier_counts = flights >> count(_.carrier, sort=True)\ntbl_carrier_counts\n\n\n\n\n\n  \n    \n      \n      carrier\n      n\n    \n  \n  \n    \n      0\n      UA\n      58665\n    \n    \n      1\n      B6\n      54635\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      14\n      HA\n      342\n    \n    \n      15\n      OO\n      32\n    \n  \n\n16 rows √ó 2 columns\n\n\n\nNotice two pieces:\n\nThere are 16 rows, so 16 carriers\nThere is a big difference between the number of \"UA\" and \"OO\" flights (58,665 vs 32).\n\nLet‚Äôs use fct_lump() to keep only the 7 biggest carriers, and relable the rest to ‚ÄúOther‚Äù.\n\nfct_lump(tbl_carrier_counts.carrier, w=tbl_carrier_counts.n, n=7)\n\n0        UA\n1        B6\n      ...  \n14    Other\n15    Other\nLength: 16, dtype: category\nCategories (8, object): ['AA', 'B6', 'DL', 'EV', 'MQ', 'UA', 'US', 'Other']\n\n\nIn the code above, we told fct_lump() to lump categories for carrier, weighted by the n column, and resulting in n=7 of the original groups.\nHere‚Äôs an example using the above code to order a barchart.\n\nfrom plotnine import ggplot, aes, geom_col\n\n(tbl_carrier_counts\n    >> mutate(binned = fct_lump(_.carrier, w=_.n, n=7))\n    >> ggplot(aes(\"binned\", \"n\", fill=\"carrier\")) \n    + geom_col()\n)\n\n\n\n\n<ggplot: (8787817900475)>\n\n\nNotice that all of the smaller carriers are grouped into the ‚ÄúOther‚Äù bar.\nThis plot looks okay, but there are two limitations:\n\nThe first bar on the left is ‚ÄúAA‚Äù, but the color legend is in alphabetical order, so starts with ‚Äú9E‚Äù. It would be nice if the legend were in the same order as the bars.\nThe bars themselves are not ordered by frequency.\n\nWe‚Äôll tackle these pieces in the section below.\n\n\nRespecting category order\nfct_lump() preserves existing category order. This enables you to order categories before collapsing them down.\n\nfrom plotnine import ggplot, aes, geom_col\n\n(tbl_carrier_counts\n    >> mutate(carrier = fct_inorder(_.carrier))\n    >> mutate(binned = fct_lump(_.carrier, w=_.n, n=7))\n    >> ggplot(aes(\"binned\", \"n\", fill=\"carrier\")) \n    + geom_col()\n)\n\n\n\n\n<ggplot: (8787748402583)>"
  },
  {
    "objectID": "guide/ops-datetime.html",
    "href": "guide/ops-datetime.html",
    "title": "Datetime operations (.dt)",
    "section": "",
    "text": "Warning\n\n\n\nThis section is in the draft phase."
  },
  {
    "objectID": "guide/ops-datetime.html#overview",
    "href": "guide/ops-datetime.html#overview",
    "title": "Datetime operations (.dt)",
    "section": "Overview",
    "text": "Overview\nThis page covers how to work with dates and times in siuba. siuba works by using pandas methods, either by calling them directly, or translating them to SQL.\n\nimport pandas as pd\n\nfrom siuba import _, count, mutate\nfrom siuba.siu import call\n\n\nUsing datetime methods (.dt)\nsiuba uses pandas methods, so can use any of the datetime methods it makes available, like .dt.month_name().\n\ndf_dates = pd.DataFrame({\n    \"dates\": pd.to_datetime([\"2021-01-02\", \"2021-02-03\"]),\n    \"raw\": [\"2023-04-05 06:07:08\", \"2024-05-06 07:08:09\"],\n})\n\n\ndf_dates.dates.dt.month_name()\n\n0     January\n1    February\nName: dates, dtype: object\n\n\n\n\nUse in a verb\n\ndf_dates >> count(month = _.dates.dt.month_name())\n\n\n\n\n\n  \n    \n      \n      month\n      n\n    \n  \n  \n    \n      0\n      February\n      1\n    \n    \n      1\n      January\n      1\n    \n  \n\n\n\n\nYou can call functions like pd.to_datetime() using siuba‚Äôs call().\n\nres = df_dates >> mutate(parsed = call(pd.to_datetime, _.raw))\n\nres.parsed\n\n0   2023-04-05 06:07:08\n1   2024-05-06 07:08:09\nName: parsed, dtype: datetime64[ns]\n\n\nNotice that this creates a new datetime column by calling pd.to_datetime(df.raw).\n\n\nCreating datetime columns\nThere are roughly two ways to create a datetime column in pandas:\n\npd.to_datetime() which takes a range of inputs.\npd.Series.astype(\"datetime64[ns]\") method call.\n\nThe pd.to_datetime() function is flexible, and can also take a list of datetimes or a Series.\n\ndt_index = pd.to_datetime([\"2021-01-02 01:02:03\", \"2022-02-03 04:05:06\"])\ndt_index\n\nDatetimeIndex(['2021-01-02 01:02:03', '2022-02-03 04:05:06'], dtype='datetime64[ns]', freq=None)\n\n\nNote that sometimes the result is not a Series. For example, the above object is a DatetimeIndex. Generally, everything is easier after wrapping it in a pandas Series.\n\nser_times = pd.Series(dt_index)\n\nOn the other hand, the .astype() method is a simple way to convert a series to a datetime.\n\npd.Series([\"2021-01-02 03:04:05\"]).astype(\"datetime64[ns]\")\n\n0   2021-01-02 03:04:05\ndtype: datetime64[ns]\n\n\nThe pandas time series docs discuss in exquisite detail the intricacies of different datetime objects, and how they‚Äôre created! The rest of this page will just use pandas Series to look at datetime operations."
  },
  {
    "objectID": "guide/ops-datetime.html#change-granularity-with-floor_date",
    "href": "guide/ops-datetime.html#change-granularity-with-floor_date",
    "title": "Datetime operations (.dt)",
    "section": "Change granularity with floor_date()",
    "text": "Change granularity with floor_date()\nSiuba has an experimental function called floor_date() for rounding down to a specific unit of time (e.g.¬†the week, the day, or the hour).\n\nimport pandas as pd\n\nfrom siuba.experimental.datetime import floor_date, ceil_date\n\ndates = pd.to_datetime([\"2021-01-01 01:02:03\", \"2021-01-08 01:02:03\"])\n\n\nBasics\nThe floor_date() functions takes two arguments:\n\nA column to round down (e.g.¬†a pandas Series).\nA datetime unit to round to (e.g.¬†‚ÄúMS‚Äù for ‚ÄúMonth Start‚Äù; see the pandas unit alias doc)\n\nFor example, the code below rounds dates down to the nearest week.\n\nfloor_date(dates, \"W\")\n\nDatetimeIndex(['2021-01-01', '2021-01-08'], dtype='datetime64[ns]', freq=None)\n\n\n\n\nDatetime unit options\nThere are a lot of useful time units, such as ‚ÄúMS‚Äù for the start of a month. Below is a table of some of the most useful ones.\n\n\n\nhuman speak\npandas alias\n\n\n\n\nsecond\nS\n\n\nminute\nM\n\n\nhour\nH\n\n\nday\nD\n\n\nweek\nW\n\n\nmonth\nM\n\n\nbimonth\n2M\n\n\nquarter\nQ\n\n\nseason\n\n\n\nhalfyear\n\n\n\nyear\nY\n\n\n\n\n# month start\nfloor_date(dates, \"MS\")\n\nDatetimeIndex(['2021-01-01', '2021-01-01'], dtype='datetime64[ns]', freq=None)\n\n\n\n\nRound up with ceil_date()\nThe counterpart function ceil_date() rounds up to the specified unit of time.\n\n# round up to month end\nceil_date(dates, \"M\")\n\nDatetimeIndex(['2021-01-31', '2021-01-31'], dtype='datetime64[ns]', freq=None)\n\n\n\n\nPreserving input type\nNote that pandas has multiple formats for representing datetime:\n\nTimestamp: directly representing points in time.\nPeriod: representing time as number of spans of some time unit from a reference point (e.g.¬†120 months from Jan, 1970).\n\n\nx = [\"2021-01-01 01:02:03\", \"2021-02-03 01:02:03\"]\n\ndt_index = pd.DatetimeIndex(x)\nfloor_date(dt_index, \"W\")\n\nDatetimeIndex(['2021-01-01', '2021-02-03'], dtype='datetime64[ns]', freq=None)\n\n\n\n# note freq=\"m\" refers to minute frequency\nper_index = pd.PeriodIndex(x, freq=\"m\")\nfloor_date(per_index, \"W\")\n\nPeriodIndex(['2020-12-28/2021-01-03', '2021-02-01/2021-02-07'], dtype='period[W-SUN]')\n\n\nNote that the \"W\" stands for week.\n\n\nUnit start vs unit end\nNote that the units we discussed here all referred to ‚Äúunit start‚Äù. This is a bit tricky, so might be explained best in an example using month units.\n\nmonth start: ‚Äú2021-02-03‚Äù becomes ‚Äú2021-02-01‚Äù\nmonth end: ‚Äú2021-02-03‚Äù becomes ‚Äú2021-01-31‚Äù\n\nFor most time units pandas has a unit end version, such as:\n\n‚ÄúM‚Äù: month end (vs ‚ÄúMS‚Äù for month start)\n‚ÄúY‚Äù: year end (vs ‚ÄúYS‚Äù for year start)\n‚ÄúQ‚Äù: quarter end (vs ‚ÄúQS‚Äù for quarter start)\n\nIt‚Äôs a bit confusing that ‚ÄúM‚Äù stands for month end, but ‚ÄúD‚Äù stands for day start. In general, time units at the day level or finer grain only do unit start, so be careful with units!"
  },
  {
    "objectID": "guide/ops-datetime.html#learning-more",
    "href": "guide/ops-datetime.html#learning-more",
    "title": "Datetime operations (.dt)",
    "section": "Learning more",
    "text": "Learning more\n\nPandas user guide on timeseries\nPandas Series API on .dt accessor methods"
  },
  {
    "objectID": "guide/ops-siu-expr.html",
    "href": "guide/ops-siu-expr.html",
    "title": "Lazy functions",
    "section": "",
    "text": "A siu expression is a way of specifying what action you want to perform. This allows siuba verbs to decide how to execute the action, depending on whether your data is a local DataFrame or remote table.\nNotice how the output represents each step in our lazy expression, with these pieces:"
  },
  {
    "objectID": "guide/ops-siu-expr.html#lazy-functions",
    "href": "guide/ops-siu-expr.html#lazy-functions",
    "title": "Lazy functions",
    "section": "Lazy functions",
    "text": "Lazy functions\n\nfrom siuba import ops\n\nexpr_n = ops.add(_, _)\nexpr_n\n\n‚ñà‚îÄ'__call__'\n‚îú‚îÄ‚ñà‚îÄ'__custom_func__'\n‚îÇ ‚îî‚îÄ<function singledispatch.<locals>.wrapper at 0x7fde645d3430>\n‚îú‚îÄ_\n‚îî‚îÄ_"
  },
  {
    "objectID": "guide/ops-siu-expr.html#lazy-methods",
    "href": "guide/ops-siu-expr.html#lazy-methods",
    "title": "Lazy functions",
    "section": "Lazy methods",
    "text": "Lazy methods\nThe simplest lazy operation is called a method, which\n\nimport operator as op\n\n_.__getitem__(\"a\")\nop.getitem(_, \"a\")\n_[\"a\"]\n\n‚ñà‚îÄ[\n‚îú‚îÄ_\n‚îî‚îÄ‚ñà‚îÄ'__siu_slice__'\n  ‚îî‚îÄ'a'"
  },
  {
    "objectID": "guide/ops-siu-expr.html#as-a-lambda-shorthand",
    "href": "guide/ops-siu-expr.html#as-a-lambda-shorthand",
    "title": "Lazy functions",
    "section": "As a lambda shorthand",
    "text": "As a lambda shorthand\nWe can use siu expressions like lambda functions. For example, to keep specific rows of a pandas DataFrame.\n\nfrom siuba.data import mtcars\n\n# old approach: repeat name\nmtcars[mtcars.cyl == 4]\n\n# old approach: lambda\nmtcars[lambda _: _.cyl == 4]\n\n# siu approach\nmtcars[_.cyl == 4]\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      2\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      7\n      24.4\n      4\n      146.7\n      62\n      3.69\n      3.190\n      20.00\n      1\n      0\n      4\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      27\n      30.4\n      4\n      95.1\n      113\n      3.77\n      1.513\n      16.90\n      1\n      1\n      5\n      2\n    \n    \n      31\n      21.4\n      4\n      121.0\n      109\n      4.11\n      2.780\n      18.60\n      1\n      1\n      4\n      2\n    \n  \n\n11 rows √ó 11 columns"
  },
  {
    "objectID": "guide/ops-strings.html",
    "href": "guide/ops-strings.html",
    "title": "String operations (str) üìù",
    "section": "",
    "text": "Warning\n\n\n\nThis page is largely complete, but is actively being refined / improved."
  },
  {
    "objectID": "guide/ops-strings.html#overview",
    "href": "guide/ops-strings.html#overview",
    "title": "String operations (str) üìù",
    "section": "Overview",
    "text": "Overview\nString operations allow you to perform actions like:\n\nMatch: detect when a string matches a pattern.\nTransform: e.g.¬†convert something from mIxED to lower case, or replace part of it.\nExtract: grab specific parts of string value (e.g.¬†a matching pattern).\n\nThis page will cover different methods for performing these actions, but will ultimately focus on str.contains(), str.replace(), and str.extract() for common match, transform, and extract tasks.\n\nfrom siuba.data import penguins\nfrom siuba import _, mutate, summarize, group_by, filter\n\nfruits = pd.Series([\n        \"apple\",\n        \"apricot\",\n        \"avocado\",\n        \"banana\",\n        \"bell pepper\"\n])\n\ndf_fruits = pd.DataFrame({\"name\": fruits})\n\n\nUsing string methods\nsiuba uses Pandas methods, so can use any of the string methods it makes available, like .str.upper().\n\nfruits.str.upper()\n\n0          APPLE\n1        APRICOT\n2        AVOCADO\n3         BANANA\n4    BELL PEPPER\ndtype: object\n\n\nNote that most string methods use .str.<method_name>() syntax. These are called ‚Äústring accessor methods‚Äù, since they are accessed from a special place (.str).\n\n\nUsing in verbs\nUse string methods as you would any other methods inside verbs.\n\nmutate(df_fruits, loud = _.name.str.upper())\n\n\n\n\n\n  \n    \n      \n      name\n      loud\n    \n  \n  \n    \n      0\n      apple\n      APPLE\n    \n    \n      1\n      apricot\n      APRICOT\n    \n    \n      2\n      avocado\n      AVOCADO\n    \n    \n      3\n      banana\n      BANANA\n    \n    \n      4\n      bell pepper\n      BELL PEPPER"
  },
  {
    "objectID": "guide/ops-strings.html#matching-patterns",
    "href": "guide/ops-strings.html#matching-patterns",
    "title": "String operations (str) üìù",
    "section": "Matching patterns",
    "text": "Matching patterns\n\nFixed text\nThere are three common approaches for simple string matches:\n\nAn exact match with ==.\nA match from an anchor point, using str.startswith() or str.endswith().\nA match from any point, using str.contains()\n\n\n# exact match\nfruits == \"banana\"\n\n# starts with \"ap\"\nfruits.str.startswith(\"ap\")\n\n# ends with \"cado\"\nfruits.str.endswith(\"cado\")\n\n# has an \"e\" anywhere\nfruits.str.contains(\"e\", regex=False)\n\n0     True\n1    False\n2    False\n3    False\n4     True\ndtype: bool\n\n\nAll these operations return a boolean Series, so can be used to filter rows.\n\nfilter(df_fruits, _.name.str.startswith(\"ap\"))\n\n\n\n\n\n  \n    \n      \n      name\n    \n  \n  \n    \n      0\n      apple\n    \n    \n      1\n      apricot\n    \n  \n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that for str.contains() we set the regex=False argument. This is because‚Äîunlike operations like str.startswith()‚Äîpandas by default assumes you are passing something called a regular expression to str.contains().\n\n\n\n\nstr.contains() patterns\nUse str.contains(...) to perform matches with regular expressions‚Äîa special string syntax for specifying patterns to match.\nFor example, you can use \"^\" or \"$\" to match the start or end of a string, respectively.\n\n# check if starts with \"ap\" ----\n\npenguins.species.str.contains(\"^ap\")\n\n0      False\n1      False\n       ...  \n342    False\n343    False\nName: species, Length: 344, dtype: bool\n\n\n\n# check if endswith with \"a\" ----\n\npenguins.species.str.contains(\"a$\")\n\n0      False\n1      False\n       ...  \n342    False\n343    False\nName: species, Length: 344, dtype: bool\n\n\nNote that \"$\" and \"^\" are called anchor points."
  },
  {
    "objectID": "guide/ops-strings.html#transforming-strings",
    "href": "guide/ops-strings.html#transforming-strings",
    "title": "String operations (str) üìù",
    "section": "Transforming strings",
    "text": "Transforming strings\nString transformations take a string and return a new, changed version. For example, by converting all the letters to lower, upper, or title case.\n\nSimple transformations\n\nfruits.str.lower()\n\nfruits.str.upper()\n\n0          APPLE\n1        APRICOT\n2        AVOCADO\n3         BANANA\n4    BELL PEPPER\ndtype: object\n\n\n\n\nstr.replace() patterns\nUse .str.replace(..., regex=True) with regular expressions to replace patterns in strings.\nFor example, the code below uses \"p.\", where . is called a wildcard‚Äìwhich matches any character.\n\nfruits.str.replace(\"p.\", \"XX\", regex=True)\n\n0          aXXle\n1        aXXicot\n2        avocado\n3         banana\n4    bell XXXXer\ndtype: object"
  },
  {
    "objectID": "guide/ops-strings.html#extracting-parts",
    "href": "guide/ops-strings.html#extracting-parts",
    "title": "String operations (str) üìù",
    "section": "Extracting parts",
    "text": "Extracting parts\n\n.str[] to slice\n\n\n\n\n\n\nWarning\n\n\n\nIt is currently not possible to apply a sequence of slices to .str. You can only apply the same slice to every string in the Series.\n\n\n\n\n.str.extract() patterns\nUse str.extract() with a regular expression to pull out a matching piece of text.\nFor example the regular expression ‚Äú^(.*) ‚Äù contains the following pieces:\n\na matches the literal letter ‚Äúa‚Äù\n.* has a . which matches anything, and * which modifies it to apply 0 or more times.\n\n\nfruits.str.extract(\"a(.*)\")\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      0\n      pple\n    \n    \n      1\n      pricot\n    \n    \n      2\n      vocado\n    \n    \n      3\n      nana\n    \n    \n      4\n      NaN"
  },
  {
    "objectID": "guide/ops-strings.html#split-and-flatten",
    "href": "guide/ops-strings.html#split-and-flatten",
    "title": "String operations (str) üìù",
    "section": "Split and flatten",
    "text": "Split and flatten\n\n.str.split() into list-entries\nUse .str.split() to split each entry on a character, producing a list per row of split strings.\n\nfruits.str.split(\"pp\")\n\n0          [a, le]\n1        [apricot]\n2        [avocado]\n3         [banana]\n4    [bell pe, er]\ndtype: object\n\n\nSeeing each entry be a list may surprising, and is fairly rare in pandas.\n\n\n.str.join() is the inverse of split\n\npenguins.species.str.split(\"e\").str.join(\"e\")\n\n0         Adelie\n1         Adelie\n         ...    \n342    Chinstrap\n343    Chinstrap\nName: species, Length: 344, dtype: object\n\n\n\n\n.explode() to unnest entries\nUse .str.explode() to take a column with list-entries (like those returned by .str.split()) and unnest each entry, so there is 1 row per each element in each list.\n\nsplits = fruits.str.split(\"pp\")\nsplits\n\n0          [a, le]\n1        [apricot]\n2        [avocado]\n3         [banana]\n4    [bell pe, er]\ndtype: object\n\n\nNotice that the result above has 4 list-entries (rows). The first and last rows are the splits [\"a\", \"le\"] and [\"bell pe\", \"er\"], so there are 7 elements total.\nThe .explode() method makes each of the 7 elements its own row.\n\nsplits.explode()\n\n0          a\n0         le\n      ...   \n4    bell pe\n4         er\nLength: 7, dtype: object\n\n\nBe careful to note that it‚Äôs .explode() and not .str.explode(), since it can be used on lists of other things as well!\n\n\n.str.findall() for advanced splitting\nFor example, the code below uses \"pp?\", where ? means the preceding character (‚Äúp‚Äù) is optional for matching:\n\nfruits.str.findall(\"pp?\")\n\n0       [pp]\n1        [p]\n2         []\n3         []\n4    [p, pp]\ndtype: object"
  },
  {
    "objectID": "guide/ops-strings.html#more-regular-expressions",
    "href": "guide/ops-strings.html#more-regular-expressions",
    "title": "String operations (str) üìù",
    "section": "More regular expressions",
    "text": "More regular expressions\n\nAnchor points\n\n^ - matches the beginning of a string.\n$ - matches the end of a string.\n\n\n\nRepetition qualifiers\n\n* - matches 0 or more\n+ - matches 1 or more\n? - matches 0 or 1\n\n\n\nGrouping\n\n()\n{}\n[]\n\n\n\nAlternatives\n\n|"
  },
  {
    "objectID": "guide/ops-support-table.html",
    "href": "guide/ops-support-table.html",
    "title": "Operation support table",
    "section": "",
    "text": "op_name\npandas\nduckdb\nbigquery\nmysql\npostgresql\nsnowflake\nsqlite\n\n\n\n\n0\nabs\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n1\nadd\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n2\nall\n‚úÖ\n‚úÖ\n‚úÖ\n\n‚úÖ\n‚úÖ\n\n\n\n3\nany\n‚úÖ\n‚úÖ\n‚úÖ\n\n‚úÖ\n‚úÖ\n\n\n\n4\nasfreq\n‚úÖ\n\n\n\n\n\n\n\n\n5\nastype\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n6\nbetween\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n7\nbfill\n‚úÖ\n\n\n\n\n\n\n\n\n8\ncat.add_categories\n‚úÖ\n\n\n\n\n\n\n\n\n9\ncat.as_ordered\n‚úÖ\n\n\n\n\n\n\n\n\n10\ncat.as_unordered\n‚úÖ\n\n\n\n\n\n\n\n\n11\ncat.remove_categories\n‚úÖ\n\n\n\n\n\n\n\n\n12\ncat.rename_categories\n‚úÖ\n\n\n\n\n\n\n\n\n13\ncat.reorder_categories\n‚úÖ\n\n\n\n\n\n\n\n\n14\ncat.set_categories\n‚úÖ\n\n\n\n\n\n\n\n\n15\nclip\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n\n16\ncopy\n‚úÖ\n\n\n\n\n\n\n\n\n17\ncount\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n18\ncummax\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n19\ncummin\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n20\ncumprod\n‚úÖ\n\n\n\n\n\n\n\n\n21\ncumsum\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n22\ndiff\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n23\ndiv\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n24\ndivide\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n25\ndt.ceil\n‚úÖ\n\n\n\n\n\n\n\n\n26\ndt.date\n‚úÖ\n\n\n\n\n\n\n\n\n27\ndt.day\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n28\ndt.day_name\n‚úÖ\n\n\n\n\n\n\n\n\n29\ndt.dayofweek\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n30\ndt.dayofyear\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n31\ndt.days_in_month\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n32\ndt.daysinmonth\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n33\ndt.floor\n‚úÖ\n\n\n\n\n\n\n\n\n34\ndt.freq\n‚úÖ\n\n\n\n\n\n\n\n\n35\ndt.hour\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n36\ndt.is_leap_year\n‚úÖ\n\n\n\n\n\n\n\n\n37\ndt.is_month_end\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n38\ndt.is_month_start\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n39\ndt.is_quarter_end\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n\n40\ndt.is_quarter_start\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n\n41\ndt.is_year_end\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n42\ndt.is_year_start\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n43\ndt.microsecond\n‚úÖ\n\n\n\n\n\n\n\n\n44\ndt.minute\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n45\ndt.month\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n46\ndt.month_name\n‚úÖ\n\n‚úÖ\n\n\n\n\n\n\n47\ndt.nanosecond\n‚úÖ\n\n\n\n\n\n\n\n\n48\ndt.normalize\n‚úÖ\n\n\n\n\n\n\n\n\n49\ndt.quarter\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n50\ndt.round\n‚úÖ\n\n\n\n\n\n\n\n\n51\ndt.second\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n52\ndt.strftime\n‚úÖ\n\n\n\n\n\n\n\n\n53\ndt.time\n‚úÖ\n\n\n\n\n\n\n\n\n54\ndt.timetz\n‚úÖ\n\n\n\n\n\n\n\n\n55\ndt.to_period\n‚úÖ\n\n\n\n\n\n\n\n\n56\ndt.tz\n‚úÖ\n\n\n\n\n\n\n\n\n57\ndt.tz_localize\n‚úÖ\n\n\n\n\n\n\n\n\n58\ndt.week\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n59\ndt.weekday\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n60\ndt.weekofyear\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n61\ndt.year\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n62\ndtype\n‚úÖ\n\n\n\n\n\n\n\n\n63\neq\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n64\nffill\n‚úÖ\n\n\n\n\n\n\n\n\n65\nfillna\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n66\nfloordiv\n‚úÖ\n\n\n\n\n\n\n\n\n67\nge\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n68\ngt\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n69\nhead\n‚úÖ\n\n\n\n\n\n\n\n\n70\nidxmax\n‚úÖ\n\n\n\n\n\n\n\n\n71\nidxmin\n‚úÖ\n\n\n\n\n\n\n\n\n72\nis_monotonic_decreasing\n‚úÖ\n\n\n\n\n\n\n\n\n73\nis_monotonic_increasing\n‚úÖ\n\n\n\n\n\n\n\n\n74\nisin\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n75\nisna\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n76\nisnull\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n77\nle\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n78\nlt\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n79\nmad\n‚úÖ\n\n\n\n\n\n\n\n\n80\nmask\n‚úÖ\n\n\n\n\n\n\n\n\n81\nmax\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n82\nmean\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n83\nmedian\n‚úÖ\n\n(win)\n\n\n\n\n\n\n84\nmin\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n85\nmod\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n86\nmul\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n87\nmultiply\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n88\nname\n‚úÖ\n\n\n\n\n\n\n\n\n89\nndim\n‚úÖ\n\n\n\n\n\n\n\n\n90\nne\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n91\nnotna\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n92\nnotnull\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n93\nnunique\n‚úÖ\n(agg)\n‚úÖ\n(agg)\n(agg)\n(agg)\n(agg)\n\n\n94\npct_change\n‚úÖ\n\n\n\n\n\n\n\n\n95\npow\n‚úÖ\n\n\n\n\n\n\n\n\n96\nprod\n‚úÖ\n\n\n\n\n\n\n\n\n97\nquantile\n‚úÖ\n(agg)\n(win)\n\n(agg)\n‚úÖ\n\n\n\n98\nradd\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n99\nrank\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n100\nrdiv\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n101\nreplace\n‚úÖ\n\n\n\n\n\n\n\n\n102\nrfloordiv\n‚úÖ\n\n\n\n\n\n\n\n\n103\nrmod\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n104\nrmul\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n105\nround\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n106\nrpow\n‚úÖ\n\n\n\n\n\n\n\n\n107\nrsub\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n108\nrtruediv\n‚úÖ\n\n\n\n\n\n\n\n\n109\nsem\n‚úÖ\n\n\n\n\n\n\n\n\n110\nshift\n‚úÖ\n\n\n\n\n\n\n\n\n111\nsize\n‚úÖ\n‚úÖ\n‚úÖ\n(agg)\n‚úÖ\n(agg)\n(agg)\n\n\n112\nskew\n‚úÖ\n\n\n\n\n\n\n\n\n113\nstd\n‚úÖ\n‚úÖ\n‚úÖ\n\n‚úÖ\n‚úÖ\n\n\n\n114\nstr.capitalize\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n115\nstr.center\n‚úÖ\n\n\n\n\n\n\n\n\n116\nstr.contains\n‚úÖ\n‚úÖ\n‚úÖ\n\n‚úÖ\n\n\n\n\n117\nstr.count\n‚úÖ\n\n\n\n\n\n\n\n\n118\nstr.encode\n‚úÖ\n\n\n\n\n\n\n\n\n119\nstr.endswith\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n120\nstr.find\n‚úÖ\n\n\n\n\n\n\n\n\n121\nstr.findall\n‚úÖ\n\n\n\n\n\n\n\n\n122\nstr.isalnum\n‚úÖ\n\n\n\n\n\n\n\n\n123\nstr.isalpha\n‚úÖ\n\n\n\n\n\n\n\n\n124\nstr.isdecimal\n‚úÖ\n\n\n\n\n\n\n\n\n125\nstr.isdigit\n‚úÖ\n\n\n\n\n\n\n\n\n126\nstr.islower\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n127\nstr.isnumeric\n‚úÖ\n\n\n\n\n\n\n\n\n128\nstr.isspace\n‚úÖ\n\n\n\n\n\n\n\n\n129\nstr.istitle\n‚úÖ\n\n\n\n\n\n\n\n\n130\nstr.isupper\n‚úÖ\n\n\n\n\n\n\n\n\n131\nstr.len\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n132\nstr.ljust\n‚úÖ\n\n\n\n\n\n\n\n\n133\nstr.lower\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n134\nstr.lstrip\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n135\nstr.match\n‚úÖ\n\n\n\n\n\n\n\n\n136\nstr.pad\n‚úÖ\n\n\n\n\n\n\n\n\n137\nstr.replace\n‚úÖ\n\n‚úÖ\n\n\n\n\n\n\n138\nstr.rfind\n‚úÖ\n\n\n\n\n\n\n\n\n139\nstr.rjust\n‚úÖ\n\n\n\n\n\n\n\n\n140\nstr.rsplit\n‚úÖ\n\n\n\n\n\n\n\n\n141\nstr.rstrip\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n142\nstr.slice\n‚úÖ\n\n\n\n\n\n\n\n\n143\nstr.slice_replace\n‚úÖ\n\n\n\n\n\n\n\n\n144\nstr.split\n‚úÖ\n\n\n\n\n\n\n\n\n145\nstr.startswith\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n146\nstr.strip\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n147\nstr.swapcase\n‚úÖ\n\n\n\n\n\n\n\n\n148\nstr.title\n‚úÖ\n\n‚úÖ\n\n‚úÖ\n‚úÖ\n\n\n\n149\nstr.upper\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n150\nstr.wrap\n‚úÖ\n\n\n\n\n\n\n\n\n151\nsub\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n152\nsubtract\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n153\nsum\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n154\ntail\n‚úÖ\n\n\n\n\n\n\n\n\n155\ntruediv\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n‚úÖ\n\n\n156\nvar\n‚úÖ\n‚úÖ\n‚úÖ\n\n‚úÖ\n‚úÖ\n\n\n\n157\nwhere\n‚úÖ"
  },
  {
    "objectID": "guide/overview.html",
    "href": "guide/overview.html",
    "title": "Overview",
    "section": "",
    "text": "Siuba is a tool for concise, flexible data-analysis over multiple data sources. It currently supports pandas DataFrames and SQL tables."
  },
  {
    "objectID": "guide/overview.html#installing",
    "href": "guide/overview.html#installing",
    "title": "Overview",
    "section": "Installing",
    "text": "Installing\npip install --pre siuba"
  },
  {
    "objectID": "guide/overview.html#basic-use",
    "href": "guide/overview.html#basic-use",
    "title": "Overview",
    "section": "Basic use",
    "text": "Basic use\nThe code below uses the example DataFrame mtcars, to get the average horsepower (hp) per cylinder.\n\nfrom siuba import _, group_by, summarize\nfrom siuba.data import mtcars\n\n(mtcars\n  >> group_by(_.cyl)\n  >> summarize(avg_hp = _.hp.mean())\n  )\n\n\n\n\n\n  \n    \n      \n      cyl\n      avg_hp\n    \n  \n  \n    \n      0\n      4\n      82.636364\n    \n    \n      1\n      6\n      122.285714\n    \n    \n      2\n      8\n      209.214286\n    \n  \n\n\n\n\nThere are three key concepts in this example:\n\n\n\n\n\n\n\n\nconcept\nexample\nmeaning\n\n\n\n\nverb\ngroup_by(...)\na function that operates on a table, like a DataFrame or SQL table\n\n\nlazy expression\n_.hp.mean()\nan expression created with siuba._, that represents actions you want to perform\n\n\npipe\nmtcars >> group_by(...)\na syntax that allows you to chain verbs with the >> operator"
  },
  {
    "objectID": "guide/overview.html#lazy-expressions-_",
    "href": "guide/overview.html#lazy-expressions-_",
    "title": "Overview",
    "section": "Lazy expressions (_)",
    "text": "Lazy expressions (_)\nA siu expression is a way of specifying what action you want to perform. This allows siuba verbs to decide how to execute the action, depending on whether your data is a local DataFrame or remote table.\n\nfrom siuba import _\n\n_.cyl == 4\n\n‚ñà‚îÄ==\n‚îú‚îÄ‚ñà‚îÄ.\n‚îÇ ‚îú‚îÄ_\n‚îÇ ‚îî‚îÄ'cyl'\n‚îî‚îÄ4\n\n\nNotice how the output represents each step in our lazy expression, with these pieces:\n\nblack box ‚ñà - a method like checking equality (==) or getting an attribute (.).\nunderscore (_) - a placeholder for a table of data.\n\nWe can use these expressions like lambda functions. For example, to keep specific rows of a pandas DataFrame.\n\n# old approach: repeat name\nmtcars[mtcars.cyl == 4]\n\n# old approach: lambda\nmtcars[lambda _: _.cyl == 4]\n\n# siu approach\nmtcars[_.cyl == 4]\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      2\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      7\n      24.4\n      4\n      146.7\n      62\n      3.69\n      3.190\n      20.00\n      1\n      0\n      4\n      2\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      27\n      30.4\n      4\n      95.1\n      113\n      3.77\n      1.513\n      16.90\n      1\n      1\n      5\n      2\n    \n    \n      31\n      21.4\n      4\n      121.0\n      109\n      4.11\n      2.780\n      18.60\n      1\n      1\n      4\n      2\n    \n  \n\n11 rows √ó 11 columns\n\n\n\nNote that like the lambda function, siuba avoids typing the same (potentially_very_long) name twice, while also being a bit shorter."
  },
  {
    "objectID": "guide/overview.html#table-verbs",
    "href": "guide/overview.html#table-verbs",
    "title": "Overview",
    "section": "Table verbs",
    "text": "Table verbs\nVerbs are functions that operate on a table of data. They can be combined using a pipe with the >> operator.\n\nfrom siuba import _, mutate, filter, group_by, summarize\nfrom siuba.data import mtcars\n\n\nMutate\nThe previous example can be re-written in siuba as the following.\n\n(mtcars\n  >> group_by(_.cyl)\n  >> mutate(demeaned = _.hp - _.hp.mean())\n  )\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n      demeaned\n    \n  \n  \n    \n      0\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n      -12.285714\n    \n    \n      1\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n      -12.285714\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      15.0\n      8\n      301.0\n      335\n      3.54\n      3.570\n      14.60\n      0\n      1\n      5\n      8\n      125.785714\n    \n    \n      31\n      21.4\n      4\n      121.0\n      109\n      4.11\n      2.780\n      18.60\n      1\n      1\n      4\n      2\n      26.363636\n    \n  \n\n32 rows √ó 12 columns\n\n\n\nNote that there is a key difference: mutate returned a pandas DataFrame with the new column (demeaned) at the end. This is a core feature of siuba verbs‚Äìtables in and tables out.\n\n\nFilter\nBelow are examples of keeping certain rows with filter, and calculating a single number per group with summarize.\n\ng_cyl = group_by(mtcars, _.cyl)\n\n# keep lowest hp per group\ng_cyl >> filter(_.hp == _.hp.min())\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      5\n      18.1\n      6\n      225.0\n      105\n      2.76\n      3.460\n      20.22\n      1\n      0\n      3\n      1\n    \n    \n      18\n      30.4\n      4\n      75.7\n      52\n      4.93\n      1.615\n      18.52\n      1\n      1\n      4\n      2\n    \n    \n      21\n      15.5\n      8\n      318.0\n      150\n      2.76\n      3.520\n      16.87\n      0\n      0\n      3\n      2\n    \n    \n      22\n      15.2\n      8\n      304.0\n      150\n      3.15\n      3.435\n      17.30\n      0\n      0\n      3\n      2\n    \n  \n\n\n\n\n\n\nSummarize\n\ng_cyl >> summarize(avg_hp = _.hp.mean())\n\n\n\n\n\n  \n    \n      \n      cyl\n      avg_hp\n    \n  \n  \n    \n      0\n      4\n      82.636364\n    \n    \n      1\n      6\n      122.285714\n    \n    \n      2\n      8\n      209.214286"
  },
  {
    "objectID": "guide/overview.html#column-operations",
    "href": "guide/overview.html#column-operations",
    "title": "Overview",
    "section": "Column operations",
    "text": "Column operations\nThe verbs above received a few different calculations as arguments:\n\n_.hp.mean()\n_.hp.min()\n\nYou can use any methods from the underlying pandas objects as methods.\n\n# outside\nmtcars.shape[0] + 1\n\n# inside mutate\nmtcars >> mutate(res = _.shape[0] + 1)\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n      res\n    \n  \n  \n    \n      0\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n      33\n    \n    \n      1\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n      33\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      15.0\n      8\n      301.0\n      335\n      3.54\n      3.570\n      14.60\n      0\n      1\n      5\n      8\n      33\n    \n    \n      31\n      21.4\n      4\n      121.0\n      109\n      4.11\n      2.780\n      18.60\n      1\n      1\n      4\n      2\n      33\n    \n  \n\n32 rows √ó 12 columns\n\n\n\nThis includes the str and dt attribute accessor methods:\n\nimport pandas as pd\n\ndf = pd.DataFrame({\"x\": [\"apple\", \"banana\"]})\n\n# outside\ndf.x.str.contains(\"a\")\n\n# inside mutate\ndf >> mutate(res = _.x.str.contains(\"a\"))\n\n\n\n\n\n  \n    \n      \n      x\n      res\n    \n  \n  \n    \n      0\n      apple\n      True\n    \n    \n      1\n      banana\n      True"
  },
  {
    "objectID": "guide/overview.html#using-with-plotnine",
    "href": "guide/overview.html#using-with-plotnine",
    "title": "Overview",
    "section": "Using with plotnine",
    "text": "Using with plotnine\nFortnuately, plotnine supports siuba‚Äôs style of piping, so is easy to plug in to!\n\nfrom siuba import mutate, _\nfrom plotnine import ggplot, aes, geom_point\n\n(mtcars\n  >> mutate(hp_per_cyl = _.hp / _.cyl)\n  >> ggplot(aes(\"cyl\", \"hp_per_cyl\"))\n   + geom_point()\n)\n\n\n\n\n<ggplot: (8775643137660)>"
  },
  {
    "objectID": "guide/overview.html#next-steps",
    "href": "guide/overview.html#next-steps",
    "title": "Overview",
    "section": "Next steps",
    "text": "Next steps\nTODO"
  },
  {
    "objectID": "guide/programming-across.html",
    "href": "guide/programming-across.html",
    "title": "Across column apply",
    "section": "",
    "text": "Use the across() function to apply the same transformation to multiple columns."
  },
  {
    "objectID": "guide/programming-across.html#basic-use",
    "href": "guide/programming-across.html#basic-use",
    "title": "Across column apply",
    "section": "Basic use",
    "text": "Basic use\n\nmtcars >> mutate(across(_[\"mpg\", \"hp\"], Fx - Fx.mean(), names=\"demeaned_{col}\"))\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n      demeaned_mpg\n      demeaned_hp\n    \n  \n  \n    \n      0\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n      0.909375\n      -36.6875\n    \n    \n      1\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n      0.909375\n      -36.6875\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      15.0\n      8\n      301.0\n      335\n      3.54\n      3.570\n      14.60\n      0\n      1\n      5\n      8\n      -5.090625\n      188.3125\n    \n    \n      31\n      21.4\n      4\n      121.0\n      109\n      4.11\n      2.780\n      18.60\n      1\n      1\n      4\n      2\n      1.309375\n      -37.6875\n    \n  \n\n32 rows √ó 13 columns\n\n\n\nNote three important pieces in the code above:\n\nselect: _[\"mpg\", \"hp\"] chooses the columns to transform.\ntransform: Fx - Fx.mean() is the transformation, where Fx stands for the column being operated on.\nrename: names= is an optional argument, specifying how to name the result. The {col} in \"demeaned_{col}\" gets replaced with the column name."
  },
  {
    "objectID": "guide/programming-across.html#selecting-columns",
    "href": "guide/programming-across.html#selecting-columns",
    "title": "Across column apply",
    "section": "Selecting columns",
    "text": "Selecting columns\nAny selection that can be passed to select(), can also be used in across(). Note that you can use _[...] to combine selections.\n\nmtcars >> summarize(across(_[_.startswith(\"m\"), _.endswith(\"p\")], Fx.mean()))\n\n\n\n\n\n  \n    \n      \n      mpg\n      disp\n      hp\n    \n  \n  \n    \n      0\n      20.090625\n      230.721875\n      146.6875"
  },
  {
    "objectID": "guide/programming-across.html#passing-multiple-transformations",
    "href": "guide/programming-across.html#passing-multiple-transformations",
    "title": "Across column apply",
    "section": "Passing multiple transformations",
    "text": "Passing multiple transformations\n\nmtcars >> summarize(across(_[\"mpg\", \"hp\"], {\"avg\": Fx.mean(), \"std\": Fx.std()}))\n\n\n\n\n\n  \n    \n      \n      mpg_avg\n      mpg_std\n      hp_avg\n      hp_std\n    \n  \n  \n    \n      0\n      20.090625\n      6.026948\n      146.6875\n      68.562868"
  },
  {
    "objectID": "guide/programming-across.html#with-grouped-data",
    "href": "guide/programming-across.html#with-grouped-data",
    "title": "Across column apply",
    "section": "With grouped data",
    "text": "With grouped data\n\nmtcars >> group_by(_.cyl) >> summarize(across(_[_.mpg, _.hp], Fx.mean()))\n\n\n\n\n\n  \n    \n      \n      cyl\n      mpg\n      hp\n    \n  \n  \n    \n      0\n      4\n      26.663636\n      82.636364\n    \n    \n      1\n      6\n      19.742857\n      122.285714\n    \n    \n      2\n      8\n      15.100000\n      209.214286"
  },
  {
    "objectID": "guide/programming-adv-sql.html",
    "href": "guide/programming-adv-sql.html",
    "title": "Advanced SQL üöß",
    "section": "",
    "text": "Single code-chunk review of basics: show_query, collect, preview\nLazyTbl from query string (in dbcooper currently?)\nsimplify query (show_query(simplify=True))\nsiuba.sql.sql_raw\nLazyTbl core properties\n\nlast_op"
  },
  {
    "objectID": "guide/programming-new-ops.html",
    "href": "guide/programming-new-ops.html",
    "title": "Custom column ops",
    "section": "",
    "text": "Use symbolic_dispatch() to create new functions for operating on columns.\nThis function creates what are called single generic functions‚Äî which let you register different ways to handle different types of data."
  },
  {
    "objectID": "guide/programming-new-ops.html#define-a-new-function",
    "href": "guide/programming-new-ops.html#define-a-new-function",
    "title": "Custom column ops",
    "section": "Define a new function",
    "text": "Define a new function\n\nfrom siuba.siu import symbolic_dispatch\nfrom pandas.core.groupby import SeriesGroupBy\nfrom pandas import Series\n\n@symbolic_dispatch\ndef all_like(col, text):\n    raise NotImplementedError(f\"Not implemented for class {type(col)}\")\n\n@all_like.register\ndef _all_like_ser(col: Series, text: str) -> Series:\n    \"\"\"Return transformation. Checks whether text is in each entry of col.\"\"\"\n    return col.str.contains(text).all()"
  },
  {
    "objectID": "guide/programming-new-ops.html#check-for-a-translation",
    "href": "guide/programming-new-ops.html#check-for-a-translation",
    "title": "Custom column ops",
    "section": "Check for a translation",
    "text": "Check for a translation\n\nall_like.dispatch(Series)\n\n<function __main__._all_like_ser(col: pandas.core.series.Series, text: str) -> pandas.core.series.Series>"
  },
  {
    "objectID": "guide/programming-new-ops.html#register-an-error-with-functionlookupbound",
    "href": "guide/programming-new-ops.html#register-an-error-with-functionlookupbound",
    "title": "Custom column ops",
    "section": "Register an error with FunctionLookupBound",
    "text": "Register an error with FunctionLookupBound\n\nfrom siuba.siu import FunctionLookupBound\n\n@symbolic_dispatch\ndef some_func(x):\n    return 1\n\nsome_func.register(Series, FunctionLookupBound(\"Not implemented\"))\n\n<siuba.siu.visitors.FunctionLookupBound at 0x7f0338780d90>\n\n\n\nf_concrete = some_func.dispatch(Series)\n\n# indicates that a function is *not* implemented\nisinstance(f_concrete, FunctionLookupBound)\n\nTrue"
  },
  {
    "objectID": "guide/programming-new-ops.html#register-a-sql-translation",
    "href": "guide/programming-new-ops.html#register-a-sql-translation",
    "title": "Custom column ops",
    "section": "Register a SQL translation",
    "text": "Register a SQL translation\n\nfrom sqlalchemy import sql\n\nfrom siuba.sql.dialects.postgresql import PostgresqlColumn, PostgresqlColumnAgg\nfrom siuba.sql.translate import AggOver\n\n\nTransformation\n\n@all_like.register\ndef _all_like_pg(\n    codata: PostgresqlColumn,\n    col: sql.ClauseElement,\n    text: str\n) -> sql.ClauseElement:\n    return AggOver(sql.func.bool_and(col.like(text)))\n\n\nexpr = all_like(PostgresqlColumn(), sql.column(\"a\"), \"yo\")\nprint(expr)\n\nbool_and(a LIKE :a_1) OVER ()\n\n\nNote that AggOver ensures the result is a transformation by using an OVER clause. The partition and ordering of the clause are set automatically by siuba verbs.\nThere are three special over clauses:\n\nAggOver: handle an aggregate (e.g.¬†AVG(hp) -> AVG(hp) OVER(...)).\nRankOver: handle a ranking function (e.g.¬†RANK() OVER (... ORDER BY ...)).\nCumlOver: handle a cumulative function (e.g.¬†SUM(hp) OVER (... ORDER BY ...)).\n\n\n\nAggregation\n\n@all_like.register\ndef _like_pg_agg(\n    codata: PostgresqlColumnAgg,\n    col: sql.ClauseElement,\n    text: str\n) -> sql.ClauseElement:\n    return sql.func.bool_and(col.like(text))\n\n\nexpr = all_like(PostgresqlColumnAgg(), sql.column(\"a\"), \"yo\")\nprint(expr)\n\nbool_and(a LIKE :a_1)\n\n\n\n\nCall functions in functions\nUse the codata parameter when calling another generic function inside your custom function.\n\n# these are the functions used to translate pandas methods\nfrom siuba.ops import mean, std\n\n@symbolic_dispatch(cls = PostgresqlColumn)\ndef scale(codata, x):\n    return (x - mean(codata, x)) / std(codata, x)\n\n\nfrom siuba.data import cars_sql\nfrom siuba import _, mutate, group_by, show_query\n\nq = mutate(cars_sql, res = scale(_.mpg)) >> show_query(simplify=True)\n\nSELECT *, (cars.mpg - avg(cars.mpg) OVER ()) / stddev_samp(cars.mpg) OVER () AS res \nFROM cars\n\n\n\nq = cars_sql >> group_by(_.cyl) >> mutate(res = scale(_.mpg)) >> show_query(simplify=True)\n\nSELECT *, (cars.mpg - avg(cars.mpg) OVER (PARTITION BY cars.cyl)) / stddev_samp(cars.mpg) OVER (PARTITION BY cars.cyl) AS res \nFROM cars"
  },
  {
    "objectID": "guide/programming-new-verbs.html",
    "href": "guide/programming-new-verbs.html",
    "title": "Custom table verbs",
    "section": "",
    "text": "You may be wondering how a siuba function, like mutate(), could work on a SQL database. This is because these functions are defined using a technique called single dispatch. This approach allows you to define class-specific versions of a function."
  },
  {
    "objectID": "guide/programming-new-verbs.html#create-a-new-verb-with-verb_dispatch",
    "href": "guide/programming-new-verbs.html#create-a-new-verb-with-verb_dispatch",
    "title": "Custom table verbs",
    "section": "Create a new verb with verb_dispatch()",
    "text": "Create a new verb with verb_dispatch()\nUse the verb_dispatch() decorator to create a new table verb function.\nThe code below creates a function called head(), with an implementation that works specifically on a DataFrame.\n\n# DataFrame version of function ---\n\n@verb_dispatch(pd.DataFrame)\ndef head(__data, n = 5):\n    return __data.head(n)\n\nhead(mtcars, 2)\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      21.0\n      6\n      160.0\n      110\n      3.9\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      21.0\n      6\n      160.0\n      110\n      3.9\n      2.875\n      17.02\n      0\n      1\n      4\n      4"
  },
  {
    "objectID": "guide/programming-new-verbs.html#register-a-sql-translation",
    "href": "guide/programming-new-verbs.html#register-a-sql-translation",
    "title": "Custom table verbs",
    "section": "Register a SQL translation",
    "text": "Register a SQL translation\nWe can define a SQL specific version, that acts on a SqlAlchemy Table by registering a new translation of our function head, with the @head.register decorator.\n\nfrom siuba.sql import LazyTbl\n\n@head.register\ndef _head_sql(__data: LazyTbl, n = 5):\n    new_select = __data.last_select.limit(n)\n    \n    return __data.append_op(new_select)\n\nHere is the function being run on a mock postgresql backend:\n\nfrom siuba.data import cars_sql\nfrom siuba import show_query\n\nq = cars_sql >> head(3) >> show_query()\n\nSELECT cars.cyl, cars.mpg, cars.hp \nFROM cars \n LIMIT 3\n\n\n\n\n\n\n\n\nNote\n\n\n\nsiuba uses SqlAlchemy internally to handle SQL queries. Most verbs in siuba involve creating a new subquery, or modifying the current query.\nLazyTbl uses the properties .group_by and .order_by to track options set by group_by() and arrange(), respectively."
  },
  {
    "objectID": "guide/programming-new-verbs.html#grouped-data",
    "href": "guide/programming-new-verbs.html#grouped-data",
    "title": "Custom table verbs",
    "section": "Grouped data",
    "text": "Grouped data\nSince single dispatch functions define how to execute an action for a specific class of data, it allows siuba to handle grouped data in two ways:\n\npandas - register dispatchers for its special grouped data classes (DataFrameGroupBy, SeriesGroupBy).\nSQL - use a single class for grouped and ungrouped data, with grouping info as an attribute (siuba.sql.LazyTbl).\n\nFor example, here is a simple verb that calculates the number of rows in a grouped DataFrame.\n\nfrom pandas.core.groupby import DataFrameGroupBy\n\n@verb_dispatch(DataFrameGroupBy)\ndef size(__data):\n    return __data.size()\n\nsize(mtcars.groupby('cyl'))\n\ncyl\n4    11\n6     7\n8    14\ndtype: int64"
  },
  {
    "objectID": "guide/programming-new-verbs.html#why-not-use-methods",
    "href": "guide/programming-new-verbs.html#why-not-use-methods",
    "title": "Custom table verbs",
    "section": "Why not use methods?",
    "text": "Why not use methods?\nWhy use singledispatch rather than create a class method like mtcars.head()?\nThere are two big benefits:\n\nAnyone can define and package a function. Using it is just a matter of importing it. With a method, you need to somehow put it onto the class representing your data. You end up with 300+ methods on a class.\nYour function might do something that is not the class‚Äôs core responsibility. In this case, it should not be part of the class definition.\n\nSee the post Single dispatch for democratizing data science tools for more."
  },
  {
    "objectID": "guide/programming-pipes.html",
    "href": "guide/programming-pipes.html",
    "title": "Flexible pipes",
    "section": "",
    "text": "A commonly used feature of siuba is the >> operator for piping the result of one verb into another. This feature may seem silly compared to method chaining. However, it makes it easy for other packages to contribute new verbs, and lazy pipes can be combined together.\nThis page will focus on three pieces of piping:"
  },
  {
    "objectID": "guide/programming-pipes.html#basic-pipe",
    "href": "guide/programming-pipes.html#basic-pipe",
    "title": "Flexible pipes",
    "section": "Basic pipe",
    "text": "Basic pipe\nUse the >> operator to pass the result of one verb into another.\n\n(mtcars\n    >> group_by(_.cyl)\n    >> summarize(res = _.hp.mean())\n)\n\n\n\n\n\n  \n    \n      \n      cyl\n      res\n    \n  \n  \n    \n      0\n      4\n      82.636364\n    \n    \n      1\n      6\n      122.285714\n    \n    \n      2\n      8\n      209.214286\n    \n  \n\n\n\n\nNote that this is equivalent to the code below, but much more readable.\n\nsummarize(\n    group_by(\n        mtcars,\n        _.cyl\n    ),\n    res = _.hp.mean()\n)\n\n\n\n\n\n  \n    \n      \n      cyl\n      res\n    \n  \n  \n    \n      0\n      4\n      82.636364\n    \n    \n      1\n      6\n      122.285714\n    \n    \n      2\n      8\n      209.214286"
  },
  {
    "objectID": "guide/programming-pipes.html#lazy-pipes",
    "href": "guide/programming-pipes.html#lazy-pipes",
    "title": "Flexible pipes",
    "section": "Lazy pipes",
    "text": "Lazy pipes\nStart a pipe with _ to create a lazy pipes. Lazy pipes are a lot like functions‚Äîthey run once data is passed to them.\n\nfilter_cyl = _ >> filter(_.cyl.isin([4, 8]))\n\nmtcars >> filter_cyl >> head()\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      2\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.32\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      4\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.44\n      17.02\n      0\n      0\n      3\n      2\n    \n    \n      6\n      14.3\n      8\n      360.0\n      245\n      3.21\n      3.57\n      15.84\n      0\n      0\n      3\n      4\n    \n    \n      7\n      24.4\n      4\n      146.7\n      62\n      3.69\n      3.19\n      20.00\n      1\n      0\n      4\n      2\n    \n    \n      8\n      22.8\n      4\n      140.8\n      95\n      3.92\n      3.15\n      22.90\n      1\n      0\n      4\n      2\n    \n  \n\n\n\n\nMultiple lazy pipes can be combined in a pipe.\n\nhead_5 = _ >> head(n=5)\n\nmtcars >> filter_cyl >> head_5\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      2\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.32\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      4\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.44\n      17.02\n      0\n      0\n      3\n      2\n    \n    \n      6\n      14.3\n      8\n      360.0\n      245\n      3.21\n      3.57\n      15.84\n      0\n      0\n      3\n      4\n    \n    \n      7\n      24.4\n      4\n      146.7\n      62\n      3.69\n      3.19\n      20.00\n      1\n      0\n      4\n      2\n    \n    \n      8\n      22.8\n      4\n      140.8\n      95\n      3.92\n      3.15\n      22.90\n      1\n      0\n      4\n      2\n    \n  \n\n\n\n\nThis allows them to work as building blocks during an analysis!"
  },
  {
    "objectID": "guide/programming-pipes.html#how-do-verbs-work",
    "href": "guide/programming-pipes.html#how-do-verbs-work",
    "title": "Flexible pipes",
    "section": "How do verbs work?",
    "text": "How do verbs work?\nThe key to using verbs in a pipe is they have two modes, depending on what they receive as their first argument:\n\nverb(DataFrame, ...): execute the verb right away.\nverb(...): delay execution, and return a Call, which can be used in a pipe.\n\nFor example, here is a summarize being executed directly.\n\nsummarize(mtcars, avg = _.mpg.mean())\n\n\n\n\n\n  \n    \n      \n      avg\n    \n  \n  \n    \n      0\n      20.090625\n    \n  \n\n\n\n\nHere is an example of a group_by call that could be used in a pipe.\n\ngroup_cyl = group_by(_.cyl)\n\ntype(group_cyl)\n\nsiuba.siu.calls.Call\n\n\n\nmtcars >> group_cyl >> summarize(res = _.hp.mean())\n\n\n\n\n\n  \n    \n      \n      cyl\n      res\n    \n  \n  \n    \n      0\n      4\n      82.636364\n    \n    \n      1\n      6\n      122.285714\n    \n    \n      2\n      8\n      209.214286\n    \n  \n\n\n\n\n\nExplicit use of verbs in a pipe\nUse _ as the first argument to a verb, in order to return a Call.\n\n# eagerly evaluated ----\ngroup_by(mtcars, _.cyl)\n\n# lazy: both of these can be used in a pipe ----\n\n# implicit\ngroup_by(_.cyl)\n\n# explicit\ngroup_by(_, _.cyl)\n\n<function group_by at 0x7f6bd5f49430>(_,_.cyl())\n\n\nThis is much more explicit, but also a bit more clunky looking."
  },
  {
    "objectID": "guide/programming-pipes.html#call-two-table-verbs",
    "href": "guide/programming-pipes.html#call-two-table-verbs",
    "title": "Flexible pipes",
    "section": "Call two-table verbs",
    "text": "Call two-table verbs\nSome verbs take two tables of data. For example, inner_join() merges two tables of data based on some condition matching them up.\nFor two-table verbs, use _ as the first argument, to indicate it is being used in a pipe.\n\ntbl_labels = pd.DataFrame({\"cyl\": [4, 6, 8], \"label\": [\"four\", \"six\", \"eight\"]})\n\n# executed right away\ninner_join(mtcars, tbl_labels, \"cyl\")\n\n# piping approach\nmtcars >> inner_join(_, tbl_labels, \"cyl\")\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n      label\n    \n  \n  \n    \n      0\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n      six\n    \n    \n      1\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n      six\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      15.8\n      8\n      351.0\n      264\n      4.22\n      3.170\n      14.50\n      0\n      1\n      5\n      4\n      eight\n    \n    \n      31\n      15.0\n      8\n      301.0\n      335\n      3.54\n      3.570\n      14.60\n      0\n      1\n      5\n      8\n      eight\n    \n  \n\n32 rows √ó 12 columns"
  },
  {
    "objectID": "guide/programming-pipes.html#call-dataframe-methods",
    "href": "guide/programming-pipes.html#call-dataframe-methods",
    "title": "Flexible pipes",
    "section": "Call DataFrame methods",
    "text": "Call DataFrame methods\nSometimes it is helpful to use Pandas DataFrame methods, in addition to siuba verbs. This can be done by piping the data to _.<some_method>().\nHere is an example using the siuba verb count(), with the pandas method .sort_values().\n\nfrom siuba import _, count\nfrom siuba.data import mtcars\n\n(mtcars\n    >> count(_.cyl)         # this is a siuba verb\n    >> _.sort_values(\"n\")   # this is a pandas method\n)\n\n\n\n\n\n  \n    \n      \n      cyl\n      n\n    \n  \n  \n    \n      1\n      6\n      7\n    \n    \n      0\n      4\n      11\n    \n    \n      2\n      8\n      14\n    \n  \n\n\n\n\nHere is another example, using the DataFrame .shape attribute.\n\n\n\n# siuba pipe\nmtcars >> _.shape[0]\n\n32\n\n\n\n# regular pandas\nmtcars.shape[0]"
  },
  {
    "objectID": "guide/programming-pipes.html#call-external-functions",
    "href": "guide/programming-pipes.html#call-external-functions",
    "title": "Flexible pipes",
    "section": "Call external functions",
    "text": "Call external functions\nUse call() to pipe data into any function call.\nThe example below pipes to the seaborn‚Äôs barplot function.\n\nfrom siuba.siu import call\nimport seaborn as sns\n\n(mtcars\n    >> count(_.cyl)\n    >> call(sns.barplot, x=\"cyl\", y=\"n\", data=_)\n)\n\n<AxesSubplot: xlabel='cyl', ylabel='n'>\n\n\n\n\n\nNote that sns.barplot() expects the data as a named argument, so we pass data=_, where _ is a placeholder for the data.\ncall() can also take a single function to call the data on.\n\n\n\n# piping\nmtcars >> call(len)\n\n32\n\n\n\n# regular function call\nlen(mtcars)\n\n32"
  },
  {
    "objectID": "guide/programming-pipes.html#pipe-as-an-alternative-to",
    "href": "guide/programming-pipes.html#pipe-as-an-alternative-to",
    "title": "Flexible pipes",
    "section": "pipe() as an alternative to >>",
    "text": "pipe() as an alternative to >>\n\n\n\n(\n    mtcars\n    >> group_by(_.cyl, _.gear)\n    >> summarize(res = _.hp.mean())\n    >> call(print, \"Printed output -\\n\", _)\n)\n\nPrinted output -\n     cyl  gear         res\n0     4     3   97.000000\n1     4     4   76.000000\n..  ...   ...         ...\n6     8     3  194.166667\n7     8     5  299.500000\n\n[8 rows x 3 columns]\n\n\n\npipe(\n    mtcars\n    , group_by(_.cyl, _.gear)\n    , summarize(res = _.hp.mean())\n    , call(print, \"Printed output -\\n\", _)\n)"
  },
  {
    "objectID": "guide/verb-arrange.html",
    "href": "guide/verb-arrange.html",
    "title": "Arrange rows",
    "section": "",
    "text": "choosing columns to arrange by\nspecifying an order (ascending or descending)\n\nBelow, we‚Äôll illustrate this function with a single variable, multiple variables, and more general expressions.\n\nfrom siuba import _, arrange, select\nfrom siuba.data import mtcars\n\nsmall_mtcars = mtcars >> select(_.cyl, _.mpg, _.hp)\n\nsmall_mtcars\n\n\n\n\n\n  \n    \n      \n      cyl\n      mpg\n      hp\n    \n  \n  \n    \n      0\n      6\n      21.0\n      110\n    \n    \n      1\n      6\n      21.0\n      110\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      8\n      15.0\n      335\n    \n    \n      31\n      4\n      21.4\n      109\n    \n  \n\n32 rows √ó 3 columns\n\n\n\n\nBasics\nThe simplest way to use arrange is to specify a column name. The arrange function uses pandas.sort_values under the hood, and arranges rows in ascending order.\nFor example, the code below arranges the rows from least to greatest horsepower (hp).\n\n# simple arrange of 1 var\nsmall_mtcars >> arrange(_.hp)\n\n\n\n\n\n  \n    \n      \n      cyl\n      mpg\n      hp\n    \n  \n  \n    \n      18\n      4\n      30.4\n      52\n    \n    \n      7\n      4\n      24.4\n      62\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      28\n      8\n      15.8\n      264\n    \n    \n      30\n      8\n      15.0\n      335\n    \n  \n\n32 rows √ó 3 columns\n\n\n\n\n\nSort in descending order\nIf you add a - before a column or expression, arrange will sort the rows in descending order. This applies to all types of columns, including arrays of strings and categories!\n\nsmall_mtcars >> arrange(-_.hp)\n\n\n\n\n\n  \n    \n      \n      cyl\n      mpg\n      hp\n    \n  \n  \n    \n      30\n      8\n      15.0\n      335\n    \n    \n      28\n      8\n      15.8\n      264\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      7\n      4\n      24.4\n      62\n    \n    \n      18\n      4\n      30.4\n      52\n    \n  \n\n32 rows √ó 3 columns\n\n\n\n\n\nArrange by multiple variables\nWhen arrange receives multiple arguments, it sorts so that the one specified first changes the slowest, followed by the second, and so on.\n\nsmall_mtcars >> arrange(_.cyl, -_.mpg)\n\n\n\n\n\n  \n    \n      \n      cyl\n      mpg\n      hp\n    \n  \n  \n    \n      19\n      4\n      33.9\n      65\n    \n    \n      17\n      4\n      32.4\n      66\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      14\n      8\n      10.4\n      205\n    \n    \n      15\n      8\n      10.4\n      215\n    \n  \n\n32 rows √ó 3 columns\n\n\n\nNotice that in the result above, cyl values are sorted first. In other words, all of the 4‚Äôs are bunched together, with mpg sorted in descending order within each bunch.\n\n\nUsing expressions\nYou can also arrange the rows of your data using more complex expressions, similar to those you would use in a mutate.\nFor example, the code below sorts by horsepower (hp) per cylinder (cyl).\n\nsmall_mtcars >> arrange(_.hp / _.cyl)\n\n\n\n\n\n  \n    \n      \n      cyl\n      mpg\n      hp\n    \n  \n  \n    \n      18\n      4\n      30.4\n      52\n    \n    \n      7\n      4\n      24.4\n      62\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      28\n      8\n      15.8\n      264\n    \n    \n      30\n      8\n      15.0\n      335\n    \n  \n\n32 rows √ó 3 columns\n\n\n\n\n\nCategorical series behavior\nArrange uses pd.sort_values() behind the scenes, which sorts pd.Categorical series by their category order.\n\nser = pd.Categorical([\"a\", \"z\"], categories=[\"z\", \"a\"])\n\nser\n\n['a', 'z']\nCategories (2, object): ['z', 'a']\n\n\n\nser.sort_values()\n\n['z', 'a']\nCategories (2, object): ['z', 'a']\n\n\nSiuba contains a submodule called forcats that make it easy to change the category order.\n\nfrom siuba.dply.forcats import fct_rev\n\n# reverse the category order\nfct_rev(ser)\n\n['a', 'z']\nCategories (2, object): ['a', 'z']\n\n\nYou can learn more in the siuba forcats docs."
  },
  {
    "objectID": "guide/verb-filter.html",
    "href": "guide/verb-filter.html",
    "title": "Filter rows",
    "section": "",
    "text": "The filter() function keeps rows of data that meet all specified conditions."
  },
  {
    "objectID": "guide/verb-filter.html#what-counts-as-na",
    "href": "guide/verb-filter.html#what-counts-as-na",
    "title": "Filter rows",
    "section": "What counts as NA?",
    "text": "What counts as NA?\nUse pandas.isna() to determine whether a value is considered to be NA.\n\ndf = pd.DataFrame({\n    \"x\": [True, False, None],\n    })\n\ndf.x\n\n0     True\n1    False\n2     None\nName: x, dtype: object\n\n\nNotice in the code above that the last value is None. We can confirm pandas sees this as an NA with the code below.\n\npd.isna(df.x)\n\n0    False\n1    False\n2     True\nName: x, dtype: bool\n\n\nSince None is considered an NA, its row gets removed in the filter below.\n\ndf >> filter(_.x)\n\n\n\n\n\n  \n    \n      \n      x\n    \n  \n  \n    \n      0\n      True"
  },
  {
    "objectID": "guide/verb-filter.html#drop-only-by-na",
    "href": "guide/verb-filter.html#drop-only-by-na",
    "title": "Filter rows",
    "section": "Drop only by NA",
    "text": "Drop only by NA\nIf you want to remove only by NA values from your data, use the pandas .notna() method.\nThis effectively says, ‚Äúkeep any values of x that are not NA‚Äù.\n\ndf >> filter(_.x.notna())\n\n\n\n\n\n  \n    \n      \n      x\n    \n  \n  \n    \n      0\n      True\n    \n    \n      1\n      False"
  },
  {
    "objectID": "guide/verb-group-by.html",
    "href": "guide/verb-group-by.html",
    "title": "Group by",
    "section": "",
    "text": "This function is used to specify groups in your data for verbs‚Äîlike mutate(), filter(), and summarize()‚Äîto perform operations over.\nFor example, in the mtcars dataset, there are 3 possible values for cylinders (cyl). You could use group_by to say that you want to perform operations separately for each of these 3 groups of values.\nAn important compliment to group_by() is ungroup(), which removes all current groupings."
  },
  {
    "objectID": "guide/verb-group-by.html#group-by-column",
    "href": "guide/verb-group-by.html#group-by-column",
    "title": "Group by",
    "section": "Group by column",
    "text": "Group by column\nThe simplest way to use group by is to specify your grouping column directly. This is shown below, by grouping mtcars according to its 3 groups of cylinder values (4, 6, or 8 cylinders).\n\ng_cyl = small_cars >> group_by(_.cyl)\n\ng_cyl\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      cyl\n      gear\n      hp\n    \n  \n  \n    \n      0\n      6\n      4\n      110\n    \n    \n      1\n      6\n      4\n      110\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      8\n      5\n      335\n    \n    \n      31\n      4\n      4\n      109\n    \n  \n\n32 rows √ó 3 columns\n\n\n\nNote that the result is simply a pandas GroupedDataFrame, which is what is returned if you use mtcars.groupby('cyl'). Normally, a GroupedDataFrame doesn‚Äôt print out a preview of itself, but siuba modifies it to do so, since this is very handy.\nThe group_by function is most often used with filter, mutate, and summarize.\n\nFilter\n\n# keep rows where hp is greater than mean hp within cyl group\ng_cyl >> filter(_.hp > _.hp.mean())\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      cyl\n      gear\n      hp\n    \n  \n  \n    \n      2\n      4\n      4\n      93\n    \n    \n      6\n      8\n      3\n      245\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      8\n      5\n      335\n    \n    \n      31\n      4\n      4\n      109\n    \n  \n\n15 rows √ó 3 columns\n\n\n\n\n\nMutate\n\ng_cyl >> mutate(avg_hp = _.hp.mean())\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      cyl\n      gear\n      hp\n      avg_hp\n    \n  \n  \n    \n      0\n      6\n      4\n      110\n      122.285714\n    \n    \n      1\n      6\n      4\n      110\n      122.285714\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      8\n      5\n      335\n      209.214286\n    \n    \n      31\n      4\n      4\n      109\n      82.636364\n    \n  \n\n32 rows √ó 4 columns\n\n\n\n\n\nSummarize\n\ng_cyl >> summarize(avg_hp = _.hp.mean())\n\n\n\n\n\n  \n    \n      \n      cyl\n      avg_hp\n    \n  \n  \n    \n      0\n      4\n      82.636364\n    \n    \n      1\n      6\n      122.285714\n    \n    \n      2\n      8\n      209.214286"
  },
  {
    "objectID": "guide/verb-group-by.html#group-by-multiple-columns",
    "href": "guide/verb-group-by.html#group-by-multiple-columns",
    "title": "Group by",
    "section": "Group by multiple columns",
    "text": "Group by multiple columns\nIn order to group by multiple columns, simply specify them all as arguments to group_by.\n\nsmall_cars >> group_by(_.cyl, _.gear)\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      cyl\n      gear\n      hp\n    \n  \n  \n    \n      0\n      6\n      4\n      110\n    \n    \n      1\n      6\n      4\n      110\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      8\n      5\n      335\n    \n    \n      31\n      4\n      4\n      109\n    \n  \n\n32 rows √ó 3 columns"
  },
  {
    "objectID": "guide/verb-group-by.html#group-by-an-expression",
    "href": "guide/verb-group-by.html#group-by-an-expression",
    "title": "Group by",
    "section": "Group by an expression",
    "text": "Group by an expression\n\nsmall_cars >> group_by(high_hp = _.hp > 300)\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      cyl\n      gear\n      hp\n      high_hp\n    \n  \n  \n    \n      0\n      6\n      4\n      110\n      False\n    \n    \n      1\n      6\n      4\n      110\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      8\n      5\n      335\n      True\n    \n    \n      31\n      4\n      4\n      109\n      False\n    \n  \n\n32 rows √ó 4 columns"
  },
  {
    "objectID": "guide/verb-group-by.html#count-rows",
    "href": "guide/verb-group-by.html#count-rows",
    "title": "Group by",
    "section": "Count rows",
    "text": "Count rows\n\nfrom siuba import _, group_by, count\n\n# count number of rows per group\nmtcars >> group_by(_.cyl, _.gear) >> summarize(n = _.shape[0])\n\n# equivalent\nmtcars >> count(_.cyl, _.gear)\n\n\n\n\n\n  \n    \n      \n      cyl\n      gear\n      n\n    \n  \n  \n    \n      0\n      4\n      3\n      1\n    \n    \n      1\n      4\n      4\n      8\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      6\n      8\n      3\n      12\n    \n    \n      7\n      8\n      5\n      2\n    \n  \n\n8 rows √ó 3 columns"
  },
  {
    "objectID": "guide/verb-group-by.html#ungroup",
    "href": "guide/verb-group-by.html#ungroup",
    "title": "Group by",
    "section": "Ungroup",
    "text": "Ungroup\n\nsmall_cars >> group_by(_.cyl) >> ungroup()\n\n\n\n\n\n  \n    \n      \n      cyl\n      gear\n      hp\n    \n  \n  \n    \n      0\n      6\n      4\n      110\n    \n    \n      1\n      6\n      4\n      110\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      8\n      5\n      335\n    \n    \n      31\n      4\n      4\n      109\n    \n  \n\n32 rows √ó 3 columns"
  },
  {
    "objectID": "guide/verb-mutate.html",
    "href": "guide/verb-mutate.html",
    "title": "Mutate to transform",
    "section": "",
    "text": "The mutate() function creates a new column of data, or overwrite an existing one.\nWe‚Äôll use a subset of the mtcars dataset for examples."
  },
  {
    "objectID": "guide/verb-mutate.html#basics",
    "href": "guide/verb-mutate.html#basics",
    "title": "Mutate to transform",
    "section": "Basics",
    "text": "Basics\n\nsmall_cars >> mutate(mpg_per_cyl = _.mpg / _.cyl)\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      hp\n      mpg_per_cyl\n    \n  \n  \n    \n      0\n      21.0\n      6\n      110\n      3.500\n    \n    \n      1\n      21.0\n      6\n      110\n      3.500\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      15.0\n      8\n      335\n      1.875\n    \n    \n      31\n      21.4\n      4\n      109\n      5.350\n    \n  \n\n32 rows √ó 4 columns"
  },
  {
    "objectID": "guide/verb-mutate.html#replacing-columns",
    "href": "guide/verb-mutate.html#replacing-columns",
    "title": "Mutate to transform",
    "section": "Replacing columns",
    "text": "Replacing columns\nWhen a created column is given the same name as an existing column, it replaces that column in the data.\n\nsmall_cars >> mutate(mpg = _.mpg - _.mpg.mean(), new_column = 1)\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      hp\n      new_column\n    \n  \n  \n    \n      0\n      0.909375\n      6\n      110\n      1\n    \n    \n      1\n      0.909375\n      6\n      110\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      -5.090625\n      8\n      335\n      1\n    \n    \n      31\n      1.309375\n      4\n      109\n      1\n    \n  \n\n32 rows √ó 4 columns\n\n\n\nNote that replacement columns are put in the same position as the original columns. For example, in the result above, the mpg column is still in the first position on the left."
  },
  {
    "objectID": "guide/verb-mutate.html#using-previous-arguments",
    "href": "guide/verb-mutate.html#using-previous-arguments",
    "title": "Mutate to transform",
    "section": "Using previous arguments",
    "text": "Using previous arguments\nArguments can refer to columns that were created in earlier arguments.\n\nsmall_cars >> mutate(cyl2 = _.cyl * 2, cyl4 = _.cyl2 * 2)\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      hp\n      cyl2\n      cyl4\n    \n  \n  \n    \n      0\n      21.0\n      6\n      110\n      12\n      24\n    \n    \n      1\n      21.0\n      6\n      110\n      12\n      24\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      15.0\n      8\n      335\n      16\n      32\n    \n    \n      31\n      21.4\n      4\n      109\n      8\n      16\n    \n  \n\n32 rows √ó 5 columns\n\n\n\nIn the code above, cyl4 uses the earlier argument cyl2."
  },
  {
    "objectID": "guide/verb-mutate.html#grouped-mutates",
    "href": "guide/verb-mutate.html#grouped-mutates",
    "title": "Mutate to transform",
    "section": "Grouped mutates",
    "text": "Grouped mutates\n\n(small_cars\n  >> group_by(_.cyl)\n  >> mutate(\n       hp_mean = _.hp.mean(),\n       demeaned_hp = _.hp - _.hp_mean\n     )\n  )\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      mpg\n      cyl\n      hp\n      hp_mean\n      demeaned_hp\n    \n  \n  \n    \n      0\n      21.0\n      6\n      110\n      122.285714\n      -12.285714\n    \n    \n      1\n      21.0\n      6\n      110\n      122.285714\n      -12.285714\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      15.0\n      8\n      335\n      209.214286\n      125.785714\n    \n    \n      31\n      21.4\n      4\n      109\n      82.636364\n      26.363636\n    \n  \n\n32 rows √ó 5 columns\n\n\n\n\n(small_cars\n  >> group_by(_.cyl)\n  >> mutate(\n       hp_per_cyl = _.hp / _.cyl,\n       diff = _.hp_per_cyl - _.hp_per_cyl.shift(1)\n     )\n  )\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      mpg\n      cyl\n      hp\n      hp_per_cyl\n      diff\n    \n  \n  \n    \n      0\n      21.0\n      6\n      110\n      18.333333\n      NaN\n    \n    \n      1\n      21.0\n      6\n      110\n      18.333333\n      0.000\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      15.0\n      8\n      335\n      41.875000\n      8.875\n    \n    \n      31\n      21.4\n      4\n      109\n      27.250000\n      -1.000\n    \n  \n\n32 rows √ó 5 columns"
  },
  {
    "objectID": "guide/verb-select.html",
    "href": "guide/verb-select.html",
    "title": "Select columns",
    "section": "",
    "text": "This function lets you select specific columns of your data to keep.\nThere are three different building blocks that can used in a selection:"
  },
  {
    "objectID": "guide/verb-select.html#select-by-name-or-position",
    "href": "guide/verb-select.html#select-by-name-or-position",
    "title": "Select columns",
    "section": "Select by name or position",
    "text": "Select by name or position\nThe simplest way to select a column to keep is to refer to it by name or position.\n\nselect(penguins, _.species, _.island, 6, -1)\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      sex\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      male\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      female\n      2007\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      342\n      Chinstrap\n      Dream\n      male\n      2009\n    \n    \n      343\n      Chinstrap\n      Dream\n      female\n      2009\n    \n  \n\n344 rows √ó 4 columns\n\n\n\nThe code above does the following:\n\nselects by name the species and island columns.\nselects by position the index 6 and -1 columns (the last item).\n\nSelecting by position should produce the same results as indexing a list of names.\npenguins.columns[6]       # \"sex\"\npenguins.columns[-1]      # \"year\""
  },
  {
    "objectID": "guide/verb-select.html#excluding-columns",
    "href": "guide/verb-select.html#excluding-columns",
    "title": "Select columns",
    "section": "Excluding columns",
    "text": "Excluding columns\nYou can remove a column from the data by putting a tilde operator (~) in front of it.\n\npenguins >> select(~_.body_mass_g, ~_.sex, ~_.year)\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      39.1\n      18.7\n      181.0\n    \n    \n      1\n      Adelie\n      Torgersen\n      39.5\n      17.4\n      186.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      342\n      Chinstrap\n      Dream\n      50.8\n      19.0\n      210.0\n    \n    \n      343\n      Chinstrap\n      Dream\n      50.2\n      18.7\n      198.0\n    \n  \n\n344 rows √ó 5 columns\n\n\n\nThe code above keeps all columns except body_mass_g, sex, and year.\nNote that the ~ operator flips the value of True and False in pandas, and is called the ‚Äúinvert operator‚Äù.\n\n~pd.Series([True, False])\n\n0    False\n1     True\ndtype: bool"
  },
  {
    "objectID": "guide/verb-select.html#renaming-columns",
    "href": "guide/verb-select.html#renaming-columns",
    "title": "Select columns",
    "section": "Renaming columns",
    "text": "Renaming columns\nYou can rename a specified column by using the equality operator (==). This operation takes the following form.\n\n_.new_name == _.old_name\n\n\npenguins >> select(_.species_name == _.species)\n\n\n\n\n\n  \n    \n      \n      species_name\n    \n  \n  \n    \n      0\n      Adelie\n    \n    \n      1\n      Adelie\n    \n    \n      ...\n      ...\n    \n    \n      342\n      Chinstrap\n    \n    \n      343\n      Chinstrap\n    \n  \n\n344 rows √ó 1 columns\n\n\n\nNote that expressing the new column name on the left is similar to how creating a python dictionary works. For example‚Ä¶\n\nselect(_.a == _.x, _.b == _.y)\ndict(a = \"x\", b = \"y\")\n\nboth create new entries named ‚Äúa‚Äù and ‚Äúb‚Äù."
  },
  {
    "objectID": "guide/verb-select.html#select-by-slice",
    "href": "guide/verb-select.html#select-by-slice",
    "title": "Select columns",
    "section": "Select by slice",
    "text": "Select by slice\nWhen the columns are adjacent to each other, you can select them using _[\"start_col\":\"end_col\"].\n\npenguins >> select(_.species, _[\"bill_length_mm\":\"body_mass_g\"])\n\n\n\n\n\n  \n    \n      \n      species\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n      body_mass_g\n    \n  \n  \n    \n      0\n      Adelie\n      39.1\n      18.7\n      181.0\n      3750.0\n    \n    \n      1\n      Adelie\n      39.5\n      17.4\n      186.0\n      3800.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      342\n      Chinstrap\n      50.8\n      19.0\n      210.0\n      4100.0\n    \n    \n      343\n      Chinstrap\n      50.2\n      18.7\n      198.0\n      3775.0\n    \n  \n\n344 rows √ó 5 columns\n\n\n\nYou can use three methods to specify a column in a slice:\n\n_.some_col\n\"some_col\"\na position number\n\n\nExclusion\nYou can exclude slice selections using the ~ operator.\n\npenguins >> select(~_[\"bill_length_mm\":\"body_mass_g\"])\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      sex\n      year\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n      male\n      2007\n    \n    \n      1\n      Adelie\n      Torgersen\n      female\n      2007\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      342\n      Chinstrap\n      Dream\n      male\n      2009\n    \n    \n      343\n      Chinstrap\n      Dream\n      female\n      2009\n    \n  \n\n344 rows √ó 4 columns\n\n\n\n\n\nPosition number\nNote that when position number is used to slice columns, the end position is not included in the selection.\n\n# these are equivalent\n\npenguins >> select(0, 1)\npenguins >> select(_[0:2])\n\n\n\n\n\n  \n    \n      \n      species\n      island\n    \n  \n  \n    \n      0\n      Adelie\n      Torgersen\n    \n    \n      1\n      Adelie\n      Torgersen\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      342\n      Chinstrap\n      Dream\n    \n    \n      343\n      Chinstrap\n      Dream\n    \n  \n\n344 rows √ó 2 columns"
  },
  {
    "objectID": "guide/verb-select.html#select-by-pattern-e.g.-endswith",
    "href": "guide/verb-select.html#select-by-pattern-e.g.-endswith",
    "title": "Select columns",
    "section": "Select by pattern (e.g.¬†endswith)",
    "text": "Select by pattern (e.g.¬†endswith)\n\npenguins >> select(_.species, _.endswith(\"mm\"))\n\n\n\n\n\n  \n    \n      \n      species\n      bill_length_mm\n      bill_depth_mm\n      flipper_length_mm\n    \n  \n  \n    \n      0\n      Adelie\n      39.1\n      18.7\n      181.0\n    \n    \n      1\n      Adelie\n      39.5\n      17.4\n      186.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      342\n      Chinstrap\n      50.8\n      19.0\n      210.0\n    \n    \n      343\n      Chinstrap\n      50.2\n      18.7\n      198.0\n    \n  \n\n344 rows √ó 4 columns\n\n\n\n\npenguins >> select(_.contains(\"length\"))\n\n\n\n\n\n  \n    \n      \n      bill_length_mm\n      flipper_length_mm\n    \n  \n  \n    \n      0\n      39.1\n      181.0\n    \n    \n      1\n      39.5\n      186.0\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      342\n      50.8\n      210.0\n    \n    \n      343\n      50.2\n      198.0\n    \n  \n\n344 rows √ó 2 columns"
  },
  {
    "objectID": "guide/verb-select.html#pandas-comparison",
    "href": "guide/verb-select.html#pandas-comparison",
    "title": "Select columns",
    "section": "Pandas comparison",
    "text": "Pandas comparison\n\nimport pandas as pd\n\nfrom siuba.data import mtcars\nfrom siuba import select, _\n\nClick between tabs to compare code across siuba and pandas.\n\nSiubaPandas\n\n\n\n# keep cyl column\nmtcars >> select(_.cyl)\n\n# keep all *except* cyl column\nmtcars >> select(-_.cyl)\n\n# complex select, plus rename cyl to cylinder\nmtcars >> select(_.cylinder == _.cyl, _.startswith(\"m\"))\n\n\n\n\n# keep cyl column\nmtcars[[\"cyl\"]]\n\n# keep all *except* cyl column\nmtcars.drop([\"cyl\"], axis=1)\n\n# complex select, plus rename cyl to cylinder\ncols = mtcars.columns\nmtcars.loc[:, (cols == \"cyl\") | cols.str.startswith(\"m\")] \\\n      .rename({\"cyl\": \"cylinder\"})"
  },
  {
    "objectID": "guide/verb-summarize.html",
    "href": "guide/verb-summarize.html",
    "title": "Summarize to aggregate",
    "section": "",
    "text": "The summarize() creates new columns in your table, based on an aggregation. Aggregations take data and reduces it to a single number. When applied to grouped data, this function returns one row per grouping."
  },
  {
    "objectID": "guide/verb-summarize.html#summarize-over-all-rows",
    "href": "guide/verb-summarize.html#summarize-over-all-rows",
    "title": "Summarize to aggregate",
    "section": "Summarize over all rows",
    "text": "Summarize over all rows\n\nmtcars >> summarize(avg_mpg = _.mpg.mean())\nmtcars\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      15.0\n      8\n      301.0\n      335\n      3.54\n      3.570\n      14.60\n      0\n      1\n      5\n      8\n    \n    \n      31\n      21.4\n      4\n      121.0\n      109\n      4.11\n      2.780\n      18.60\n      1\n      1\n      4\n      2\n    \n  \n\n32 rows √ó 11 columns"
  },
  {
    "objectID": "guide/verb-summarize.html#summarize-over-groups",
    "href": "guide/verb-summarize.html#summarize-over-groups",
    "title": "Summarize to aggregate",
    "section": "Summarize over groups",
    "text": "Summarize over groups\nUse group_by() to split the data up, apply some aggregation, and then combine results.\n\n(mtcars\n  >> group_by(_.cyl)\n  >> summarize(\n       avg = _.mpg.mean(),\n       range = _.mpg.max() - _.mpg.min(),\n       avg_per_cyl = (_.mpg / _.cyl).mean()\n  )\n)\n\n\n\n\n\n  \n    \n      \n      cyl\n      avg\n      range\n      avg_per_cyl\n    \n  \n  \n    \n      0\n      4\n      26.663636\n      12.5\n      6.665909\n    \n    \n      1\n      6\n      19.742857\n      3.6\n      3.290476\n    \n    \n      2\n      8\n      15.100000\n      8.8\n      1.887500\n    \n  \n\n\n\n\nNote there are 3 unique groupings for cyl (4, 6, and 8), so the resulting table has 3 rows."
  },
  {
    "objectID": "guide/workflows-backends.html",
    "href": "guide/workflows-backends.html",
    "title": "Backend Examples üöß",
    "section": "",
    "text": "from siuba import _, group_by, filter, show_query, tbl\nfrom siuba.sql import LazyTbl\nfrom siuba.data import cars\n\nfrom sqlalchemy import create_engine"
  },
  {
    "objectID": "guide/workflows-backends.html#demo-query",
    "href": "guide/workflows-backends.html#demo-query",
    "title": "Backend Examples üöß",
    "section": "Demo query",
    "text": "Demo query\nWe‚Äôll use the following lazy pipe to demonstrate each backend. It groups by the cyl column, then filters by mpg.\n\nfilter_mpg = (\n    _\n    >> group_by(_.cyl)\n    >> filter(_.mpg < _.mpg.mean())\n)\n\nFor example, here is the pipe called on pandas data.\n\ncars >> filter_mpg\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      cyl\n      mpg\n      hp\n    \n  \n  \n    \n      2\n      4\n      22.8\n      93\n    \n    \n      5\n      6\n      18.1\n      105\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      8\n      15.0\n      335\n    \n    \n      31\n      4\n      21.4\n      109\n    \n  \n\n16 rows √ó 3 columns"
  },
  {
    "objectID": "guide/workflows-backends.html#duckdb",
    "href": "guide/workflows-backends.html#duckdb",
    "title": "Backend Examples üöß",
    "section": "duckdb",
    "text": "duckdb\n\nengine = create_engine(\"duckdb:///:memory:\")\n\ntbl_cars_duckdb = tbl(engine, \"cars\", cars)\n\n/opt/hostedtoolcache/Python/3.9.15/x64/lib/python3.9/site-packages/duckdb_engine/__init__.py:229: DuckDBEngineWarning: duckdb-engine doesn't yet support reflection on indices\n  warnings.warn(\n\n\n\ntbl_filtered = tbl_cars_duckdb >> filter_mpg\ntbl_filtered\n\n\n# Source: lazy query\n# DB Conn: Engine(duckdb:///:memory:)\n# Preview:\n\n\n\n  \n    \n      \n      cyl\n      mpg\n      hp\n    \n  \n  \n    \n      0\n      4\n      22.8\n      93\n    \n    \n      1\n      4\n      24.4\n      62\n    \n    \n      2\n      4\n      22.8\n      95\n    \n    \n      3\n      4\n      21.5\n      97\n    \n    \n      4\n      4\n      26.0\n      91\n    \n  \n\n# .. may have more rows\n\n\n\nq = tbl_filtered >> show_query(simplify=True)\n\nSELECT anon_1.cyl, anon_1.mpg, anon_1.hp \nFROM (SELECT *, cars_1.mpg < avg(cars_1.mpg) OVER (PARTITION BY cars_1.cyl) AS win1 \nFROM cars AS cars_1) AS anon_1 \nWHERE anon_1.win1"
  },
  {
    "objectID": "guide/workflows-backends.html#sqlite",
    "href": "guide/workflows-backends.html#sqlite",
    "title": "Backend Examples üöß",
    "section": "sqlite",
    "text": "sqlite\n\nengine = create_engine(\"sqlite:///:memory:\")\ncars.to_sql(\"cars\", engine, index=False)\n\ntbl_cars_sqlite = tbl(engine, \"cars\")\n\n\ntbl_filtered = tbl_cars_sqlite >> filter_mpg\ntbl_filtered\n\n/home/runner/work/siuba-guide/siuba-guide/src/siuba/siuba/sql/utils.py:85: SAWarning: Class AggOver will not make use of SQL compilation caching as it does not set the 'inherit_cache' attribute to ``True``.  This can have significant performance implications including some performance degradations in comparison to prior SQLAlchemy versions.  Set this attribute to True if this object can make use of the cache key generated by the superclass.  Alternatively, this attribute may be set to False which will disable this warning. (Background on this error at: https://sqlalche.me/e/14/cprf)\n  return self.connectable.execute(*args, **kwargs)\n\n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n  \n    \n      \n      cyl\n      mpg\n      hp\n    \n  \n  \n    \n      0\n      4\n      22.8\n      93\n    \n    \n      1\n      4\n      24.4\n      62\n    \n    \n      2\n      4\n      22.8\n      95\n    \n    \n      3\n      4\n      21.5\n      97\n    \n    \n      4\n      4\n      26.0\n      91\n    \n  \n\n# .. may have more rows\n\n\n\nq = tbl_filtered >> show_query(simplify=True)\n\nSELECT anon_1.cyl, anon_1.mpg, anon_1.hp \nFROM (SELECT *, cars_1.mpg < avg(cars_1.mpg) OVER (PARTITION BY cars_1.cyl) AS win1 \nFROM cars AS cars_1) AS anon_1 \nWHERE anon_1.win1 = 1"
  },
  {
    "objectID": "guide/workflows-backends.html#bigquery",
    "href": "guide/workflows-backends.html#bigquery",
    "title": "Backend Examples üöß",
    "section": "bigquery",
    "text": "bigquery\nbigquery is thoroughly tested, but needs to be added to this page!"
  },
  {
    "objectID": "guide/workflows-backends.html#mysql",
    "href": "guide/workflows-backends.html#mysql",
    "title": "Backend Examples üöß",
    "section": "mysql",
    "text": "mysql\nmysql is thoroughly tested, but needs to be added to this page!"
  },
  {
    "objectID": "guide/workflows-backends.html#postgresql",
    "href": "guide/workflows-backends.html#postgresql",
    "title": "Backend Examples üöß",
    "section": "postgresql",
    "text": "postgresql\npostgresql is thoroughly tested, but needs to be added to this page!"
  },
  {
    "objectID": "guide/workflows-backends.html#snowflake",
    "href": "guide/workflows-backends.html#snowflake",
    "title": "Backend Examples üöß",
    "section": "snowflake",
    "text": "snowflake\nsnowflake is thoroughly tested, but needs to be added to this page!"
  },
  {
    "objectID": "guide/wrangle-helpers.html",
    "href": "guide/wrangle-helpers.html",
    "title": "Helpers: count, separate, complete",
    "section": "",
    "text": "Some combinations of verbs and column operations get used so frequently that they earn their own helper verbs. These helpers make things a little quicker or concise to type.\nThis page discusses 3 helper functions that will super charge your workflow:"
  },
  {
    "objectID": "guide/wrangle-helpers.html#count-values",
    "href": "guide/wrangle-helpers.html#count-values",
    "title": "Helpers: count, separate, complete",
    "section": "Count values",
    "text": "Count values\nUse count() to quickly tally up the number of rows per grouping.\n\nmtcars >> count(_.cyl, _.gear)\n\n\n\n\n\n  \n    \n      \n      cyl\n      gear\n      n\n    \n  \n  \n    \n      0\n      4\n      3\n      1\n    \n    \n      1\n      4\n      4\n      8\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      6\n      8\n      3\n      12\n    \n    \n      7\n      8\n      5\n      2\n    \n  \n\n8 rows √ó 3 columns\n\n\n\nThe output above has 8 rows‚Äîone for each grouping‚Äîand a column named n with the number of observations in each grouping.\nNote that count() is a shorthand for combining group_by() and summarize().\n\nfrom siuba import group_by, summarize\n\nmtcars >> group_by(_.cyl, _.gear) >> summarize(n = _.shape[0])\n\n\n\n\n\n  \n    \n      \n      cyl\n      gear\n      n\n    \n  \n  \n    \n      0\n      4\n      3\n      1\n    \n    \n      1\n      4\n      4\n      8\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      6\n      8\n      3\n      12\n    \n    \n      7\n      8\n      5\n      2\n    \n  \n\n8 rows √ó 3 columns\n\n\n\n\nSorting results\nUse the sort argument to sort results by number of observations, in descending order.\n\nmtcars >> count(_.cyl, sort=True)\n\n\n\n\n\n  \n    \n      \n      cyl\n      n\n    \n  \n  \n    \n      0\n      8\n      14\n    \n    \n      1\n      4\n      11\n    \n    \n      2\n      6\n      7\n    \n  \n\n\n\n\n\n\nWith expressions\n\nmtcars >> count(_.cyl, high_mpg = _.mpg > 30)\n\n\n\n\n\n  \n    \n      \n      cyl\n      high_mpg\n      n\n    \n  \n  \n    \n      0\n      4\n      False\n      7\n    \n    \n      1\n      4\n      True\n      4\n    \n    \n      2\n      6\n      False\n      7\n    \n    \n      3\n      8\n      False\n      14\n    \n  \n\n\n\n\nHere‚Äôs a somewhat advanced example that puts mpg in different bins.\n\nfrom siuba import count\nfrom siuba.siu import call\nfrom siuba.data import mtcars\n\ncar_counts = mtcars >> count(_.cyl, mpg = call(pd.cut, _.mpg, bins = 3))\ncar_counts\n\n\n\n\n\n  \n    \n      \n      cyl\n      mpg\n      n\n    \n  \n  \n    \n      0\n      4\n      (10.376, 18.233]\n      0\n    \n    \n      1\n      4\n      (18.233, 26.067]\n      6\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      7\n      8\n      (18.233, 26.067]\n      2\n    \n    \n      8\n      8\n      (26.067, 33.9]\n      0\n    \n  \n\n9 rows √ó 3 columns\n\n\n\nNote these important pieces:\n\ncall(pd.cut, ...) is used to lazily call pd.cut(...).\npd.cut splits the data into bins, that count then uses as a grouping.\n\nHere‚Äôs a barchart of this data in plotnine.\n\nfrom plotnine import ggplot, aes, geom_col, facet_wrap, theme, element_text\n\n(car_counts\n    >> ggplot(aes(\"mpg\", \"n\", fill=\"mpg\"))\n    + geom_col()\n    + facet_wrap(\"~cyl\")\n    + theme(axis_text_x = element_text(angle=45, hjust=1))\n)\n\n\n\n\n<ggplot: (8781648485464)>\n\n\nAs vehicles increase in cylinders, they have fewer low mpg vehicles. For example, no 8 cylinder vehicles are in the highest mpg bin (above 26 mpg).\n\n\nWeighted counts\nUse the wt argument to do a weighted count (i.e.¬†sum the weight values per grouping).\nThis is useful if you have data that already contains a count, such as the count of cyl and gear below.\n\ntbl_n_cyl_gear = mtcars >> count(_.cyl, _.gear)\n\nThe wt argument lets us roll this result up to count observations per cyl.\n\ntbl_n_cyl_gear >> count(_.cyl, wt=_.n)\n\n\n\n\n\n  \n    \n      \n      cyl\n      nn\n    \n  \n  \n    \n      0\n      4\n      11\n    \n    \n      1\n      6\n      7\n    \n    \n      2\n      8\n      14\n    \n  \n\n\n\n\nNotice that this is equivalent to counting cyl on the raw data.\n\nmtcars >> count(_.cyl)\n\n\n\n\n\n  \n    \n      \n      cyl\n      n\n    \n  \n  \n    \n      0\n      4\n      11\n    \n    \n      1\n      6\n      7\n    \n    \n      2\n      8\n      14"
  },
  {
    "objectID": "guide/wrangle-helpers.html#separate-strings-into-columns",
    "href": "guide/wrangle-helpers.html#separate-strings-into-columns",
    "title": "Helpers: count, separate, complete",
    "section": "Separate strings into columns",
    "text": "Separate strings into columns\nUse separate() to split a column using a pattern, and produce new columns.\nBy default, it splits strings on any non-alphanumeric character, so is helpful for quickly splitting in common situations where values are seperated by dashes (e.g.¬†‚Äúa-1‚Äù).\nFor example, here is some data where the condition column could be split on \"-\".\n\nmeasures = pd.DataFrame({\n    \"condition\": [\"a-1\", \"a-2\", \"b-1\", \"b-2\"],\n    \"value\": [1, 2, 3, 4]\n})\n\nmeasures\n\n\n\n\n\n  \n    \n      \n      condition\n      value\n    \n  \n  \n    \n      0\n      a-1\n      1\n    \n    \n      1\n      a-2\n      2\n    \n    \n      2\n      b-1\n      3\n    \n    \n      3\n      b-2\n      4\n    \n  \n\n\n\n\nSeparate takes the column we want to split, the names of the new columns. It produces a new table with the new columns in the place of the old one.\n\nmeasures >> separate(_.condition, [\"name\", \"number\"])\n\n\n\n\n\n  \n    \n      \n      value\n      name\n      number\n    \n  \n  \n    \n      0\n      1\n      a\n      1\n    \n    \n      1\n      2\n      a\n      2\n    \n    \n      2\n      3\n      b\n      1\n    \n    \n      3\n      4\n      b\n      2\n    \n  \n\n\n\n\nNotice that the condition column got split into name and number columns."
  },
  {
    "objectID": "guide/wrangle-helpers.html#complete-combinations-of-data",
    "href": "guide/wrangle-helpers.html#complete-combinations-of-data",
    "title": "Helpers: count, separate, complete",
    "section": "Complete combinations of data",
    "text": "Complete combinations of data\nUse complete() to fill in missing combinations in the data.\nFor example, the code below counts penguins across species and island, but not all species are on each island.\n\ntbl_n_penguins = penguins >> count(_.species, _.island)\ntbl_n_penguins\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      n\n    \n  \n  \n    \n      0\n      Adelie\n      Biscoe\n      44\n    \n    \n      1\n      Adelie\n      Dream\n      56\n    \n    \n      2\n      Adelie\n      Torgersen\n      52\n    \n    \n      3\n      Chinstrap\n      Dream\n      68\n    \n    \n      4\n      Gentoo\n      Biscoe\n      124\n    \n  \n\n\n\n\nNotice the following in the count:\n\nAdelie penguins are on three islands, BUT\nChinstrap are only on Dream.\nGentoo are only on Biscoe.\n\nWe can use complete to add rows with 0 counts for islands Chinstrap and Gentoo aren‚Äôt on.\n\ntbl_n_penguins >> complete(_.species, _.island, fill = {\"n\": 0})\n\n\n\n\n\n  \n    \n      \n      species\n      island\n      n\n    \n  \n  \n    \n      0\n      Adelie\n      Biscoe\n      44.0\n    \n    \n      1\n      Adelie\n      Dream\n      56.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      7\n      Gentoo\n      Dream\n      0.0\n    \n    \n      8\n      Gentoo\n      Torgersen\n      0.0\n    \n  \n\n9 rows √ó 3 columns\n\n\n\nNow the data has 9 rows, one for each combination of species and island. The fill = {\"n\": 0} argument allowed us to set a default value of 0 for the previously missing rows."
  },
  {
    "objectID": "guide/wrangle-joins.html",
    "href": "guide/wrangle-joins.html",
    "title": "Join tables",
    "section": "",
    "text": "Joins allow you to combine data from two tables, using two steps:\nThe number of ways joins can be used is surprisingly deep, and provides an important foundation for most data analyses!"
  },
  {
    "objectID": "guide/wrangle-joins.html#syntax",
    "href": "guide/wrangle-joins.html#syntax",
    "title": "Join tables",
    "section": "Syntax",
    "text": "Syntax\nLike other siuba verbs, joins can be used in two ways: by directly passing both data as arguments, or by piping.\n\n# directly passing data\ninner_join(lhs, rhs, on=\"id\")\n\n# piping\nlhs >> inner_join(_, rhs, on=\"id\")\n\n\n\n\n\n  \n    \n      \n      id\n      val_x\n      val_y\n    \n  \n  \n    \n      0\n      1\n      lhs.1\n      rhs.1\n    \n    \n      1\n      2\n      lhs.2\n      rhs.2\n    \n  \n\n\n\n\nNote that when used in a pipe, the first argument to the join must be _, to represent the data."
  },
  {
    "objectID": "guide/wrangle-joins.html#how-joins-work",
    "href": "guide/wrangle-joins.html#how-joins-work",
    "title": "Join tables",
    "section": "How joins work",
    "text": "How joins work\nThis page will use graphs to illustrate how joins work. They show the values being joined on for the left-hand side on the rows, and those for the right-hand side on the columns.\nFor example, consider the inner join from the last section:\n\ninner_join(lhs, rhs, on=\"id\")\n\n\n\n\n\n  \n    \n      \n      id\n      val_x\n      val_y\n    \n  \n  \n    \n      0\n      1\n      lhs.1\n      rhs.1\n    \n    \n      1\n      2\n      lhs.2\n      rhs.2\n    \n  \n\n\n\n\nHere is a graph for this inner join:\n\n\n\n\n\n\n\n\n\n\n\nNotice that 1, 2, 3 come from the lhs.id column, and 1, 2, 4 from rhs.id.\nThere are two steps to any join:\n\nDetermine matches. The colored squares mark new rows that will be created. These are determined by looking at every pair of values from the left- and right-hand sides, and checking whether they‚Äôre equal. This means that there are 9 checks in total (one for each square in the grid).\nMerge rows. For each match, a row is created that has columns from both tables.\n\nThe next two sections on this page discuss different types of joins:\n\nMutating joins: what should happen if a row or column doesn‚Äôt have any matches?\nFiltering joins: using matching rules without merging to filter the left-hand table.\n\nThe remaining four sections discuss important situations related to matching: duplicates, missing values, using multiple columns, and expressions."
  },
  {
    "objectID": "guide/wrangle-joins.html#mutating-joins",
    "href": "guide/wrangle-joins.html#mutating-joins",
    "title": "Join tables",
    "section": "Mutating joins",
    "text": "Mutating joins\nUp to this point we‚Äôve looked at an inner join. We‚Äôll call this a mutating join, because it puts the columns from both tables into the final result. In this section, we‚Äôll look at two more important joins: left join and full join.\nThe diagram below shows these three mutating joins.\n\nLeft and full joins define a new matching rule for rows of data that don‚Äôt have any matches:\nLeft join. Include rows from the left-hand table that didn‚Äôt have any matches. This is done by matching left-hand rows with no matches, to a ‚Äúdummy‚Äù right-hand row of all missing values. This is signified in the graph by the circled NA column.\nFull join: Similar to left join, but extended to include unmatched rows from both tables. Notice that the graph for full join has two ‚Äúdummy circled NA‚Äù entries, one to match rows, and one to match columns.\n\nInner join\n\ninner_join(lhs, rhs, on=\"id\")\n\n\n\n\n\n  \n    \n      \n      id\n      val_x\n      val_y\n    \n  \n  \n    \n      0\n      1\n      lhs.1\n      rhs.1\n    \n    \n      1\n      2\n      lhs.2\n      rhs.2\n    \n  \n\n\n\n\n\n\nOuter joins\n\nleft_join(lhs, rhs, on=\"id\")\n\n\n\n\n\n  \n    \n      \n      id\n      val_x\n      val_y\n    \n  \n  \n    \n      0\n      1\n      lhs.1\n      rhs.1\n    \n    \n      1\n      2\n      lhs.2\n      rhs.2\n    \n    \n      2\n      3\n      lhs.3\n      NaN\n    \n  \n\n\n\n\n\nfull_join(lhs, rhs, on=\"id\")\n\n\n\n\n\n  \n    \n      \n      id\n      val_x\n      val_y\n    \n  \n  \n    \n      0\n      1\n      lhs.1\n      rhs.1\n    \n    \n      1\n      2\n      lhs.2\n      rhs.2\n    \n    \n      2\n      3\n      lhs.3\n      NaN\n    \n    \n      3\n      4\n      NaN\n      rhs.3"
  },
  {
    "objectID": "guide/wrangle-joins.html#filtering-joins",
    "href": "guide/wrangle-joins.html#filtering-joins",
    "title": "Join tables",
    "section": "Filtering joins",
    "text": "Filtering joins\nFiltering joins use some of the same logic to determine matches as a join, but only to filter (remove) rows from the left-hand table.\nWe‚Äôll use the data from the previous section to look at filtering joins.\n\n\n\nlhs\n\n\n\n\n\n  \n    \n      \n      id\n      val\n    \n  \n  \n    \n      0\n      1\n      lhs.1\n    \n    \n      1\n      2\n      lhs.2\n    \n    \n      2\n      3\n      lhs.3\n    \n  \n\n\n\n\n\nrhs\n\n\n\n\n\n  \n    \n      \n      id\n      val\n    \n  \n  \n    \n      0\n      1\n      rhs.1\n    \n    \n      1\n      2\n      rhs.2\n    \n    \n      2\n      4\n      rhs.3\n    \n  \n\n\n\n\n\n\nUse semi_join() to keep rows in the left-hand table that would have a match:\n\nsemi_join(lhs, rhs, on=\"id\")\n\n\n\n\n\n  \n    \n      \n      id\n      val\n    \n  \n  \n    \n      0\n      1\n      lhs.1\n    \n    \n      1\n      2\n      lhs.2\n    \n  \n\n\n\n\nUse anti_join() to keep the rows that a semi_join() would drop‚Äîthe rows with no matches:\n\nanti_join(lhs, rhs, on=\"id\")\n\n\n\n\n\n  \n    \n      \n      id\n      val\n    \n  \n  \n    \n      2\n      3\n      lhs.3"
  },
  {
    "objectID": "guide/wrangle-joins.html#duplicate-matches",
    "href": "guide/wrangle-joins.html#duplicate-matches",
    "title": "Join tables",
    "section": "Duplicate matches",
    "text": "Duplicate matches\nMutating joins create a new row for each match between value pairs. This means that a join can duplicate rows from the left- or right-hand data.\nFor example, consider the situation where the left- and right-hand data both have the value 2 multiple times in the columns they‚Äôre joining on.\n\n\n\n\n\n\n\n\n\n\n\nThis is shown in the graph above‚Äîfor an inner join, where both tables contain the value 2 twice. In this case, it results in 4 matches, which will each produce a row in the final result.\nHere is the code corresponding to the graph above.\n\nimport pandas as pd\nlhs_dupes = pd.DataFrame({\n    \"id\": [1, 2, 2, 3], \n    \"val\": [\"lhs.1\", \"lhs.2\", \"lhs.3\", \"lhs.4\"]\n})\n\nrhs_dupes = pd.DataFrame({\n    \"id\": [1, 2, 2, 4],\n    \"val\": [\"rhs.1\", \"rhs.2\", \"rhs.3\", \"rhs.4\"]\n})\n\n\n\n\nlhs_dupes\n\n\n\n\n\n  \n    \n      \n      id\n      val\n    \n  \n  \n    \n      0\n      1\n      lhs.1\n    \n    \n      1\n      2\n      lhs.2\n    \n    \n      2\n      2\n      lhs.3\n    \n    \n      3\n      3\n      lhs.4\n    \n  \n\n\n\n\n\nrhs_dupes\n\n\n\n\n\n  \n    \n      \n      id\n      val\n    \n  \n  \n    \n      0\n      1\n      rhs.1\n    \n    \n      1\n      2\n      rhs.2\n    \n    \n      2\n      2\n      rhs.3\n    \n    \n      3\n      4\n      rhs.4\n    \n  \n\n\n\n\n\n\n\ninner_join(lhs_dupes, rhs_dupes, on=\"id\")\n\n\n\n\n\n  \n    \n      \n      id\n      val_x\n      val_y\n    \n  \n  \n    \n      0\n      1\n      lhs.1\n      rhs.1\n    \n    \n      1\n      2\n      lhs.2\n      rhs.2\n    \n    \n      2\n      2\n      lhs.2\n      rhs.3\n    \n    \n      3\n      2\n      lhs.3\n      rhs.2\n    \n    \n      4\n      2\n      lhs.3\n      rhs.3"
  },
  {
    "objectID": "guide/wrangle-joins.html#na-handling",
    "href": "guide/wrangle-joins.html#na-handling",
    "title": "Join tables",
    "section": "NA handling",
    "text": "NA handling\nBy default, missing values are matched to each other in joins.\nThis is shown in the left join below, where each table has a single NA value.\n\n\n\n\n\n\n\n\n\n\n\nNote the purple mark indicating a match between the NA values. Importantly, literal NA values here are different from the circled dummy NA (top right), which indicates the special matching rule for left joins.\nHere is the code for the example:\n\nimport pandas as pd\nlhs_na = pd.DataFrame({\"id\": [1, pd.NA, 3], 'val': ['lhs.1', 'lhs.2', 'lhs.3']})\nrhs_na = pd.DataFrame({\"id\": [1, pd.NA, 2], 'val': ['rhs.1', 'rhs.2', 'rhs.3']})\nleft_join(lhs_na, rhs_na, on=\"id\")\n\n\n\n\n\n  \n    \n      \n      id\n      val_x\n      val_y\n    \n  \n  \n    \n      0\n      1\n      lhs.1\n      rhs.1\n    \n    \n      1\n      <NA>\n      lhs.2\n      rhs.2\n    \n    \n      2\n      3\n      lhs.3\n      NaN"
  },
  {
    "objectID": "guide/wrangle-joins.html#match-on-multiple-columns",
    "href": "guide/wrangle-joins.html#match-on-multiple-columns",
    "title": "Join tables",
    "section": "Match on multiple columns",
    "text": "Match on multiple columns\nJoins can be performed by comparing multiple pairs of columns to each other‚Äî such as the source and id columns in the data below.\n\nlhs_multi = pd.DataFrame({\n    \"source\": [\"a\", \"a\", \"b\"],\n    \"id\": [1, 2, 1],\n    \"val\": [\"lhs.1\", \"lhs.2\", \"lhs.3\"]\n})\n\nrhs_multi = pd.DataFrame({\n    \"source\": [\"a\", \"b\", \"c\"],\n    \"id\": [1, 1, 1],\n    \"val\": [\"rhs.1\", \"rhs.2\", \"rhs.3\"]\n})\n\n\n\n\nlhs_multi\n\n\n\n\n\n  \n    \n      \n      source\n      id\n      val\n    \n  \n  \n    \n      0\n      a\n      1\n      lhs.1\n    \n    \n      1\n      a\n      2\n      lhs.2\n    \n    \n      2\n      b\n      1\n      lhs.3\n    \n  \n\n\n\n\n\nrhs_multi\n\n\n\n\n\n  \n    \n      \n      source\n      id\n      val\n    \n  \n  \n    \n      0\n      a\n      1\n      rhs.1\n    \n    \n      1\n      b\n      1\n      rhs.2\n    \n    \n      2\n      c\n      1\n      rhs.3\n    \n  \n\n\n\n\n\n\nIn this case, a match happens when source matches AND id matches‚Äîas shown by the composite values (source, id) in the graph below.\n\n\n\n\n\n\n\n\n\n\n\n\nUsing a list of columns\nUse the on= argument with a list of columns, in order to join on multiple columns.\n\ninner_join(lhs_multi, rhs_multi, on=[\"source\", \"id\"])\n\n\n\n\n\n  \n    \n      \n      source\n      id\n      val_x\n      val_y\n    \n  \n  \n    \n      0\n      a\n      1\n      lhs.1\n      rhs.1\n    \n    \n      1\n      b\n      1\n      lhs.3\n      rhs.2\n    \n  \n\n\n\n\n\n\nUsing a dictionary of columns\nUse the on= argument with a dictionary to join pairs of columns with different names.\n\nfrom siuba import rename\n\nrhs_multi2 = rename(rhs_multi, some_source = \"source\")\n\ninner_join(lhs_multi, rhs_multi2, {\"source\": \"some_source\", \"id\": \"id\"})\n\n\n\n\n\n  \n    \n      \n      source\n      id\n      val_x\n      some_source\n      val_y\n    \n  \n  \n    \n      0\n      a\n      1\n      lhs.1\n      a\n      rhs.1\n    \n    \n      1\n      b\n      1\n      lhs.3\n      b\n      rhs.2"
  },
  {
    "objectID": "guide/wrangle-joins.html#match-on-expressions",
    "href": "guide/wrangle-joins.html#match-on-expressions",
    "title": "Join tables",
    "section": "Match on expressions",
    "text": "Match on expressions\nSome siuba backends‚Äîlike those that execute SQL‚Äîcan join by expressions for determining matches.\nFor example, the graph below shows an inner join, where a match occurs when the left-hand value is less than or equal to the right-hand value.\n\n\n\n\n\n\n\n\n\n\n\nNotice that the value 1 on the left-hand side matches everything on the right-hand side (top row).\n\nSQL backend sql_on argument.\nHere is a full example, using sqlite:\n\nfrom sqlalchemy import create_engine\nfrom siuba.sql import LazyTbl\n\nengine = create_engine(\"sqlite:///:memory:\")\n\nlhs = pd.DataFrame({'id': [1,2,3], 'val': ['lhs.1', 'lhs.2', 'lhs.3']})\nrhs = pd.DataFrame({'id': [1,2,4], 'val': ['rhs.1', 'rhs.2', 'rhs.3']})\n\nlhs.to_sql(\"lhs\", engine, index=False)\nrhs.to_sql(\"rhs\", engine, index=False)\n\ntbl_sql_lhs = LazyTbl(engine, \"lhs\")\ntbl_sql_rhs = LazyTbl(engine, \"rhs\")\n\ninner_join(\n    tbl_sql_lhs,\n    tbl_sql_rhs,\n    sql_on = lambda lhs, rhs: lhs.val <= rhs.val\n)\n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n  \n    \n      \n      id_x\n      val_x\n      val_y\n      id_y\n    \n  \n  \n    \n      0\n      1\n      lhs.1\n      rhs.1\n      1\n    \n    \n      1\n      1\n      lhs.1\n      rhs.2\n      2\n    \n    \n      2\n      1\n      lhs.1\n      rhs.3\n      4\n    \n    \n      3\n      2\n      lhs.2\n      rhs.1\n      1\n    \n    \n      4\n      2\n      lhs.2\n      rhs.2\n      2\n    \n  \n\n# .. may have more rows\n\n\nNote that the function passed to sql_on takes sqlalchemy columns for the left- and right-hand sides, so currently limited to what can be done in sqlalchemy."
  },
  {
    "objectID": "guide/wrangle-reshape.html",
    "href": "guide/wrangle-reshape.html",
    "title": "Reshape tables üìù",
    "section": "",
    "text": "import pandas as pd\nfrom siuba import _, spread, gather\n\ncosts = pd.DataFrame({\n    'id': [1,2],\n    'price_x': [.1, .2],\n    'price_y': [.4, .5],\n    'price_z': [.7, .8]\n})\n\ncosts\n\n\n\n\n\n  \n    \n      \n      id\n      price_x\n      price_y\n      price_z\n    \n  \n  \n    \n      0\n      1\n      0.1\n      0.4\n      0.7\n    \n    \n      1\n      2\n      0.2\n      0.5\n      0.8"
  },
  {
    "objectID": "guide/wrangle-reshape.html#gather-data-long",
    "href": "guide/wrangle-reshape.html#gather-data-long",
    "title": "Reshape tables üìù",
    "section": "Gather data long",
    "text": "Gather data long\n\n# selecting each variable manually\ncosts >> gather('measure', 'value', _.price_x, _.price_y, _.price_z)\n\n# selecting variables using a slice\ncosts >> gather('measure', 'value', _[\"price_x\":\"price_z\"])\n\n# selecting by excluding id\ncosts >> gather('measure', 'value', -_.id)\n\n\n\n\n\n  \n    \n      \n      id\n      measure\n      value\n    \n  \n  \n    \n      0\n      1\n      price_x\n      0.1\n    \n    \n      1\n      2\n      price_x\n      0.2\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      4\n      1\n      price_z\n      0.7\n    \n    \n      5\n      2\n      price_z\n      0.8\n    \n  \n\n6 rows √ó 3 columns"
  },
  {
    "objectID": "guide/wrangle-reshape.html#spread-data-wide",
    "href": "guide/wrangle-reshape.html#spread-data-wide",
    "title": "Reshape tables üìù",
    "section": "Spread data wide",
    "text": "Spread data wide\n\n(costs\n    >> gather('measure', 'value', -_.id)\n    >> spread('measure', 'value')\n)\n\n\n\n\n\n  \n    \n      \n      id\n      price_x\n      price_y\n      price_z\n    \n  \n  \n    \n      0\n      1\n      0.1\n      0.4\n      0.7\n    \n    \n      1\n      2\n      0.2\n      0.5\n      0.8"
  },
  {
    "objectID": "guide/wrangle-reshape.html#experimental-pivot_wider-pivot_longer",
    "href": "guide/wrangle-reshape.html#experimental-pivot_wider-pivot_longer",
    "title": "Reshape tables üìù",
    "section": "Experimental pivot_wider, pivot_longer",
    "text": "Experimental pivot_wider, pivot_longer\n\n\n\n\n\n\nWarning\n\n\n\nThese functions are thoroughly tested, but currently experimental. The sections below describe their basic use. See their reference page for more examples.\n\n\n\nfrom siuba.experimental.pivot import pivot_wider, pivot_longer\nfrom siuba import Fx\n\n\npivot_wider()\nIf there would be multiple entries per cell in the spread wide data, then the spread() function raises an error.\nThis is shown below, where there are duplicate entries where id=1 and measure=\"a\".\n\ndf = pd.DataFrame({\n    \"id\": [1, 1, 2],\n    \"measure\": [\"a\", \"a\", \"b\"],\n    \"value\": [8, 9, 10]\n})\n\ndf >> spread(\"measure\", \"value\")\n\nValueError: Index contains duplicate entries, cannot reshape\n\n\nUse the pivot_wider() function to deal with this situation.\n\ndf >> pivot_wider(_.id, names_from=_.measure, values_from=_.value, values_fn=tuple)\n\n\n\n\n\n  \n    \n      \n      id\n      a\n      b\n    \n  \n  \n    \n      0\n      1\n      (8, 9)\n      NaN\n    \n    \n      1\n      2\n      NaN\n      (10,)\n    \n  \n\n\n\n\nNotice that the top-left entry is a list of two values, (8, 9). The values_fn argument is able to reduce those values down to one.\nFor example, by taking the mean.\n\ndf >> pivot_wider(_.id, names_from=_.measure, values_from=_.value, values_fn=\"mean\")\n\n\n\n\n\n  \n    \n      \n      id\n      a\n      b\n    \n  \n  \n    \n      0\n      1\n      8.5\n      NaN\n    \n    \n      1\n      2\n      NaN\n      10.0\n    \n  \n\n\n\n\nNote that values_fn= may be a lambda function.\n\n\n\n\n\n\nNote\n\n\n\nIf you‚Äôd rather use pandas‚Äôs pivot_table() method, siuba can pipe to it using this syntax:\n\n(df\n    >> _.pivot_table(...)\n)\n\nWhere you would replace ... with your arguments. See flexible piping for more details.\n\n\n\n\npivot_longer()\nUse pivot_longer() to stack columns of data, turning them into rows.\n\nwide = pd.DataFrame({\"id\": [1, 2], \"x\": [5, 6], \"y\": [7, 8]})\n\nwide\n\n\n\n\n\n  \n    \n      \n      id\n      x\n      y\n    \n  \n  \n    \n      0\n      1\n      5\n      7\n    \n    \n      1\n      2\n      6\n      8\n    \n  \n\n\n\n\nNotice that this data has two columns (x and y) that you might want to stack on top of each other.\n\ndf >> pivot_longer(~_.id, names_to=\"variable\", values_to=\"number\")\n\n\n\n\n\n  \n    \n      \n      id\n      variable\n      number\n    \n  \n  \n    \n      0\n      1\n      measure\n      a\n    \n    \n      0\n      1\n      value\n      8\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      2\n      2\n      measure\n      b\n    \n    \n      2\n      2\n      value\n      10\n    \n  \n\n6 rows √ó 3 columns\n\n\n\nNote that in the code above, we do the following:\n\nUse ~_.id selects every columns except id for stacking.\nSet column names to be stored in a new variable column.\nSet column values to be stored in a new value column."
  },
  {
    "objectID": "examples/golden_age_of_tv.html",
    "href": "examples/golden_age_of_tv.html",
    "title": "Golden Age of Television Analysis",
    "section": "",
    "text": "import pandas as pd\n\nfrom siuba import *\nfrom siuba.dply.vector import row_number, n\n\nfrom plotnine import *"
  },
  {
    "objectID": "examples/golden_age_of_tv.html#glance-at-data-for-a-single-show",
    "href": "examples/golden_age_of_tv.html#glance-at-data-for-a-single-show",
    "title": "Golden Age of Television Analysis",
    "section": "Glance at data for a single show",
    "text": "Glance at data for a single show\n\ntv_ratings >> filter(_, _.title.str.contains(\"Buffy\"))\n\n\n\n\n\n  \n    \n      \n      titleId\n      seasonNumber\n      title\n      date\n      av_rating\n      share\n      genres\n    \n  \n  \n    \n      275\n      tt0118276\n      1\n      Buffy the Vampire Slayer\n      1997-04-14\n      7.9629\n      11.70\n      Action,Drama,Fantasy\n    \n    \n      276\n      tt0118276\n      2\n      Buffy the Vampire Slayer\n      1997-12-31\n      8.4191\n      19.41\n      Action,Drama,Fantasy\n    \n    \n      277\n      tt0118276\n      3\n      Buffy the Vampire Slayer\n      1999-01-29\n      8.6233\n      17.12\n      Action,Drama,Fantasy\n    \n    \n      278\n      tt0118276\n      4\n      Buffy the Vampire Slayer\n      2000-01-19\n      8.2205\n      16.19\n      Action,Drama,Fantasy\n    \n    \n      279\n      tt0118276\n      5\n      Buffy the Vampire Slayer\n      2001-01-12\n      8.3028\n      11.99\n      Action,Drama,Fantasy\n    \n    \n      280\n      tt0118276\n      6\n      Buffy the Vampire Slayer\n      2002-01-29\n      8.1008\n      8.45\n      Action,Drama,Fantasy\n    \n    \n      281\n      tt0118276\n      7\n      Buffy the Vampire Slayer\n      2003-01-18\n      8.0460\n      9.89\n      Action,Drama,Fantasy"
  },
  {
    "objectID": "examples/golden_age_of_tv.html#count-season-number",
    "href": "examples/golden_age_of_tv.html#count-season-number",
    "title": "Golden Age of Television Analysis",
    "section": "Count season number",
    "text": "Count season number\n\n(tv_ratings\n  >> count(_, _.seasonNumber)\n  >> ggplot(aes(\"seasonNumber\", \"n\"))\n   + geom_col()\n   + labs(\n       title = \"Season Number Frequency\",\n       x = \"season number\",\n       y = \"count\"\n  )\n)\n\n\n\n\n<ggplot: (8742457309675)>"
  },
  {
    "objectID": "examples/golden_age_of_tv.html#average-rating-throughout-season",
    "href": "examples/golden_age_of_tv.html#average-rating-throughout-season",
    "title": "Golden Age of Television Analysis",
    "section": "Average rating throughout season",
    "text": "Average rating throughout season\n\n(tv_ratings\n  >> filter(_, _.seasonNumber <= 7)\n  >> group_by(_, _.seasonNumber)\n  >> summarize(_, av_rating = _.av_rating.mean())\n  >> ggplot(aes(\"seasonNumber\", \"av_rating\"))\n   + geom_line()\n   + labs(\n       title = \"Average rating across seasons\",\n       x = \"season number\",\n       y = \"average rating\"\n  )\n)\n\n\n\n\n<ggplot: (8742457244853)>"
  },
  {
    "objectID": "examples/golden_age_of_tv.html#shows-with-most-variable-ratings",
    "href": "examples/golden_age_of_tv.html#shows-with-most-variable-ratings",
    "title": "Golden Age of Television Analysis",
    "section": "Shows with most variable ratings",
    "text": "Shows with most variable ratings\n\nFilter down\n\nby_show = (tv_ratings\n  >> group_by(_, \"title\")\n  >> summarize(_,\n       avg_rating = _.av_rating.mean(),\n       sd = _.av_rating.std(),\n       seasons = n(_)\n     )\n  >> arrange(_, -_.avg_rating)\n)\n\nmost_variable_shows = (by_show\n  >> filter(_, _.seasons >= 5)\n  >> arrange(_, -_.sd)\n  >> head(_, 6)\n)\n\nmost_variable_shows\n\n\n\n\n\n  \n    \n      \n      title\n      avg_rating\n      sd\n      seasons\n    \n  \n  \n    \n      49\n      Are You Afraid of the Dark?\n      8.422971\n      1.390834\n      7\n    \n    \n      263\n      Friday Night Lights\n      8.085020\n      0.749403\n      5\n    \n    \n      650\n      The 100\n      8.314140\n      0.708071\n      5\n    \n    \n      582\n      Scrubs\n      8.236744\n      0.702544\n      9\n    \n    \n      195\n      Dexter\n      8.582400\n      0.694169\n      8\n    \n    \n      562\n      Roseanne\n      7.332537\n      0.670299\n      8\n    \n  \n\n\n\n\n\n\nPlot show ratings\n\n(tv_ratings\n  >> inner_join(_, most_variable_shows, \"title\")\n  >> ggplot(aes(\"seasonNumber\", \"av_rating\", color = \"title\"))\n   + geom_line()\n   + geom_point()\n   + scale_x_continuous(breaks = range(11))\n   + facet_wrap(\"~ title\")\n   + theme(legend_position = \"none\")\n   + labs(\n       x = \"season number\",\n       y = \"average rating\"\n  )\n)\n\n\n\n\n<ggplot: (8742455023836)>"
  },
  {
    "objectID": "examples/index.html",
    "href": "examples/index.html",
    "title": "Siuba",
    "section": "",
    "text": "TV Ratings Analysis\n        \n    \n    \n    \n        \n    \n    \n    \n        Examining season ratings for different TV Series.\nPlotting shows with the most variable ratings.\n\nUses grouped filters, joins, n() for getting the total number of rows.\n\n      \n        \n          View example\n        \n      \n    \n  \n  \n    \n        \n            Space Launches over the Years\n        \n    \n    \n    \n        \n    \n    \n    \n        Space Launches have become dominated by private companies, like Spacex.\nThis analysis looks at the agencies launching rockets over time.\nUses anti_join to figure out why a join is dropping rows,\nfct_lump to group rare agencies together, and a lot of counting.\n\n      \n        \n          View example\n        \n      \n    \n  \n\n\nNo matching items"
  },
  {
    "objectID": "examples/space-launches.html",
    "href": "examples/space-launches.html",
    "title": "Space Launches",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport requests\n\nfrom plotnine import ggplot, aes, geom_col, geom_line, labs\nfrom siuba import *\nfrom siuba.dply.forcats import fct_lump\nfrom siuba.siu import call"
  },
  {
    "objectID": "examples/space-launches.html#launches-per-year",
    "href": "examples/space-launches.html#launches-per-year",
    "title": "Space Launches",
    "section": "Launches per year",
    "text": "Launches per year\n\n(launches\n   >> count(_.launch_year, _.agency_type)\n   >> ggplot(aes(\"launch_year\", \"n\", color = \"agency_type\"))\n    + geom_line()\n    + labs(x = \"Time\", y = \"# of launches this year\", color = \"Agency type\")\n)\n\n\n\n\n<ggplot: (8769715097522)>"
  },
  {
    "objectID": "examples/space-launches.html#which-agencies-have-the-most-launches",
    "href": "examples/space-launches.html#which-agencies-have-the-most-launches",
    "title": "Space Launches",
    "section": "Which agencies have the most launches?",
    "text": "Which agencies have the most launches?\nIn this section, we‚Äôll join launches with agencies, so we can get the agency short names (e.g.¬†SpaceX).\nFirst, though‚Äìwe want to make sure the column we‚Äôre joining on in agencies does not have duplicate values. Otherwise, it will create multiple rows per launch when we join.\n\nagencies >> count(_.agency) >> filter(_.n > 1)\n\n\n\n\n\n  \n    \n      \n      agency\n      n\n    \n  \n  \n  \n\n\n\n\nNow that we know each agencies.agency occurs once, let‚Äôs do our join.\n\nagency_launches = launches >> inner_join(_, agencies, \"agency\")\n\nagency_launches >> count()\n\n\n\n\n\n  \n    \n      \n      n\n    \n  \n  \n    \n      0\n      950\n    \n  \n\n\n\n\n\nlaunches >> count()\n\n\n\n\n\n  \n    \n      \n      n\n    \n  \n  \n    \n      0\n      5726\n    \n  \n\n\n\n\nNotice that the joined data only has 950 rows, while the original data has almost 6,000. What could be the cause?\nTo investigate we can do an anti_join, which keeps only the rows in launches which do not match in the join.\n\nlaunches >> anti_join(_, agencies, \"agency\") >> count(_.agency, _.agency_type, sort=True)\n\n\n\n\n\n  \n    \n      \n      agency\n      agency_type\n      n\n    \n  \n  \n    \n      0\n      US\n      state\n      1202\n    \n    \n      1\n      RU\n      state\n      619\n    \n    \n      2\n      CN\n      state\n      302\n    \n    \n      3\n      J\n      state\n      78\n    \n    \n      4\n      IN\n      state\n      65\n    \n    \n      5\n      I-ESA\n      state\n      13\n    \n    \n      6\n      F\n      state\n      11\n    \n    \n      7\n      IL\n      state\n      10\n    \n    \n      8\n      I\n      state\n      9\n    \n    \n      9\n      IR\n      state\n      8\n    \n    \n      10\n      KP\n      state\n      5\n    \n    \n      11\n      I-ELDO\n      state\n      3\n    \n    \n      12\n      KR\n      state\n      3\n    \n    \n      13\n      BR\n      state\n      2\n    \n    \n      14\n      UK\n      state\n      2\n    \n  \n\n\n\n\nNotice that for rows that were dropped in the original join, the agency_type was always \"state\"!\n\n(agency_launches\n   >> count(_.launch_year, _.short_name)\n   >> ggplot(aes(\"launch_year\", \"n\", fill = \"short_name\"))\n   + geom_col()\n)\n\n\n\n\n<ggplot: (8769696422214)>\n\n\n\n(agency_launches\n   >> mutate(short_name_lumped = fct_lump(_.short_name, n = 6))\n   >> count(_.launch_year, _.short_name_lumped)\n   >> ggplot(aes(\"launch_year\", \"n\", fill = \"short_name_lumped\"))\n   + geom_col()\n)\n\n\n\n\n<ggplot: (8769696962648)>"
  },
  {
    "objectID": "examples/space-launches.html#extra-potential-improvements",
    "href": "examples/space-launches.html#extra-potential-improvements",
    "title": "Space Launches",
    "section": "Extra: potential improvements",
    "text": "Extra: potential improvements\nWhen we joined agencies and launches, columns that had the same names ended up prefixed with _x or _y. We should double check that those columns have identical information, and then drop the duplicate column before joining.\nHere‚Äôs how you can select just the columns that end with _x or _y:\n\nagency_launches >> select(_.endswith(\"_x\"), _.endswith(\"_y\"))\n\n\n\n\n\n  \n    \n      \n      type_x\n      state_code_x\n      agency_type_x\n      state_code_y\n      type_y\n      agency_type_y\n    \n  \n  \n    \n      0\n      Ariane 3\n      F\n      private\n      F\n      O/LA\n      private\n    \n    \n      1\n      Ariane 1\n      F\n      private\n      F\n      O/LA\n      private\n    \n    \n      2\n      Ariane 3\n      F\n      private\n      F\n      O/LA\n      private\n    \n    \n      3\n      Ariane 3\n      F\n      private\n      F\n      O/LA\n      private\n    \n    \n      4\n      Ariane 3\n      F\n      private\n      F\n      O/LA\n      private\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      945\n      Atlas V 411\n      US\n      private\n      US\n      LA\n      private\n    \n    \n      946\n      Atlas V 541\n      US\n      private\n      US\n      LA\n      private\n    \n    \n      947\n      Atlas V 551\n      US\n      private\n      US\n      LA\n      private\n    \n    \n      948\n      Atlas V 401\n      US\n      private\n      US\n      LA\n      private\n    \n    \n      949\n      Atlas V 551\n      US\n      private\n      US\n      LA\n      private\n    \n  \n\n950 rows √ó 6 columns"
  },
  {
    "objectID": "examples/space-launches.html#extra-parsing-big-dates-in-pandas",
    "href": "examples/space-launches.html#extra-parsing-big-dates-in-pandas",
    "title": "Space Launches",
    "section": "Extra: parsing big dates in pandas",
    "text": "Extra: parsing big dates in pandas\nYou might have noticed that this data has a launch_date column, but we only used launch_year. This is because there launch_date has values pandas struggles with: very large dates (e.g.¬†2918-10-11).\nIn this section we‚Äôll show two approaches to parsing dates:\n\nthe .astype(\"Period[D]\") method.\nthe pd.to_datetime(...) function.\n\nAs you‚Äôll see, the first approach can handle large dates, while the best the second can do is turn them into missing values.\nFirst we‚Äôll grab just the columns we care bout.\n\nlaunch_dates = launches >> select(_.startswith(\"launch_\"))\n\nlaunch_dates >> head()\n\n\n\n\n\n  \n    \n      \n      launch_date\n      launch_year\n    \n  \n  \n    \n      0\n      1967-06-29\n      1967\n    \n    \n      1\n      1967-08-23\n      1967\n    \n    \n      2\n      1967-10-11\n      1967\n    \n    \n      3\n      1968-05-23\n      1968\n    \n    \n      4\n      1968-10-23\n      1968\n    \n  \n\n\n\n\nNext we‚Äôll create columns parsing the dates using the two methods.\n\ndates_parsed = (launch_dates\n    >> mutate(\n        launch_date_per = _.launch_date.astype(\"Period[D]\"),\n        launch_date_dt = call(pd.to_datetime, _.launch_date, errors = \"coerce\")\n    )   \n)\n\ndates_parsed >> filter(_.launch_date_dt.isna()) >> head()\n\n\n\n\n\n  \n    \n      \n      launch_date\n      launch_year\n      launch_date_per\n      launch_date_dt\n    \n  \n  \n    \n      547\n      NaN\n      1962\n      NaT\n      NaT\n    \n    \n      550\n      NaN\n      1963\n      NaT\n      NaT\n    \n    \n      551\n      NaN\n      1963\n      NaT\n      NaT\n    \n    \n      1012\n      2918-10-11\n      2018\n      2918-10-11\n      NaT\n    \n    \n      1160\n      NaN\n      2012\n      NaT\n      NaT\n    \n  \n\n\n\n\nNotice that one of the years was 2018, but it was miscoded as 2918 in 2918-10-11.\nNotice that it is..\n\nan NaT in launch_date_dt, since pd.to_datetime() can‚Äôt handle years that large.\nparsed fine in launch_date_per, which uses .astype(\"Period[D]\")."
  },
  {
    "objectID": "about/key_features.html",
    "href": "about/key_features.html",
    "title": "Key features",
    "section": "",
    "text": "siuba\ndplython\npandas\n\n\n\n\nColumn operations are pandas Series methods\n‚úÖ\n‚úÖ\n‚úÖ\n\n\nTable verbs supports user defined functions\n‚úÖ\n‚úÖ\n‚úÖ\n\n\npipe syntax (>>)\n‚úÖ\n‚úÖ\n‚ùå\n\n\nconcise, lazy expressions (_.a + _.b)\n‚úÖ\n‚úÖ\n‚ùå\n\n\nNo more reset_index\n‚úÖ\n‚úÖ\n‚ùå\n\n\nunified API over (un)grouped data\n‚úÖ\n‚úÖ\n‚ùå\n\n\ngenerate fast grouped operations\n‚úÖ\n‚ùå\n‚úÖ\n\n\ngenerate SQL queries\n‚úÖ\n‚ùå\n‚ùå\n\n\nAbstract syntax trees for transforming operations\n‚úÖ\n‚ùå\n‚ùå\n\n\nhandles nested data\n‚úÖ\n‚ùå\n‚ö†Ô∏è"
  },
  {
    "objectID": "about/key_features.html#built-on-pandas",
    "href": "about/key_features.html#built-on-pandas",
    "title": "Key features",
    "section": "Built on pandas",
    "text": "Built on pandas\npandas is everywhere in python data analysis. The siuba library builds on this incredible work by using pandas Series methods as its reference implementation. This means that you use the pandas methods you‚Äôve already learned!\n\nimport pandas as pd\nfrom siuba import _, mutate\n\nmy_data = pd.DataFrame({\n    'g': ['a', 'a', 'b'],\n    'x': [1,2,3],\n})\n\n# pandas\nmy_data.assign(avg = lambda d: d.x.mean())\n\n# siuba\nmutate(my_data, avg = _.x.mean())\n\n\n\n\n\n  \n    \n      \n      g\n      x\n      avg\n    \n  \n  \n    \n      0\n      a\n      1\n      2.0\n    \n    \n      1\n      a\n      2\n      2.0\n    \n    \n      2\n      b\n      3\n      2.0\n    \n  \n\n\n\n\nNote how you can debug both pieces of code by running and inspecting df.a.mean().\nWhile pandas is an incredibly powerful API, its syntax can get quite cumbersome.\n\n(my_data\n  .assign(avg = lambda d: d.x.mean())    # create new column\n  .loc[lambda d: d.x != 3]               # filter out some rows\n)\n\n\n\n\n\n  \n    \n      \n      g\n      x\n      avg\n    \n  \n  \n    \n      0\n      a\n      1\n      2.0\n    \n    \n      1\n      a\n      2\n      2.0\n    \n  \n\n\n\n\nNotice how much of this code is writing the word lambda.\nLike other ports of the popular R library, dplyr‚Äìsuch as dplython‚Äìsiuba offers a simple, flexible way to work on tables of data.\n\nPipe syntax\nThe pipe syntax allows you to import table functions (verbs), rather than having 300+ methods on your DataFrame.\n\n# actions can be imported individually\nfrom siuba import mutate, arrange\n\n# they can be combined using a pipe\nmy_data >> mutate(y = _.x + 1) >> arrange(_.g, -_.x)\n\n\n\n\n\n  \n    \n      \n      g\n      x\n      y\n    \n  \n  \n    \n      1\n      a\n      2\n      3\n    \n    \n      0\n      a\n      1\n      2\n    \n    \n      2\n      b\n      3\n      4\n    \n  \n\n\n\n\n\n\nLazy expressions\nUsing lazy expressions saves you from repeating the name of your DataFrame over and over.\n\n# rather than repeat the name of your data, you can use lazy expressions ---\nmy_data_frame = pd.DataFrame({'a': [1,2,3]})\n\n\n# bad\nmy_data_frame[\"b\"] = my_data_frame[\"a\"] + 1\nmy_data_frame[\"c\"] = my_data_frame[\"b\"] + 2\n\n# good\nmy_data_frame >> mutate(b = _.a + 1, c = _.b + 2)\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      1\n      2\n      4\n    \n    \n      1\n      2\n      3\n      5\n    \n    \n      2\n      3\n      4\n      6\n    \n  \n\n\n\n\n\n\nNo reset_index\nNotice how siuba mutate can take a DataFrame, and return a DataFrame. Moreover, it doesn‚Äôt stick columns onto the index. This means you don‚Äôt need to call reset_index all the time.\nA common place where reset_index is called is after a pandas grouped aggregation.\n\nfrom siuba.data import mtcars\nfrom siuba import summarize\n\ng_cyl = mtcars.groupby(\"cyl\")\n\nagg_res = g_cyl[[\"hp\", \"mpg\"]].agg(\"mean\")\nagg_res\n\n\n\n\n\n  \n    \n      \n      hp\n      mpg\n    \n    \n      cyl\n      \n      \n    \n  \n  \n    \n      4\n      82.636364\n      26.663636\n    \n    \n      6\n      122.285714\n      19.742857\n    \n    \n      8\n      209.214286\n      15.100000\n    \n  \n\n\n\n\n\n# bad\nagg_res.reset_index()\n\n\n\n\n\n  \n    \n      \n      cyl\n      hp\n      mpg\n    \n  \n  \n    \n      0\n      4\n      82.636364\n      26.663636\n    \n    \n      1\n      6\n      122.285714\n      19.742857\n    \n    \n      2\n      8\n      209.214286\n      15.100000\n    \n  \n\n\n\n\n\n# good\nsummarize(g_cyl, hp = _.hp.mean(), mpg = _.mpg.mean())\n\n\n\n\n\n  \n    \n      \n      cyl\n      hp\n      mpg\n    \n  \n  \n    \n      0\n      4\n      82.636364\n      26.663636\n    \n    \n      1\n      6\n      122.285714\n      19.742857\n    \n    \n      2\n      8\n      209.214286\n      15.100000\n    \n  \n\n\n\n\n\n\nUnified (un)grouped API\nIn siuba it doesn‚Äôt matter whether your data is grouped or not.\n\ng_cyl = mtcars.groupby(\"cyl\")\n\nmtcars >> mutate(demeaned = _.hp - _.hp.mean())    # uses ungrouped mean\ng_cyl  >> mutate(demeaned = _.hp - _.hp.mean())    # uses grouped mean\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n      demeaned\n    \n  \n  \n    \n      0\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n      -12.285714\n    \n    \n      1\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n      -12.285714\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      15.0\n      8\n      301.0\n      335\n      3.54\n      3.570\n      14.60\n      0\n      1\n      5\n      8\n      125.785714\n    \n    \n      31\n      21.4\n      4\n      121.0\n      109\n      4.11\n      2.780\n      18.60\n      1\n      1\n      4\n      2\n      26.363636\n    \n  \n\n32 rows √ó 12 columns\n\n\n\nIn pandas you have to change your code for grouped data.\n\ng_cyl = mtcars.groupby(\"cyl\")\n\n# ungrouped vs grouped mean\nmtcars.assign(demeaned = lambda d: d.hp - d.hp.mean())\nmtcars.assign(demeaned = g_cyl.obj.hp - g_cyl.hp.transform(\"mean\"))\n\n\n\n\n\n  \n    \n      \n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n      demeaned\n    \n  \n  \n    \n      0\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n      -12.285714\n    \n    \n      1\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n      -12.285714\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      15.0\n      8\n      301.0\n      335\n      3.54\n      3.570\n      14.60\n      0\n      1\n      5\n      8\n      125.785714\n    \n    \n      31\n      21.4\n      4\n      121.0\n      109\n      4.11\n      2.780\n      18.60\n      1\n      1\n      4\n      2\n      26.363636\n    \n  \n\n32 rows √ó 12 columns\n\n\n\nNote that g_cyl does not have an assign method, and requires passing what operation you want to do (\"mean\") as a string to .transform()."
  },
  {
    "objectID": "about/key_features.html#fast-and-flexible",
    "href": "about/key_features.html#fast-and-flexible",
    "title": "Key features",
    "section": "Fast and flexible",
    "text": "Fast and flexible\n\nFast grouped operations\nConsider some data (students) where 2,000 students have each completed 10 courses, and received a score on each course.\n# fast grouped operations (pull from dev docs)\n# PLOT of timing\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(123)\nstudents = pd.DataFrame({\n    'student_id': np.repeat(np.arange(2000), 10),\n    'course_id': np.random.randint(1, 20, 20000),\n    'score': np.random.randint(1, 100, 20000)\n})\n\ng_students = students.groupby('student_id')\ng_students\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      student_id\n      course_id\n      score\n    \n  \n  \n    \n      0\n      0\n      14\n      38\n    \n    \n      1\n      0\n      3\n      40\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      19998\n      1999\n      11\n      32\n    \n    \n      19999\n      1999\n      17\n      10\n    \n  \n\n20000 rows √ó 3 columns\n\n\n\nSuppose that we want to get the courses each student scored lowest on.\nIn pandas we could use some complex, but fast code.\n\n%%time\n# pandas\nis_student_min = g_students.obj.score == g_students.score.transform('min')\nlow_scores = students[is_student_min]\n\nCPU times: user 1.14 ms, sys: 300 ¬µs, total: 1.44 ms\nWall time: 1.37 ms\n\n\nIn siuba it is simpler, and comparable in speed.\nfrom siuba.experimental.pd_groups import fast_filter\n\n%%time\n# siuba\nlow_scores = fast_filter(g_students, _.score == _.score.min())\n\nCPU times: user 1.76 ms, sys: 199 ¬µs, total: 1.96 ms\nWall time: 1.81 ms\n\n\nThis is because siuba‚Äôs lazy expressions let it optimize grouped operations.\nHowever, dplython is over 100x slower in this case, because it uses the slower pandas DataFrame.apply() method under the hood.\n# set up code for timing\nfrom dplython import X, DplyFrame, sift, group_by as dply_group_by\n\ng_students2 = DplyFrame(students) >> dply_group_by(X.student_id)\n\n%%time\ng_students2 >> sift(X.score == X.score.min())\n\nCPU times: user 149 ms, sys: 1.23 ms, total: 150 ms\nWall time: 150 ms\n\n\n\n\n\n\n  \n    \n      \n      student_id\n      course_id\n      score\n    \n  \n  \n    \n      2\n      0\n      3\n      17\n    \n    \n      10\n      1\n      10\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      19987\n      1998\n      17\n      31\n    \n    \n      19997\n      1999\n      3\n      1\n    \n  \n\n2117 rows √ó 3 columns\n\n\n\n\n\nSQL queries\n\n# generate SQL queries\nfrom siuba.data import cars_sql\nfrom siuba import group_by, mutate, show_query\n\nq = (cars_sql\n  >> group_by(\"cyl\")\n  >> mutate(demeaned = _.hp - _.hp.mean())\n  >> show_query()\n)\n\nSELECT cars.cyl, cars.mpg, cars.hp, cars.hp - avg(cars.hp) OVER (PARTITION BY cars.cyl) AS demeaned \nFROM cars\n\n\n\n\nAbstract syntax trees\nThis is made possible because siuba represents lazy expressions with abstract syntax trees. Fast grouped operations and SQL queries are just the beginning. This allows people to produce a whole range of interesting tools!\nSiuba‚Äôs lazy expressions consist of a Symbolic and Call class.\nSymbolic is used to quickly create lazy expressions.\n\n# ASTs for transforming\nfrom siuba.siu import Symbolic, Call, strip_symbolic\n\n_ = Symbolic()\n\nsym = _.a.mean() + _[\"b\"]\nsym\n\n‚ñà‚îÄ+\n‚îú‚îÄ‚ñà‚îÄ'__call__'\n‚îÇ ‚îî‚îÄ‚ñà‚îÄ.\n‚îÇ   ‚îú‚îÄ‚ñà‚îÄ.\n‚îÇ   ‚îÇ ‚îú‚îÄ_\n‚îÇ   ‚îÇ ‚îî‚îÄ'a'\n‚îÇ   ‚îî‚îÄ'mean'\n‚îî‚îÄ‚ñà‚îÄ[\n  ‚îú‚îÄ_\n  ‚îî‚îÄ‚ñà‚îÄ'__siu_slice__'\n    ‚îî‚îÄ'b'\n\n\nEach black box in the printout above is a Call. Calls are the pieces that represent the underlying operations. They have methods to inspect and transform them.\n\ncall = strip_symbolic(sym)\n\n# get columns names used in lazy expression\ncall.op_vars(attr_calls = False)\n\n{'a'}\n\n\n\n\nNested data\n\nfrom siuba import _, mutate, unnest\n\ntagged = pd.DataFrame({\n    'id': [1,2,3],\n    'tags': ['a,b,c', 'd,e', 'f']\n})\n\n(tagged\n    >> mutate(split_tags = _.tags.str.split(','))\n    >> unnest(\"split_tags\")\n)\n\n\n\n\n\n  \n    \n      \n      id\n      tags\n      split_tags\n    \n  \n  \n    \n      0\n      1\n      a,b,c\n      a\n    \n    \n      1\n      1\n      a,b,c\n      b\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      4\n      2\n      d,e\n      e\n    \n    \n      5\n      3\n      f\n      f\n    \n  \n\n6 rows √ó 3 columns"
  },
  {
    "objectID": "about/key_features.html#learning-more",
    "href": "about/key_features.html#learning-more",
    "title": "Key features",
    "section": "Learning more",
    "text": "Learning more"
  },
  {
    "objectID": "develop/backend_pandas.html",
    "href": "develop/backend_pandas.html",
    "title": "Pandas backend",
    "section": "",
    "text": "import pandas as pd\npd.set_option(\"display.max_rows\", 5)\nThe pandas backend has two key responsibilities:\nWhile it might seem like a lot of work, this mostly involves using a few simple strategies to take advantage of logic already existing in the pandas library.\nThe strategy can be described as follows:\nAs a final note, while the SQL backend uses a custom backend class (LazyTbl), the backend for table verbs in this case is just the pandas‚Äô DataFrameGroupBy class itself."
  },
  {
    "objectID": "develop/backend_pandas.html#column-op-translation",
    "href": "develop/backend_pandas.html#column-op-translation",
    "title": "Pandas backend",
    "section": "Column op translation",
    "text": "Column op translation\n\n\n\n\n\n\nfrom siuba.ops import mean\nfrom siuba.data import mtcars\n\n# equivalent to mtcars.hp.mean()\nmean(mtcars.hp)\n\n146.6875\n\n\n\nimport pandas as pd\nfrom siuba.ops.utils import operation\n\nmean2 = operation(\"mean\", \"Agg\", 1)\n\n# Series implementation just calls the Series' mean method\nmean2.register(\n    pd.Series,\n    lambda self, *args, **kwargs: self.mean(*args, **kwargs)\n)\n\n<function __main__.<lambda>(self, *args, **kwargs)>\n\n\n\nmean2(mtcars.hp)\n\n146.6875\n\n\n\nmean2.operation.name\n\n'mean'\n\n\nThe purpose of the .operation data is to make it easy to generate new translations for functions. For example, if we want to translate pandas‚Äô ser.str.upper() method, then it helps to know it uses the str accessor.\n\nUsing an existing translation\n\nfrom siuba.experimental.pd_groups.translate import method_el_op\n\ndf = pd.DataFrame({\n    \"x\": [\"a\", \"b\", \"c\"],\n    \"g\": [0, 0, 1]\n})\n\n# notice method_ag_op uses some details from .operation\nupper = method_el_op(\"upper\", is_property = False, accessor = \"str\")\nlower = method_el_op(\"lower\", is_property = False, accessor = \"str\")\n\ng_df = df.groupby(\"g\")\n\nres = upper(g_df.x)\n\n# note: .obj is how you \"ungroup\" a SeriesGroupBy\nres.obj\n\n0    A\n1    B\n2    C\nName: x, dtype: object\n\n\n\n# convert to uppercase and back to lowercase\n# equivalent to df.x.str.upper().str.lower()\nres2 = lower(upper(g_df.x))\nres2.obj\n\n0    a\n1    b\n2    c\nName: x, dtype: object\n\n\n\nisinstance(res, pd.core.groupby.SeriesGroupBy)\n\nTrue\n\n\n\nlower(upper(g_df.x))\n\n<pandas.core.groupby.generic.SeriesGroupBy object at 0x7f2848a532b0>\n\n\nSee the internals of functions like method_el_op for details."
  },
  {
    "objectID": "develop/backend_pandas.html#new-verb-implementations",
    "href": "develop/backend_pandas.html#new-verb-implementations",
    "title": "Pandas backend",
    "section": "New verb implementations",
    "text": "New verb implementations\nLike with other backends, verbs use single dispatch to register new backend implementations.\n\nfrom pandas import DataFrame\nfrom pandas.core.groupby import DataFrameGroupBy\nfrom siuba.dply.verbs import singledispatch2\n\n@singledispatch2(DataFrame)\ndef my_verb(__data):\n    print(\"Running default.\")\n    \n# register grouped implementation ----\n@my_verb.register(DataFrameGroupBy)\ndef _my_verb_gdf(__data):\n    print(\"Running grouped!\")\n\n\n# test it out ----\nfrom siuba.data import mtcars\n\nmy_verb(mtcars.groupby(\"cyl\"))\n\nRunning grouped!"
  },
  {
    "objectID": "develop/backend_sql.html",
    "href": "develop/backend_sql.html",
    "title": "SQL backend",
    "section": "",
    "text": "import pandas as pd\npd.set_option(\"display.max_rows\", 5)\n\nfrom siuba.siu.format import Formatter\n\nshow_tree = lambda x: print(Formatter().format(x))"
  },
  {
    "objectID": "develop/backend_sql.html#step-1-column-translation",
    "href": "develop/backend_sql.html#step-1-column-translation",
    "title": "SQL backend",
    "section": "Step 1: Column Translation",
    "text": "Step 1: Column Translation\nColumn translation requires three pieces:\n\nLocals: Functions for creating the sqlalchemy clause corresponding to an operation.\nColumn Data: Classes representing columns under normal and aggregate settings.\n\nTranslator: A class that can take a symbolic expression (e.g.¬†_.x.mean()) and return it in call form: mean(_.x).\nCodata visitor: A class that takes the above call, and swaps in the sql dialect version of each call.\n\n\n# Column data =================================================================\n\nfrom siuba.sql.translate import SqlColumn, SqlColumnAgg\n\n# used if you want to add a scalar or window translation\n# (eg. something that gets used in mutate)\nclass WowSqlColumn(SqlColumn): pass\n\n# used if you want to add a aggregate translation\n# (eg. something that gets used in a query with a GROUP BY clause)\nclass WowSqlColumnAgg(WowSqlColumn, SqlColumn): pass\n\n\n# Locals ======================================================================\n\nfrom siuba.sql.translate import (\n        win_over,\n        win_agg,\n        sql_agg,\n        sql_scalar,\n        sql_colmeth,\n        sql_not_impl,\n        )\n\nscalar = {\n    \"__add__\": sql_colmeth(\"__add__\"),\n    \"round\": sql_scalar(\"round\"),\n}\n\nwindow = {\n    \"rank\": win_over(\"rank\"),\n    \"mean\": win_agg(\"mean\"),\n}\n\naggregation = {\n    \"rank\": sql_not_impl(),\n    \"mean\": sql_agg(\"mean\"),\n}\n\n\n# Translator ==================================================================\n\nfrom siuba.sql.translate import SqlTranslator\n\ntranslator = SqlTranslator.from_mappings(\n    WowSqlColumn, WowSqlColumnAgg\n)\n\n# TODO: how to work in codata visitor?"
  },
  {
    "objectID": "develop/backend_sql.html#column-data",
    "href": "develop/backend_sql.html#column-data",
    "title": "SQL backend",
    "section": "Column Data",
    "text": "Column Data\nThere are two kinds of data classes, corresponding to whether the generated outermost query in the generated SQL will use a GROUP BY clause."
  },
  {
    "objectID": "develop/backend_sql.html#locals",
    "href": "develop/backend_sql.html#locals",
    "title": "SQL backend",
    "section": "Locals",
    "text": "Locals\nThe entries of each local dictionary are functions that take a sqlalchemy.sql.ClauseElement‚Äìwhich is the parent class of many sqlalchemy elements‚Äìand returns a ClauseElement.\n\nfrom sqlalchemy import sql\n\nexpr_rank = window[\"rank\"](WowSqlColumn(), sql.column(\"a_col\"))\nexpr_rank\n\n<siuba.sql.translate.RankOver object at 0x7f8c01399b20>\n\n\n\nprint(expr_rank)\n\nrank() OVER (ORDER BY a_col)"
  },
  {
    "objectID": "develop/backend_sql.html#translator",
    "href": "develop/backend_sql.html#translator",
    "title": "SQL backend",
    "section": "Translator",
    "text": "Translator\nBelow, we set up a sqlalchemy select statement in order to demonstrate the translator in action.\n\nfrom siuba import _\n\n\nfrom sqlalchemy.sql import column, select\n\nsel = select([column('x'), column('y')])\n\nThen we feed the columns to the translated call.\n\ncall_add = translator.translate(_.x + _.y)\n\nshow_tree(call_add)\n\n‚ñà‚îÄ'__call__'\n‚îú‚îÄ‚ñà‚îÄ'__custom_func__'\n‚îÇ ‚îî‚îÄ<function singledispatch.<locals>.wrapper at 0x7f8bf984d9d0>\n‚îú‚îÄ‚ñà‚îÄ.\n‚îÇ ‚îú‚îÄ_\n‚îÇ ‚îî‚îÄ'x'\n‚îî‚îÄ‚ñà‚îÄ.\n  ‚îú‚îÄ_\n  ‚îî‚îÄ'y'\n\n\nNote that behind the scenes, the translator goes down the call tree and swaps functions like \"__add__\" with the local translations.\n\nfrom siuba.siu.visitors import CodataVisitor\ncodata = CodataVisitor(WowSqlColumn, object)\n\ncall_add_final = codata.visit(call_add)\n\nshow_tree(call_add_final)\n\n‚ñà‚îÄ'__call__'\n‚îú‚îÄ‚ñà‚îÄ'__custom_func__'\n‚îÇ ‚îî‚îÄ<function sql_colmeth.<locals>.f at 0x7f8bf96c6280>\n‚îú‚îÄ<__main__.WowSqlColumn object at 0x7f8c013997f0>\n‚îú‚îÄ‚ñà‚îÄ.\n‚îÇ ‚îú‚îÄ_\n‚îÇ ‚îî‚îÄ'x'\n‚îî‚îÄ‚ñà‚îÄ.\n  ‚îú‚îÄ_\n  ‚îî‚îÄ'y'\n\n\n\n# the root node is __add__. shown as +.\n_.x + _.y\n\n‚ñà‚îÄ+\n‚îú‚îÄ‚ñà‚îÄ.\n‚îÇ ‚îú‚îÄ_\n‚îÇ ‚îî‚îÄ'x'\n‚îî‚îÄ‚ñà‚îÄ.\n  ‚îú‚îÄ_\n  ‚îî‚îÄ'y'\n\n\n\n# We can see this in action by calling the translation directly.\nscalar[\"__add__\"](WowSqlColumn(), sel.columns.x, sel.columns.y)\n\n/tmp/ipykernel_3333/1405687756.py:2: SADeprecationWarning: The SelectBase.c and SelectBase.columns attributes are deprecated and will be removed in a future release; these attributes implicitly create a subquery that should be explicit.  Please call SelectBase.subquery() first in order to create a subquery, which then contains this attribute.  To access the columns that this SELECT object SELECTs from, use the SelectBase.selected_columns attribute. (deprecated since: 1.4)\n  scalar[\"__add__\"](WowSqlColumn(), sel.columns.x, sel.columns.y)\n\n\n<sqlalchemy.sql.elements.BinaryExpression object at 0x7f8c01399910>\n\n\nBy default the translate method assumes the expression is using window functions, so operations like .mean() return SqlAlchemy Over clauses.\n\nf_translate = translator.translate(_.x.mean())\n\nf_translate_co = codata.visit(f_translate)\nexpr = f_translate_co(sel.columns)\n\nexpr\n\n/tmp/ipykernel_3333/2826175941.py:4: SADeprecationWarning: The SelectBase.c and SelectBase.columns attributes are deprecated and will be removed in a future release; these attributes implicitly create a subquery that should be explicit.  Please call SelectBase.subquery() first in order to create a subquery, which then contains this attribute.  To access the columns that this SELECT object SELECTs from, use the SelectBase.selected_columns attribute. (deprecated since: 1.4)\n  expr = f_translate_co(sel.columns)\n\n\n<siuba.sql.translate.AggOver object at 0x7f8bf9643040>\n\n\n\nprint(expr)\n\navg(anon_1.x) OVER ()\n\n\nKeep in mind‚Äìsince the translator doesn‚Äôt know about grouping variables‚Äìit returns an empty over clause. This separation of concerns:\n\ntranslator: handles simple column ops, including returning over clauses to be filled.\nbackend: handles broader contexts‚Äìlike if the data has been grouped, arranged, or limited‚Äìby visiting the result of each translation."
  },
  {
    "objectID": "develop/backend_sql.html#user-defined-functions",
    "href": "develop/backend_sql.html#user-defined-functions",
    "title": "SQL backend",
    "section": "User defined functions",
    "text": "User defined functions\nRemember those column data classes we made and passed to the translator? They allow users to register custom column operation functions!\nBelow we show a custom round function, which calls the sqlalchemy that usually corresponds to ROUND(<col>).\n\nfrom siuba.siu import _, symbolic_dispatch\nfrom sqlalchemy import sql\n\n@symbolic_dispatch(cls = WowSqlColumn)\ndef round(self, col):\n    print(\"running round function\")\n    \n    return sql.function.round(col)\n\n# Creates a special symbolic object\nround(_)\n\n‚ñà‚îÄ'__call__'\n‚îú‚îÄ‚ñà‚îÄ'__custom_func__'\n‚îÇ ‚îî‚îÄ<function round at 0x7f8bf9644670>\n‚îî‚îÄ_\n\n\nNote that a special feature of symbolic_dispatch, is that it let‚Äôs you form complex expressions by passing _ to your function call.\n\n# Symbolic objects let you express complex operations\nround(_) + 9999\n\n‚ñà‚îÄ+\n‚îú‚îÄ‚ñà‚îÄ'__call__'\n‚îÇ ‚îú‚îÄ‚ñà‚îÄ'__custom_func__'\n‚îÇ ‚îÇ ‚îî‚îÄ<function round at 0x7f8bf9644670>\n‚îÇ ‚îî‚îÄ_\n‚îî‚îÄ9999"
  },
  {
    "objectID": "develop/backend_sql.html#section",
    "href": "develop/backend_sql.html#section",
    "title": "SQL backend",
    "section": ".",
    "text": "."
  },
  {
    "objectID": "develop/backend_sql.html#step-2-backend-class",
    "href": "develop/backend_sql.html#step-2-backend-class",
    "title": "SQL backend",
    "section": "Step 2: Backend class",
    "text": "Step 2: Backend class\nSo far we‚Äôve only discussed how to translate symbolic expressions like _.x + _.y in the mutate call below.\nmutate(backend, _.x + _.y)\nThe last piece is implementing a backend that works with the mutate function itself. If you are just adding support for a new SQL dialect, you can use the LazyTbl class provided by siuba.sql. While the translations above work on columns of data, this class handles tables and queries over data (e.g.¬†select statements).\nMore specifically, LazyTbl has 2 jobs:\n\nRepresenting a specific table of data from the database.\nCreating SQL queries via table verbs. This includes using translators above, as well as broader SQL constructs (e.g.¬†ordering, grouping / partitions, limiting rows, etc..).\n\n\n# Setup example data ----\nfrom sqlalchemy import create_engine\nfrom siuba.data import mtcars\n\n# copy pandas DataFrame to sqlite\nengine = create_engine(\"sqlite:///:memory:\")\nmtcars.to_sql(\"mtcars\", engine, if_exists = \"replace\")\n\n32"
  },
  {
    "objectID": "develop/backend_sql.html#representing-sql-tables",
    "href": "develop/backend_sql.html#representing-sql-tables",
    "title": "SQL backend",
    "section": "Representing SQL tables",
    "text": "Representing SQL tables\n\nfrom siuba.sql import LazyTbl\n\ntbl_cars = LazyTbl(engine, \"mtcars\")\ntbl_cars\n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n  \n    \n      \n      index\n      mpg\n      cyl\n      disp\n      hp\n      drat\n      wt\n      qsec\n      vs\n      am\n      gear\n      carb\n    \n  \n  \n    \n      0\n      0\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.620\n      16.46\n      0\n      1\n      4\n      4\n    \n    \n      1\n      1\n      21.0\n      6\n      160.0\n      110\n      3.90\n      2.875\n      17.02\n      0\n      1\n      4\n      4\n    \n    \n      2\n      2\n      22.8\n      4\n      108.0\n      93\n      3.85\n      2.320\n      18.61\n      1\n      1\n      4\n      1\n    \n    \n      3\n      3\n      21.4\n      6\n      258.0\n      110\n      3.08\n      3.215\n      19.44\n      1\n      0\n      3\n      1\n    \n    \n      4\n      4\n      18.7\n      8\n      360.0\n      175\n      3.15\n      3.440\n      17.02\n      0\n      0\n      3\n      2\n    \n  \n\n# .. may have more rows\n\n\nNote that you can access a number of useful attributes.\n\n# calls the underlying translator and codata\nf_add = tbl_cars.shape_call(_.mpg + _.hp)\nf_add(tbl_cars.last_op.columns)\n\n<sqlalchemy.sql.elements.BinaryExpression object at 0x7f8bf953b190>\n\n\n\n# the underlying sqlalchemy source table\ntbl_cars.tbl\n\nTable('mtcars', MetaData(bind=Engine(sqlite:///:memory:)), Column('index', BIGINT(), table=<mtcars>), Column('mpg', FLOAT(), table=<mtcars>), Column('cyl', BIGINT(), table=<mtcars>), Column('disp', FLOAT(), table=<mtcars>), Column('hp', BIGINT(), table=<mtcars>), Column('drat', FLOAT(), table=<mtcars>), Column('wt', FLOAT(), table=<mtcars>), Column('qsec', FLOAT(), table=<mtcars>), Column('vs', BIGINT(), table=<mtcars>), Column('am', BIGINT(), table=<mtcars>), Column('gear', BIGINT(), table=<mtcars>), Column('carb', BIGINT(), table=<mtcars>), schema=None)\n\n\n\n# grouping info\nfrom siuba import group_by, _\n\ntbl2 = group_by(tbl_cars, rounded_mpg = _.mpg.round(10))\ntbl2.group_by\n\n('rounded_mpg',)"
  },
  {
    "objectID": "develop/backend_sql.html#table-verbs",
    "href": "develop/backend_sql.html#table-verbs",
    "title": "SQL backend",
    "section": "Table verbs",
    "text": "Table verbs\nBackends register on a dispatcher called singledispatch2. This is shown below with an assign verb that is a limited implementation of mutate.\n\nfrom siuba.dply.verbs import singledispatch2\nfrom sqlalchemy.sql import select\n\n@singledispatch2(LazyTbl)\ndef assign(__data, **kwargs):\n\n    new_cols = []\n    for k, expr in kwargs.items():\n\n        # .shape_call mostly data.translator under the hood\n        new_call = __data.shape_call(expr)\n        sql_expr = new_call(__data.last_op.columns)\n        \n        new_cols.append(sql_expr.label(k))\n    \n    # copy of data, where .last_op is the new select statement\n    new_data = __data.append_op(select(new_cols))\n    \n    return new_data\n\n\n# test out\ntbl_assigned = assign(tbl_cars, res = _.mpg + _.hp)\n\ntbl_assigned \n\n\n# Source: lazy query\n# DB Conn: Engine(sqlite:///:memory:)\n# Preview:\n\n\n\n  \n    \n      \n      res\n    \n  \n  \n    \n      0\n      131.0\n    \n    \n      1\n      131.0\n    \n    \n      2\n      115.8\n    \n    \n      3\n      131.4\n    \n    \n      4\n      193.7\n    \n  \n\n# .. may have more rows\n\n\nNote that the backend.last_op property holds the current select statement, so the verb can adapt it, or wrap it in another select statement.\n\nprint(tbl_assigned.last_op)\n\nSELECT mtcars.mpg + mtcars.hp AS res \nFROM mtcars"
  },
  {
    "objectID": "develop/backend_sql.html#unit-tests",
    "href": "develop/backend_sql.html#unit-tests",
    "title": "SQL backend",
    "section": "Unit tests",
    "text": "Unit tests\nSiuba generates a table of all supported operations per backend, and tests against a simple example for each, to ensure they produce the same result as the pandas backend.\nHowever, some translations may deviate in the following ways:\n\nreturning a float rather than an int (or vice-versa).\nan aggregation that works in summarize, but not in mutate verbs.\n\nIn order to mark translations as deviating, you can use the annotation functions.\n\nfrom sqlalchemy import sql\nfrom siuba.sql.translate import annotate, wrap_annotate\n\n# puts an `operation` attribute on function, describing limitations\nf_and = annotate(lambda col: col & False, input_type = \"bool\")\nf_and(sql.column('a'))\n\n<sqlalchemy.sql.elements.AsBoolean object at 0x7f8c013995e0>\n\n\n\nf_and.operation\n\n{'input_type': 'bool'}\n\n\n\n# creates a new function (wrapping the old one), and annotates that\nf_and2 = wrap_annotate(f_and, something_else = False)\n\nf_and2.operation\n\n{'something_else': False}"
  },
  {
    "objectID": "develop/call_trees.html",
    "href": "develop/call_trees.html",
    "title": "Siuba",
    "section": "",
    "text": "Call trees are what siuba uses to take what users say they want to do, and convert it into an action, such as‚Ä¶\n\na SQL statement\na set of pandas operations\n\nBelow is an example expression, alongside with a graphical representation of that expression. This graphical representation is the call tree.\n\nfrom siuba import _ \n\n_.hp + _.hp.rank()\n\n‚ñà‚îÄ+\n‚îú‚îÄ‚ñà‚îÄ.\n‚îÇ ‚îú‚îÄ_\n‚îÇ ‚îî‚îÄ'hp'\n‚îî‚îÄ‚ñà‚îÄ'__call__'\n  ‚îî‚îÄ‚ñà‚îÄ.\n    ‚îú‚îÄ‚ñà‚îÄ.\n    ‚îÇ ‚îú‚îÄ_\n    ‚îÇ ‚îî‚îÄ'hp'\n    ‚îî‚îÄ'rank'\n\n\nOne thing that often catches people by surprise with call trees, is that calls for an expression like\n_.hp.rank()\nare not in order from left to right, but the other way around. Looking at its tree..\n\n_.hp.rank()\n\n‚ñà‚îÄ'__call__'\n‚îî‚îÄ‚ñà‚îÄ.\n  ‚îú‚îÄ‚ñà‚îÄ.\n  ‚îÇ ‚îú‚îÄ_\n  ‚îÇ ‚îî‚îÄ'hp'\n  ‚îî‚îÄ'rank'\n\n\nIt goes\n\ncall _.hp.rank\nget attribute rank from _.hp\nget attribute hp from _\n\nI‚Äôll call this order, the entering order. It occurs when we walk down the tree depth first.\nSometimes this order is useful, but often we‚Äôll want to think of the operations in reverse (e.g.¬†closer to how we read them). In order to allow for both situations, in siuba I often use what I‚Äôll refer to as a tree listener. This is a concept borrowed from the Antlr4 parser generator language.\n\n\n\nFor each node (black box) on a tree, a tree listener allows to to define some custom processing, by specifying enter and exit methods.\n\n_.hp + _.hp.rank()\n\n‚ñà‚îÄ+\n‚îú‚îÄ‚ñà‚îÄ.\n‚îÇ ‚îú‚îÄ_\n‚îÇ ‚îî‚îÄ'hp'\n‚îî‚îÄ‚ñà‚îÄ'__call__'\n  ‚îî‚îÄ‚ñà‚îÄ.\n    ‚îú‚îÄ‚ñà‚îÄ.\n    ‚îÇ ‚îú‚îÄ_\n    ‚îÇ ‚îî‚îÄ'hp'\n    ‚îî‚îÄ'rank'\n\n\nNote that nodes like + and . in the graph above are shorthand for their python method names, __add__ and __getattr__ respectively.\n\n\nBelow is an example tree listener that strips out a __getattr__ operation from a call.\n\nfrom siuba.siu import Call, BinaryOp, strip_symbolic\nfrom siuba.siu.visitors import CallListener\nfrom siuba import _ \n\nclass AttrStripper(CallListener):\n    def __init__(self, rm_attr):\n        self.rm_attr = rm_attr\n        \n    def exit___getattr__(self, node):\n        obj, attr_name = node.args\n        if attr_name in self.rm_attr:\n            return obj\n        \n        return node\n\n\nattr_strip = AttrStripper({'hp'})\n\ncall = strip_symbolic(_.hp + _.hp.rank())\n\nprint(call)\nprint(attr_strip.enter(call))            \n\n_.hp + _.hp.rank()\n_ + _.rank()\n\n\n\n\n\n\nclass AttrStopper(AttrStripper):\n    def enter___getattr__(self, node):\n        obj, attr_name = node.args\n        if attr_name == \"stop\":\n            # don't enter child nodes\n            return self.exit(node)\n        \n        # use generic entering method on this node (and its children)\n        return self.generic_enter(node)\n\n\nattr_stopper = AttrStopper({'hp'})\n\ncall = strip_symbolic(_.hp + _.stop.hp + _.hp.stop)\n\nprint(call)\nprint(attr_stopper.enter(call))\n\n_.hp + _.stop.hp + _.hp.stop\n_ + _.stop + _.hp.stop\n\n\n\n\n\nIn general, it‚Äôs better to use enter when you need to use info that python would execute earlier in time.\nSome useful cases include\n\nstopping further processing (by not entering child nodes)\nmodifying a child node, prior to entering (i.e.¬†starting processing)\n\nFor example, suppose we want to treat method calls in a special way. In _.rank(), we first enter the __call__ node. Moreover, we can lookup whether it is actually a method call from this node, using the rule‚Ä¶\n\nif a call node is operating on a get attribute, then it is a method call\n\nThis is shown below‚Ä¶\n\n_.dt.year\n\n‚ñà‚îÄ.\n‚îú‚îÄ‚ñà‚îÄ.\n‚îÇ ‚îú‚îÄ_\n‚îÇ ‚îî‚îÄ'dt'\n‚îî‚îÄ'year'\n\n\n\n_.dt.year()\n\n‚ñà‚îÄ'__call__'\n‚îî‚îÄ‚ñà‚îÄ.\n  ‚îú‚îÄ‚ñà‚îÄ.\n  ‚îÇ ‚îú‚îÄ_\n  ‚îÇ ‚îî‚îÄ'dt'\n  ‚îî‚îÄ'year'\n\n\n\n# want to remove dt\n# also want to treat an attribute after dt as a call\n# _.a.dt.year\n# \n# if we cut dt out in the exit, can't know year is attribute\n\n# TODO: shouldn't need to import BinaryOp, but it helps with formatting\n# maybe need factory function?\ndef is_op(node, opname):\n    if isinstance(node, Call) and node.func == opname:\n        return True\n    \n    return False\n\nclass MethodMaker(CallListener):\n    def enter___getattr__(self, node):\n        obj, attr_name = node.args\n        \n        print(\"Entering attribute: \", attr_name)\n        \n        # is to the right of another attribute call\n        # e.g. _.<left_attr>.<attr_name>\n        if is_op(obj, \"__getattr__\"):\n            left_obj, left_attr = obj.args\n            \n            print(\"  Detected attribute chain: \", left_attr, attr_name)\n            \n            # if the left attr is dt, treat this like a method call\n            # e.g. _.dt.year\n            if left_attr == \"dt\":\n                # manually enter child nodes, now that we have all the information\n                # we need about them\n                args, kwargs = node.map_subcalls(self.enter)\n                new_obj = node.__class__(\"__getattr__\", *args, **kwargs)\n                # since it follows dt, put inside a call op\n                method_call = Call(\"__call__\", new_obj)\n                return self.exit(method_call)\n        \n        # otherwise, use default behavior\n        return self.generic_enter(node)\n\n    def exit___getattr__(self, node):\n        obj, attr_name = node.args\n        \n        print(\"Exiting attribute: \", attr_name)\n        \n        return node\n        \n\nmethod_maker = MethodMaker()\n\ncall = strip_symbolic(_.dt.year)\n\nprint(\"Call: \", call)\nmethod_maker.enter(call)\n\nCall:  _.dt.year\nEntering attribute:  year\n  Detected attribute chain:  dt year\nEntering attribute:  dt\nExiting attribute:  dt\n\n\n_.dt.year()\n\n\nOne limitation of this approach is that if we have an expression like‚Ä¶\n_.dt.year()\nWe‚Äôll still convert the year attribute to a call, causing us to call it twice!\n\nmethod_maker.enter(strip_symbolic(_.dt.year()))\n\nEntering attribute:  year\n  Detected attribute chain:  dt year\nEntering attribute:  dt\nExiting attribute:  dt\n\n\n_.dt.year()()\n\n\nTo get around this, we can extend MethodShouter to check whether an attribute has converted to a call\n\nclass MethodMaker2(MethodMaker):\n    def enter___call__(self, node):\n        # needs to use an enter call, since need to know\n        #   * what child was before entering\n\n        obj = node.args[0]\n        # don't want to return multiple calls,\n        # e.g. _.dt.year() shouldn't produce _.dt.year()()\n        if is_op(obj, \"__getattr__\"):\n            args, kwargs = node.map_subcalls(self.enter)\n\n            new_obj, *func_args = args\n            \n            # getattr transformed itself into a call node, but we're already\n            # calling, so peel off the call node it produced...\n            if is_op(new_obj, \"__call__\"):\n                new_call = Call(\"__call__\", new_obj.args[0], *func_args, **kwargs)\n                return self.exit(new_call)\n        \n        return self.generic_enter(node)\n\n    def exit___call__(self, node):\n        obj = node.args[0]\n        if is_op(obj, \"__getattr__\"):\n            left_obj, left_attr = obj.args\n            print(\"Exiting method call: \", left_attr)\n        \n        return node\n    \n        \n\nmethod_maker2 = MethodMaker2()\n\nmethod_maker2.enter(strip_symbolic(_.dt.year()))\n\nEntering attribute:  year\n  Detected attribute chain:  dt year\nEntering attribute:  dt\nExiting attribute:  dt\nExiting method call:  year\nExiting method call:  year\n\n\n_.dt.year()\n\n\nKeep in mind that using an enter method for an operator can do whatever an exit method for that operator could (and more!). However, there are two important caveats to keep in mind it usually requires more code, since it also needs to enter child nodes.\nWe can think of the order of enter and exit operations as a big sandwich, where exit is the last step an enter ‚Äúblock‚Äù takes. So if the exit doesn‚Äôt handle things, the enter can.\n_.hp + _.hp.rank()\n\nenter +(_.hp, _.hp.rank())\n  enter .(_, \"hp\")\n  exit\n  enter __call__(_.hp.rank)\n    enter .(_.hp, \"rank\")\n      enter .(_, \"hp\")\n      exit\n    exit\n  exit\nexit\n    \nIn this sense exit is best for actions that can happen after all other processing for a node has happened.\n\n\n\nTo show where an exit is useful‚Äìlet‚Äôs take the extra step of cutting out dt attributes. To do this, we can override our current getattr exit method (which is only a print statement right now).\n\nclass MethodMaker3(MethodMaker2):\n    def exit___getattr__(self, node):\n        obj, attr_name = node.args\n        \n        print(\"Exiting attribute: \", attr_name)\n        \n        if attr_name == \"dt\":\n            # cut out the dt node\n            return obj\n        \n        return node\n    \nmethod_maker3 = MethodMaker3()\n\n\n# before\nmethod_maker2.enter(call)\n\nEntering attribute:  year\n  Detected attribute chain:  dt year\nEntering attribute:  dt\nExiting attribute:  dt\nExiting method call:  year\n\n\n_.dt.year()\n\n\n\n# after\nmethod_maker3.enter(call)\n\nEntering attribute:  year\n  Detected attribute chain:  dt year\nEntering attribute:  dt\nExiting attribute:  dt\nExiting method call:  year\n\n\n_.year()\n\n\nFinally, it‚Äôs worth asking what will happen with the following call‚Ä¶\n\ncall3 = strip_symbolic(_.dt.dt())\ncall3\n\n_.dt.dt()\n\n\n\nmethod_maker3.enter(call3)\n\nEntering attribute:  dt\n  Detected attribute chain:  dt dt\nEntering attribute:  dt\nExiting attribute:  dt\nExiting method call:  dt\nExiting method call:  dt\n\n\n_.dt()\n\n\nNotice that there are two dt attributes, and they were both entered, but only one exited.\nWhy is this? To find the answer, you need to look at the enter___getattr__ method of the original MethodMaker class. More specifically, why doesn‚Äôt it exit the node its processing, when it creates a new Call node?"
  },
  {
    "objectID": "develop/guide_programming.html",
    "href": "develop/guide_programming.html",
    "title": "Programming guide",
    "section": "",
    "text": "import pandas as pd\n\npd.set_option(\"display.max_rows\", 5)"
  },
  {
    "objectID": "develop/guide_programming.html#overview",
    "href": "develop/guide_programming.html#overview",
    "title": "Programming guide",
    "section": "Overview",
    "text": "Overview\nThis document can be thought of as roughly having two segments. The core analysis interface gives users flexibility to independently specify data groupings, table verbs, and column operations. It also highlights a critical piece of siuba‚Äôs design‚Äìthat its column operations correspond to pandas Series methods.\nHere‚Äôs an example of this interface in use, colored with links the relevant sections.\n\nThe lazy execution interface allows users to declare what they want to perform, so developers can create backends to optimize how to execute it on different data sources (e.g.¬†SQL)."
  },
  {
    "objectID": "develop/guide_programming.html#column-operations",
    "href": "develop/guide_programming.html#column-operations",
    "title": "Programming guide",
    "section": "Column operations",
    "text": "Column operations\nIn general, column operations in siuba are pandas Series methods.\nFor example, the code below compares two ways to produce the same result: the DataFrame.assign() method, and siuba‚Äôs mutate() function.\n\nfrom siuba.data import cars\nfrom siuba import mutate\n\n# pandas assign method\ncars.assign( demean = lambda d: d.mpg - d.mpg.mean())\n\n# siuba mutate function\nmutate(cars, demean = lambda d: d.mpg - d.mpg.mean())\n\n\n\n\n\n  \n    \n      \n      cyl\n      mpg\n      hp\n      demean\n    \n  \n  \n    \n      0\n      6\n      21.0\n      110\n      0.909375\n    \n    \n      1\n      6\n      21.0\n      110\n      0.909375\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      8\n      15.0\n      335\n      -5.090625\n    \n    \n      31\n      4\n      21.4\n      109\n      1.309375\n    \n  \n\n32 rows √ó 4 columns\n\n\n\nNote that both are using pandas Series methods under the hood. This means that you can use and debug Series methods just like you would with pandas.\nFor grouped data, or a SQL database, siuba can‚Äôt use Series methods because they don‚Äôt exist. For example, on grouped data, the same operation above in pandas would be..\n\n# create grouped data\ng_cyl = cars.groupby('cyl')\n\n# error: g_cyl doesn't have an .assign method! :/\n# g_cyl.assign\n\ncars.assign(demean = g_cyl.mpg.transform(lambda x: x - x.mean()))\n\n\n\n\n\n  \n    \n      \n      cyl\n      mpg\n      hp\n      demean\n    \n  \n  \n    \n      0\n      6\n      21.0\n      110\n      1.257143\n    \n    \n      1\n      6\n      21.0\n      110\n      1.257143\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      8\n      15.0\n      335\n      -0.100000\n    \n    \n      31\n      4\n      21.4\n      109\n      -5.263636\n    \n  \n\n32 rows √ó 4 columns\n\n\n\nIn this case, the siuba code works similar to the pandas code above, but stays the same as in the ungrouped example:\nmutate(g_cyl, demean = lambda d: d.mpg - d.mpg.mean())\nFor SQL, it needs to go through a process to convert it to a SQL query using SQLAlchemy. One key step in this process is understanding how siuba can work on a DataFrame or a SQLAlchemy connection."
  },
  {
    "objectID": "develop/guide_programming.html#table-verbs",
    "href": "develop/guide_programming.html#table-verbs",
    "title": "Programming guide",
    "section": "Table verbs",
    "text": "Table verbs\nYou may be wondering how a siuba function, like mutate, could work on a SQL database. This is because these functions are defined using a technique called single dispatch. This approach allows you to define class-specific versions of a function.\nThe code below creates a function called head(), with an implementation that works specifically on a DataFrame.\n\nimport pandas as pd\nfrom siuba.dply.verbs import singledispatch2\n\n# DataFrame version of function ---\n\n@singledispatch2(pd.DataFrame)\ndef head(__data, n = 5):\n    return __data.head(n)\n\nhead(cars, 2)\n\n\n\n\n\n  \n    \n      \n      cyl\n      mpg\n      hp\n    \n  \n  \n    \n      0\n      6\n      21.0\n      110\n    \n    \n      1\n      6\n      21.0\n      110\n    \n  \n\n\n\n\nWe can define a SQL specific version, that acts on a SqlAlchemy Table by registering a new function, _head_sql.\n\n# SQL version of function ---\nfrom sqlalchemy import Table, Column, MetaData\n\n@head.register(Table)\ndef _head_sql(__data, n = 5):\n    return __data.select().limit(n)\n\ntable = Table(\"some_table\", MetaData(), Column('a'), Column('b'))\n\nprint(\n    head(table, 2)\n)\n\nSELECT some_table.a, some_table.b \nFROM some_table\n LIMIT :param_1\n\n\nwhy use singledispatch rather than a class method like mtcars.head()?\nThere are two big benefits:\n\nAnyone can cleanly define and package a function. Using it is just a matter of importing it. With a method, you need to somehow put it onto the class representing your data. You end up with 300+ methods on a class.\nYour function might do something that is not the class‚Äôs core responsibility. In this case, it should not be part of the class definition.\n\n\nGrouped data\nSince single dispatch functions define how to execute an action for a specific class of data, it allows siuba to handle grouped data in two ways:\n\npandas - register dispatchers for its special grouped data classes (DataFrameGroupBy, SeriesGroupBy).\nSQL - use a single class for grouped and ungrouped data, with grouping info as an attribute (siuba.sql.LazyTbl).\n\nFor example, here is a simple verb that calculates the number of rows in a grouped DataFrame.\n\nfrom pandas.core.groupby import DataFrameGroupBy\n\n@singledispatch2(DataFrameGroupBy)\ndef size(__data):\n    return __data.size()\n\nsize(cars.groupby('cyl'))\n\ncyl\n4    11\n6     7\n8    14\ndtype: int64\n\n\n\n\nHandling indexes\nMost siuba table verbs take a DataFrame, and return a DataFrame. Moreover, they don‚Äôt stick columns onto the index. This means you don‚Äôt need to call reset_index all the time.\nA common place where reset_index is called is after a pandas grouped aggregation.\n\nfrom siuba.data import mtcars\nfrom siuba import summarize\n\ng_cyl = mtcars.groupby(\"cyl\")\n\nagg_res = g_cyl[[\"hp\", \"mpg\"]].agg(\"mean\")\n\n# nooooo\nagg_res\n\n\n\n\n\n  \n    \n      \n      hp\n      mpg\n    \n    \n      cyl\n      \n      \n    \n  \n  \n    \n      4\n      82.636364\n      26.663636\n    \n    \n      6\n      122.285714\n      19.742857\n    \n    \n      8\n      209.214286\n      15.100000\n    \n  \n\n\n\n\n\n# good\nsummarize(g_cyl, hp = _.hp.mean(), mpg = _.mpg.mean())\n\n\n\n\n\n  \n    \n      \n      cyl\n      hp\n      mpg\n    \n  \n  \n    \n      0\n      4\n      138.045455\n      20.502165\n    \n    \n      1\n      6\n      138.045455\n      20.502165\n    \n    \n      2\n      8\n      138.045455\n      20.502165\n    \n  \n\n\n\n\n\n\nsingledispatch2\nOne thing to note is that siuba‚Äôs singledispatch implementation is called singledispatch2. This function (whose name will likely change!) is a very light wrapper around python‚Äôs built in functools.singledispatch that does two things:\n\nAllow verbs to be piped using data >> verb1() >> verb2() syntax.\nStrip out the symbolic part of lazy expressions.\n\nThese two concepts are covered in the next two sections."
  },
  {
    "objectID": "develop/guide_programming.html#pipe-syntax",
    "href": "develop/guide_programming.html#pipe-syntax",
    "title": "Programming guide",
    "section": "Pipe syntax",
    "text": "Pipe syntax\nIn the previous section I discussed how siuba uses singledispatch. This allows people to define new functions that are easy to package and import, as well as handle both a pandas DataFrame and SqlAlchemy table.\nOne challenge with using functions, rather than methods, is finding a way to combine them so they can be read from left to right, or top to bottom. In pandas this is done using method chaining. For example, the code below starts with cars, then runs .assign(), then runs .head().\n\n(cars\n  .assign(hp_per_cyl = lambda d: d.hp / d.cyl)\n  .head(2)\n)\n\n\n\n\n\n  \n    \n      \n      cyl\n      mpg\n      hp\n      hp_per_cyl\n    \n  \n  \n    \n      0\n      6\n      21.0\n      110\n      18.333333\n    \n    \n      1\n      6\n      21.0\n      110\n      18.333333\n    \n  \n\n\n\n\nHere is a similar version in siuba of the above code without piping.\n\nfrom siuba import head, mutate\n\n# without pipe ----\nhead(\n    mutate(\n        cars,\n        hp_per_cyl = lambda d: d.hp / d.cyl\n    ),\n    2\n)\n\n\n\n\n\n  \n    \n      \n      cyl\n      mpg\n      hp\n      hp_per_cyl\n    \n  \n  \n    \n      0\n      6\n      21.0\n      110\n      18.333333\n    \n    \n      1\n      6\n      21.0\n      110\n      18.333333\n    \n  \n\n\n\n\nNotice how head is run last in our method chain, but is the outer most function call. We have to read the calls inside out, and it‚Äôs hard to quickly spot the beginning (cars) and the end (head). This has been described as the dagwood sandwich problem.\nIn siuba, this is resolved by overloading >> to create pipes.\n\n# with pipe ----\n(cars\n  >> mutate(hp_per_cyl = lambda d: d.hp / d.cyl)\n  >> head(2)\n)\n\n\n\n\n\n  \n    \n      \n      cyl\n      mpg\n      hp\n      hp_per_cyl\n    \n  \n  \n    \n      0\n      6\n      21.0\n      110\n      18.333333\n    \n    \n      1\n      6\n      21.0\n      110\n      18.333333\n    \n  \n\n\n\n\nNotice also how in these cases we can import just the head and mutate functions. They are separated from the data classes (DataFrame, SQL source) that they can act on.\n\nGroups, verbs, and operations\n\nfrom siuba import group_by, mutate, filter, _\n\ngrouping = group_by(\"cyl\")\n\nverb1 = mutate\nverb2 = filter\n\noperation = lambda _: _.hp > _.hp.mean()\n\n\ncars >> grouping >> verb1(result = operation)\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      cyl\n      mpg\n      hp\n      result\n    \n  \n  \n    \n      0\n      6\n      21.0\n      110\n      False\n    \n    \n      1\n      6\n      21.0\n      110\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      8\n      15.0\n      335\n      True\n    \n    \n      31\n      4\n      21.4\n      109\n      True\n    \n  \n\n32 rows √ó 4 columns\n\n\n\n\ncars >> grouping >> verb2(operation)\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      cyl\n      mpg\n      hp\n    \n  \n  \n    \n      2\n      4\n      22.8\n      93\n    \n    \n      6\n      8\n      14.3\n      245\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      30\n      8\n      15.0\n      335\n    \n    \n      31\n      4\n      21.4\n      109\n    \n  \n\n15 rows √ó 3 columns\n\n\n\n\n\nPipeable class\nUnder the hood, function calls like below are turned into a Pipeable object.\n\nmutate(hp_per_cyl = lambda d: d.hp / d.cyl)\n\n<function mutate at 0x7f3cb8b8fdc0>(_,hp_per_cyl = <function <lambda> at 0x7f3cb8b22430>())\n\n\nThere are two implicit cases that create a Pipeable.\n\nThe function receives no positional arguments\nThe function‚Äôs first positional argument is not a registered data source, like a DataFrame or SQL database.\n\nAlternatively, you can explicitly create a pipe by passing an instance of siuba‚Äôs Symbolic class. This is shown in the code below.\n\nfrom siuba import _\n\nmutate(_, hp_per_cyl = lambda d: d.hp / d.cyl)\n\n<function mutate at 0x7f3cb8b8fdc0>(_,hp_per_cyl = <function <lambda> at 0x7f3cb0f820d0>())\n\n\nIn this case, we are taking advantage of siuba‚Äôs lazy expressions, and using _ as a ‚Äúplaceholder‚Äù for the data. Since _ is an instance of the Symbolic class, this is just using single dispatch with one small twist.\nThe code below shows all the classes mutate can dispatch on.\n\nlist(mutate.registry.keys())\n\n[object,\n pandas.core.frame.DataFrame,\n siuba.siu.calls.Call,\n siuba.siu.dispatchers.NoArgs,\n pandas.core.groupby.generic.DataFrameGroupBy]\n\n\nNote that Symbolic isn‚Äôt in the list, but something called a Call is. The twist is that siuba‚Äôs singledispatch2 strips down the Symbolic to something less useful to users, but safer to work with: a Call.\nThe path from Symbolic to Pipeable is shown below.\n\n# this stripping down is done by singledispatch2\nfrom siuba.siu import strip_symbolic, _\nsym = _                         # a Symbolic\ncall = strip_symbolic(sym)      # a Call\n\n# dispatching on Call to create a pipeable\nmutate(call)\n\n<function mutate at 0x7f3cb8b8fdc0>(_)\n\n\nThe following section explains the relationship between Symbolic and Call, and how they enable lazy expressions."
  },
  {
    "objectID": "develop/guide_programming.html#lazy-expressions",
    "href": "develop/guide_programming.html#lazy-expressions",
    "title": "Programming guide",
    "section": "Lazy expressions",
    "text": "Lazy expressions\nTogether with single dispatch and pipes, lazy expressions allow you to separate declaring what actions to perform, from how to perform those actions.\nUp to this point, we‚Äôve used lambda functions to express operations on a DataFrame‚Äôs columns, but we could have used lazy expressions with _.\n\nfrom siuba import _, summarize\n\n# lambda approach\nsummarize(cars, hp_mean = lambda d: d.hp.mean())\n\n# lazy expression approach\nsummarize(cars, avg_hp = _.hp.mean())\n\n\n\n\n\n  \n    \n      \n      avg_hp\n    \n  \n  \n    \n      0\n      146.6875\n    \n  \n\n\n\n\nPeppering an analysis with lambda functions creates two challenges:\n\nwriting lambda d: can take up as many characters as its operation.\nlambdas can lazily do some work, but they can‚Äôt tell us what work they will do.\n\n\nWhat vs how\nConsider this symbolic, lazy expression below.\n\nf = _.hp.mean()\nf\n\n‚ñà‚îÄ'__call__'\n‚îî‚îÄ‚ñà‚îÄ.\n  ‚îú‚îÄ‚ñà‚îÄ.\n  ‚îÇ ‚îú‚îÄ_\n  ‚îÇ ‚îî‚îÄ'hp'\n  ‚îî‚îÄ'mean'\n\n\nIts print-out represents the expression as an abstract syntax tree (AST).\nThis means we can either choose to execute f like a function.\n\nf(cars)\n\n146.6875\n\n\nOr run something over it that can analyze and transform the AST.\n\n# NOTE: this cell is hidden\n\nfrom siuba.siu import strip_symbolic, Call, _, BinaryOp\n\ndef replace_attr(call, src, dst):\n    # just in case we pass a Symbolic\n    call = strip_symbolic(call)\n    \n    # check that is _.<src>, or the last part of _.abc.<src>\n    if call.func == \"__getattr__\" and call.args[1] == src:\n        # eg: obj = _, src_attr = \"hp\"\n        obj, src_attr = call.args\n        \n        # recreate call, but with dst as the attribute\n        return BinaryOp(\"__getattr__\", obj, dst)\n    \n    return call.map_replace(lambda child: replace_attr(child, src, dst))\n\n\n# replace mpg with hp\nnew_f = replace_attr(_.hp.mean() / _.cyl, 'hp', 'mpg')\n\nnew_f\n\n_.mpg.mean() / _.cyl\n\n\n\nnew_f(cars)\n\n0     3.348438\n1     3.348438\n        ...   \n30    2.511328\n31    5.022656\nName: cyl, Length: 32, dtype: float64\n\n\nAs a more involved example, here is some code that generates a SQL query.\n\nfrom siuba.data import cars_sql\nimport siuba.sql\nfrom siuba import show_query\n\nq = (cars_sql \n  >> group_by(\"cyl\")\n  >> mutate(\n       demeaned = _.hp - _.hp.mean(),\n       mpg_per_hp = _.mpg / _.hp,\n  )\n  >> show_query()\n)\n\nSELECT cars.cyl, cars.mpg, cars.hp, cars.hp - avg(cars.hp) OVER (PARTITION BY cars.cyl) AS demeaned, CAST(cars.mpg AS FLOAT) / cars.hp AS mpg_per_hp \nFROM cars\n\n\nIn this section, we‚Äôll discuss in detail the two classes that make declaring what possible‚ÄìSymbolic and Call. In the next section we‚Äôll go over the code for the replace_attr function, and the tools that make generating the SQL query above possible.\n\n\nSymbolic and Call\nLazy expressions are implemented through two classes:\n\nCall: the actual representation of a lazy expression\nSymbolic: a convenience class to quickly create Calls\n\nThe code below shows the action data.a + 1 created using only the Call approach.\n\n# call approach\nfrom siuba.siu import BinaryOp, MetaArg\n\nBinaryOp(\"__add__\",\n    BinaryOp(\"__getattr__\", MetaArg(\"_\"), \"a\"),\n    1,\n)\n\n_.a + 1\n\n\nAnd again using the Symbolic instance, _.\n\nfrom siuba.siu import _, strip_symbolic\n\nstrip_symbolic(_.a + 1)\n\n_.a + 1\n\n\nNote that a Symbolic‚Äôs only job is to create Calls, so strip_symbolic is just getting the Call out (it is a ‚Äúprivate‚Äù property).\n\n\nUser defined functions\nDeclaring operations like _.some_method() is enough for most cases, but sometimes a person might want to use an external function.\nFor example, siuba comes with functions that aren‚Äôt covered by pandas methods, or that work in a different way.\n\nfrom siuba.dply.vector import n\n\nn(_.hp)\n\n‚ñà‚îÄ'__call__'\n‚îú‚îÄ‚ñà‚îÄ'__custom_func__'\n‚îÇ ‚îî‚îÄ<function n at 0x7f3cb0d900d0>\n‚îî‚îÄ‚ñà‚îÄ.\n  ‚îú‚îÄ_\n  ‚îî‚îÄ'hp'\n\n\nIn this case, the function n() is represented as a simple subtype of Call, called a FuncArg.\n\nfrom siuba.siu import Call, FuncArg, Symbolic\n\nfunc_arg = Symbolic(FuncArg(\"__custom_func__\", n))\n\nfunc_arg(_.hp)\n\n‚ñà‚îÄ'__call__'\n‚îú‚îÄ‚ñà‚îÄ'__custom_func__'\n‚îÇ ‚îî‚îÄ<function n at 0x7f3cb0d900d0>\n‚îî‚îÄ‚ñà‚îÄ.\n  ‚îú‚îÄ_\n  ‚îî‚îÄ'hp'\n\n\nSimilar to piping, this happens because functions like n are defined using singledispatch called symbolic_dispatch. When a function using symbolic_dispatch receives a Symbolic or Call as its first argument, it returns a Symbolic.\n\nfrom siuba.siu import symbolic_dispatch\n\n@symbolic_dispatch\ndef another_n(x):\n    return len(x)\n\n\nanother_n([1,2,3])\n\n3\n\n\n\nanother_n(_.a)\n\n‚ñà‚îÄ'__call__'\n‚îú‚îÄ‚ñà‚îÄ'__custom_func__'\n‚îÇ ‚îî‚îÄ<function another_n at 0x7f3cb0cdbdc0>\n‚îî‚îÄ‚ñà‚îÄ.\n  ‚îú‚îÄ_\n  ‚îî‚îÄ'a'\n\n\n\nsym = another_n(_.a)\nsym(pd.DataFrame({'a': [0, 1, 2]}))\n\n3\n\n\nThis is a simple wrapper around python‚Äôs singledispatch, so you can use all the tools that come with it. The most useful is printing out the classes that it can dispatch on.\n\nanother_n.registry.keys()\n\ndict_keys([<class 'object'>, <class 'siuba.siu.symbolic.Symbolic'>, <class 'siuba.siu.calls.Call'>])\n\n\n\n\nCaveats\nsiuba‚Äôs lazy expressions open a whole range of behaviors, including implementing new execution backends. However, there are some limitations to their use, compared to lambda functions.\nFirst, they are not guaranteed to work inside a function that does not know about Symbolics. In these cases you can switch back to a lambda.\ndf = pd.DataFrame({'date_raw': ['2019-01-01']})\n\n# not okay\npd.to_datetime(_.date_raw)\n\n# okay\nlambda _: pd.to_datetime(_.date_raw)\nOften there is an alternative method that will do the same thing.\n\ndf = pd.DataFrame({'date_raw': ['2019-01-01']})\n\ndf.date_raw.astype(\"datetime64[ns]\")\n\n0   2019-01-01\nName: date_raw, dtype: datetime64[ns]\n\n\nThis limitation can be thought of as similar to pandas asking for method names to be strings some times. In pandas, this is due to not using lazy expressions, while in siuba the limitations are for the opposite reason!\n\ncars.groupby('cyl').hp.transform('mean')\n\n0     122.285714\n1     122.285714\n         ...    \n30    209.214286\n31     82.636364\nName: hp, Length: 32, dtype: float64\n\n\nSecond, outside of siuba functions, Symbolic cannot be called like a lambda when it ends with getting an attribute.\n\n# not okay, . is outermost (final) operation\n(_ + _).x\n\n‚ñà‚îÄ.\n‚îú‚îÄ‚ñà‚îÄ+\n‚îÇ ‚îú‚îÄ_\n‚îÇ ‚îî‚îÄ_\n‚îî‚îÄ'x'\n\n\n\n# okay\n_.x + _.y\n\n‚ñà‚îÄ+\n‚îú‚îÄ‚ñà‚îÄ.\n‚îÇ ‚îú‚îÄ_\n‚îÇ ‚îî‚îÄ'x'\n‚îî‚îÄ‚ñà‚îÄ.\n  ‚îú‚îÄ_\n  ‚îî‚îÄ'y'\n\n\nNote that for functions created through singledispatch2 or symbolic_dispatch, any Symbolic is fine.\nThird, they can‚Äôt work with Python methods that are required to return booleans. This includes methods like __and__. For the most part, this is similar to the restrictions around numpy arrays and pandas Series.\n\n# should use: _ && 1\n_ and 1\n\nTypeError: Symbolic objects can not be converted to True/False, or used with these keywords: not, and, or.\n\n\nOne case where this uniquely bites Symbolics is __contains__, but siuba is careful to raise an error.\n\n1 in _\n\nTypeError: 'Symbolic' object is not a container"
  },
  {
    "objectID": "develop/guide_programming.html#translating-operations",
    "href": "develop/guide_programming.html#translating-operations",
    "title": "Programming guide",
    "section": "Translating operations",
    "text": "Translating operations\nYou may have noticed in previous sections that some features‚Äìlike fast pandas grouped operations and executing SQL‚Äìrequire declaring what you want to do with _. Ultimately, when you write an operation like _.a + _.b, it results in a new Symbolic object that can do two things.\n\nbe executed like a lambda\nallow self-representation\n\nThis representation is called an Abstract Syntax Tree (AST).\n\nsymbol = _.a + _.b\n\nsymbol\n\n‚ñà‚îÄ+\n‚îú‚îÄ‚ñà‚îÄ.\n‚îÇ ‚îú‚îÄ_\n‚îÇ ‚îî‚îÄ'a'\n‚îî‚îÄ‚ñà‚îÄ.\n  ‚îú‚îÄ_\n  ‚îî‚îÄ'b'\n\n\n\nfrom siuba.siu import strip_symbolic\n\ncall = strip_symbolic(symbol)\ncall\n\n_.a + _.b\n\n\nA call has three attributes\n\nfunc: the function being called (eg __add__ for addition)\nargs: positional arguments passed to the call\nkwargs: keyword arguments passed to the call\n\nThis makes it very easy to inspect and modify. For example, we could change the function from addition to subtraction.\n\ncall.func = '__sub__'\ncall\n\n_.a - _.b\n\n\nNote that in practice, calls should not be modified in place like that. They also often contain other calls, shown as black boxes on the AST below.\n\n_.hp / _.mpg\n\n‚ñà‚îÄ/\n‚îú‚îÄ‚ñà‚îÄ.\n‚îÇ ‚îú‚îÄ_\n‚îÇ ‚îî‚îÄ'hp'\n‚îî‚îÄ‚ñà‚îÄ.\n  ‚îú‚îÄ_\n  ‚îî‚îÄ'mpg'\n\n\nHere, the top level call is a division (/), with two ‚Äúget attribute‚Äù child calls. We can iterate over the children manually.\n\ncall = strip_symbolic(_.hp / _.mpg)\n\nfor arg in call.args:\n    if isinstance(arg, Call):\n        print(\"child call:\", arg)\n        \nprint(\"number of kwargs:\", len(call.kwargs))\n\nchild call: _.hp\nchild call: _.mpg\nnumber of kwargs: 0\n\n\nThere are also some Call methods designed to make this easier. The first method, map_subcalls, runs a function on each subcall.\n\ncall = strip_symbolic(_.hp / _.mpg)\nargs, kwargs = call.map_subcalls(repr)\n\nprint('args:', args)\nprint('kwargs:', kwargs)\n\nargs: ('_.hp', '_.mpg')\nkwargs: {}\n\n\nThe second method, map_replace, does the same thing, but replaces the child call the result.\n\ncall.map_replace(\n    lambda child: BinaryOp(\"__getitem__\", *child.args, **child.kwargs)\n)\n\n_['hp'] / _['mpg']\n\n\n\nExample: replace_attr\nIn the Lazy expressions section, we showed off a function that could replace pieces of a Call.\n\nreplace_attr(_.hp.mean(), \"hp\", \"some_other_name\")\n\n_.some_other_name.mean()\n\n\nHere is the full code for replace_attr.\n\nfrom siuba.siu import strip_symbolic, Call, _, BinaryOp\n\ndef replace_attr(call, src, dst):\n    # just in case we pass a Symbolic\n    call = strip_symbolic(call)\n    \n    # check that is _.<src>, or the last part of _.abc.<src>\n    if call.func == \"__getattr__\" and call.args[1] == src:\n        # eg: obj = _, src_attr = \"hp\"\n        obj, src_attr = call.args\n        \n        # recreate call, but with dst as the attribute\n        return BinaryOp(\"__getattr__\", obj, dst)\n    \n    return call.map_replace(lambda child: replace_attr(child, src, dst))\n\n\n\nCallVisitor\nThe siuba.siu module implements a common class called a visitor. Using a visitor allows you to customize a translation, depending on the kind of Call (or child call) you are working on. This is nearly identical to the class of the same name in python‚Äôs built in ast module.\n\nfrom siuba.siu import _, strip_symbolic, CallVisitor\n\nclass MyVisitor(CallVisitor):\n    def visit___getattr__(self, node):\n        print('get attribute:', node.args[1])\n        \n        self.generic_visit(node)\n    \n    def visit___call__(self, node):\n        print('call', node.args[0])\n        \n        self.generic_visit(node)\n        \ncall = strip_symbolic(_.a.b.c() + _.x)\nMyVisitor().visit(call)\n\ncall _.a.b.c\nget attribute: c\nget attribute: b\nget attribute: a\nget attribute: x\n\n\nTo learn more, see the developer docs ‚ÄúCall tree processing‚Äù, ‚ÄúSQL translators‚Äù. Moreover, the architecture decision record on user defined functions, lays out the rationale behind siuba‚Äôs translation tools."
  },
  {
    "objectID": "develop/guide_programming.html#backends",
    "href": "develop/guide_programming.html#backends",
    "title": "Programming guide",
    "section": "Backends",
    "text": "Backends\n\nüößThis section is a work in progress. See THIS DOC (TODO) for a tutorial on the SQL backend.\n\nReference to‚Ä¶\n\nSQL UDF example\nADR on call trees and SQL"
  },
  {
    "objectID": "develop/guide_programming.html#nested-data",
    "href": "develop/guide_programming.html#nested-data",
    "title": "Programming guide",
    "section": "Nested data",
    "text": "Nested data\n\nüößThis section is a work in progress\n\nA final piece of siuba‚Äìwhich is more an area of active research‚Äìis how to effectively nest data. For example, can a user make a column, where each entry is a DataFrame, or fitted model? This approach is a critical component to R libraries like dplyr, as it allows users to flexibly handle hierarchical data.\nFor nesting to work with a backend like pandas, we need:\n\nDataFrame creation to be very fast (currently not the case).\nStrategies for handling combining many DataFrames (works well in pandas).\n\nUnfortunately, siuba is stuck without quick DataFrame creation. Intriguingly, aspects of this situation mirror / anticipate part of the developmental trajectory of dplyr, which implemented a stripped down form of data.frame called a tibble. Fortunately for siuba, the pandas DataFrame has well-documented options for subclassing, so we may be able to get away with a stripped down, fast DataFrame implementation for restricted cases.\nA stripped down DataFrame might involve‚Ä¶\n\nremoving most of the index (or essentially no-op‚Äôing it)\nvery little type conversion (so expensive conversions happen at the very end of a table verb)\n\nFor more details, see the blog post, ‚ÄúWhat would it take to recreate dplyr in python?‚Äù."
  },
  {
    "objectID": "develop/index.html",
    "href": "develop/index.html",
    "title": "Developing siuba",
    "section": "",
    "text": "Core\n\n\n\nProgramming guide\nDESCRIPTION\n\n\nSql Translators\nA closer look at the translation process, focused on SQL and the CallTreeLocal tree listener.\n\n\nCall Trees\nCalls represent ‚Äúwhat‚Äù operations users want to do. This document describes how they are constructed, transformed, and executed.\n\n\nFast Pandas Grouped Ops\nWhy are grouped operations cumbersome? How does siuba simplify them?"
  },
  {
    "objectID": "develop/pandas-group-ops.html",
    "href": "develop/pandas-group-ops.html",
    "title": "Siuba",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\n\nnp.random.seed(123)\nstudents = pd.DataFrame({\n    'student_id': np.repeat(np.arange(2000), 10),\n    'course_id': np.random.randint(1, 20, 20000),\n    'score': np.random.randint(1, 100, 20000)\n})\n\ng_students = students.groupby('student_id')\n\n\n\nIf you just need to make a single calculation, then pandas methods are very fast. For example, take the code below, which calculates the minimum score for each student.\n\n%%timeit\ng_students.score.min()\n\n409 ¬µs ¬± 6.34 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n\n\nThis took very little time (less than a millisecond, which is 1 thousandth of a second!).\nHowever, now suppose you wanted to do something more complex. Let‚Äôs say you wanted to get rows corresponding to each students minimum score. In pandas, there are two ways to do this:\n\ntransform with a lambda\nby using both the students and g_student data frames.\n\nThese are shown below.\n\n%%timeit\nis_student_min = g_students.score.transform(lambda x: x == x.min())\ndf_min1 = students[is_student_min]\n\n653 ms ¬± 4.77 ms per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n\n\n\n%%timeit\nis_student_min = students.score == g_students.score.transform('min')\ndf_min2 = students[is_student_min]\n\n1.15 ms ¬± 17.6 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n\n\nNote that while the first one could be expressed using only the grouped data (g_student), it took over a second to run!\nOn the other hand, while the other was fairly quick, it required juggling two forms of the data.\nSiuba attempts to optimize these operations to be quick AND require less data juggling.\n\n\n\n\nfrom siuba.experimental.pd_groups import fast_mutate, fast_filter, fast_summarize\nfrom siuba import _\n\n\n%%timeit\ndf_min3 = fast_filter(g_students, _.score == _.score.min())\n\n2.22 ms ¬± 55.5 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\n\n\n\n%%timeit\nfast_mutate(students, is_low_score = _.score == _.score.min())\n\n817 ¬µs ¬± 5.62 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1,000 loops each)\n\n\n\n%%timeit\nfast_summarize(g_students, lowest_percent = _.score.min() / 100.)\n\n2.17 ms ¬± 33.3 ¬µs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\n\n\n\n\n\nSiuba replaces important parts of the call tree‚Äìlike == and score()‚Äìwith functions that take a grouped series and return a grouped series. Because it then becomes grouped series all the way down, these operations are nicely composable.\n\n_.score == _.score.min()\n\n‚ñà‚îÄ==\n‚îú‚îÄ‚ñà‚îÄ.\n‚îÇ ‚îú‚îÄ_\n‚îÇ ‚îî‚îÄ'score'\n‚îî‚îÄ‚ñà‚îÄ'__call__'\n  ‚îî‚îÄ‚ñà‚îÄ.\n    ‚îú‚îÄ‚ñà‚îÄ.\n    ‚îÇ ‚îú‚îÄ_\n    ‚îÇ ‚îî‚îÄ'score'\n    ‚îî‚îÄ'min'\n\n\nAfter the expressions are executed, the verb in charge handles the output. For example, fast_filter uses the result (usually a boolean Series) to keep only rows where the result is True.\nAn example is shown below, for how siuba replaces the ‚Äúmean‚Äù function.\n\nfrom siuba.experimental.pd_groups.translate import method_agg_op\n\nf_mean = method_agg_op('mean', False, None)\n\n# result is a subclass of SeriesGroupBy\nres_agg = f_mean(g_students.score)\n\nprint(res_agg)\nprint(res_agg.obj.head())\n\n<siuba.experimental.pd_groups.groupby.GroupByAgg object at 0x7ffb8d1af880>\nstudent_id\n0    48.9\n1    51.2\n2    41.3\n3    46.8\n4    47.6\nName: score, dtype: float64\n\n\n\n\n\nI‚Äôm in the progress of writing out documentation on custom operations. For more context on strategic decisions made during their implementation, see this architecture doc.\nAn example of implementing a custom cumulative mean function is below.\n\nfrom siuba.siu import symbolic_dispatch\nfrom pandas.core.groupby import SeriesGroupBy, GroupBy\nfrom pandas import Series\n\n@symbolic_dispatch(cls = Series)\ndef cummean(x):\n    \"\"\"Return a same-length array, containing the cumulative mean.\"\"\"\n    return x.expanding().mean()\n\n\n@cummean.register(SeriesGroupBy)\ndef _cummean_grouped(x) -> SeriesGroupBy:\n    grouper = x.grouper\n    n_entries = x.obj.notna().groupby(grouper).cumsum()\n\n    res = x.cumsum() / n_entries\n\n    return res.groupby(grouper)\n\nfrom siuba import _, mutate\nfrom siuba.data import mtcars\n\n# a pandas DataFrameGroupBy object\ng_cyl = mtcars.groupby(\"cyl\")\n\nmutate(g_students, cumul_mean = cummean(_.score))\n\n\n(grouped data frame)\n\n\n  \n    \n      \n      student_id\n      course_id\n      score\n      cumul_mean\n    \n  \n  \n    \n      0\n      0\n      14\n      38\n      38.000000\n    \n    \n      1\n      0\n      3\n      40\n      39.000000\n    \n    \n      2\n      0\n      3\n      17\n      31.666667\n    \n    \n      3\n      0\n      7\n      74\n      42.250000\n    \n    \n      4\n      0\n      18\n      29\n      39.600000\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      19995\n      1999\n      10\n      16\n      37.833333\n    \n    \n      19996\n      1999\n      9\n      74\n      43.000000\n    \n    \n      19997\n      1999\n      3\n      1\n      37.750000\n    \n    \n      19998\n      1999\n      11\n      32\n      37.111111\n    \n    \n      19999\n      1999\n      17\n      10\n      34.400000\n    \n  \n\n20000 rows √ó 4 columns\n\n\n\nNote that this approach requires that the custom grouped function (_cummean_grouped) specify a type annotation for its return value."
  },
  {
    "objectID": "develop/sql-translators.html",
    "href": "develop/sql-translators.html",
    "title": "Siuba",
    "section": "",
    "text": "The purpose of this vignette is to walk through how expressions like _.id.mean() are converted into SQL.\nThis process involves 3 parts\n\nSQL translation functions, e.g.¬†taking column ‚Äúid‚Äù and producing the SQL ‚ÄúROUND(id)‚Äù.\nSQL translation from a symbolic call\n\n\nConverting method calls like _.id.round(2) to round(_.id, 2)\nLooking up SQL translators (e.g.¬†for ‚Äúmean‚Äù function call)\n\n\nHandling SQL partitions, like in OVER clauses\n\n\n\nThroughout this vignette, we‚Äôll use a select statement object from sqlalchemy, so we can conveniently access its columns as needed.\n\nfrom sqlalchemy import sql\ncol_names = ['id', 'x', 'y']\nsel = sql.select([sql.column(x) for x in col_names])\n\nprint(sel)\nprint(type(sel.columns))\nprint(sel.columns)\n\nSELECT id, x, y\n<class 'sqlalchemy.sql.base.ImmutableColumnCollection'>\n['id', 'x', 'y']\n\n\n\n\n\nA SQL translator function takes‚Ä¶\n\na first argument that is a sqlalchemy Column\n(optional) additional arguments for the translation\n\n\n\n\nf_simple_round = lambda col, n: sql.func.round(col, n)\n\nsql_expr = f_simple_round(sel.columns.x, 2)\n\nprint(sql_expr)\n\nround(x, :round_1)\n\n\nThe function above is essentially what most translator functions are.\nFor example, here is the round function defined for postgresql. One key difference is that it casts the column to a numeric beforehand.\n\nfrom siuba.sql.dialects.postgresql import funcs\n\nf_round = funcs['scalar']['round']\nsql_expr = f_round(sel.columns.x, 2)\n\nprint(sql_expr)\n\nround(CAST(x AS NUMERIC), :round_1)\n\n\n\n\n\n\nf_win_mean = funcs['window']['mean']\n\nsql_over_expr = f_win_mean(sel.columns.x)\n\nprint(type(sql_over_expr))\nprint(sql_over_expr)\n\n<class 'siuba.sql.translate.AggOver'>\navg(x) OVER ()\n\n\nNotice that this window expression has an empty over clause. This clause needs to be able to include any variables we‚Äôve grouped the data by.\nSiuba handles this by implementing a set_over method on these custom sqlalchemy Over clauses, which takes grouping and ordering variables as arguments.\n\ngroup_by_clause = sql.elements.ClauseList(sel.columns.x, sel.columns.y)\nprint(sql_over_expr.set_over(group_by_clause))\n\navg(x) OVER (PARTITION BY x, y)\n\n\n\n\n\n\nThe section above discusses how SQL translators are functions that take a sqlalchemy column, and return a SQL expression. However, when using siuba we often have expressions like‚Ä¶\nmutate(data, x = _.y.round(2))\nIn this case, before we can even use a SQL translator, we need to‚Ä¶\n\nfind the name and arguments of the method being called\nfind the column it is being called on\n\nThis is done by using the CallTreeLocal class to analyze the tree of operations for each expression.\n\nfrom siuba.siu import Lazy, CallTreeLocal, Call, strip_symbolic\nfrom siuba import _\n\n_.y.round(2)\n\n‚ñà‚îÄ'__call__'\n‚îú‚îÄ‚ñà‚îÄ.\n‚îÇ ‚îú‚îÄ‚ñà‚îÄ.\n‚îÇ ‚îÇ ‚îú‚îÄ_\n‚îÇ ‚îÇ ‚îî‚îÄ'y'\n‚îÇ ‚îî‚îÄ'round'\n‚îî‚îÄ2\n\n\n\n\n\nfrom siuba.sql.dialects.postgresql import funcs\n\nlocal_funcs = {**funcs['scalar'], **funcs['window']}\n\ncall_shaper = CallTreeLocal(\n    local_funcs,\n    call_sub_attr = ('dt',)\n    )\n\n\nsymbol = _.id.mean()\ncall = strip_symbolic(symbol)\nprint(call)\n\n_.id.mean()\n\n\n\nfunc_call = call_shaper.enter(call)\nprint(func_call(sel.columns))\n\navg(id) OVER ()\n\n\nThis is the same result as when we called the SQL translator for mean manually! In that section we also showed that we can set group information, so that it takes an average within each group.\nIn this case it‚Äôs easy to set group information to the Over clause. However, an additional challenge is when it‚Äôs part of a larger expression‚Ä¶\n\ncall2 = strip_symbolic(_.id.mean() + 1)\nfunc_call2 = call_shaper.enter(call2)\n\nfunc_call2(sel.columns)\n\n<sqlalchemy.sql.elements.BinaryExpression object at 0x11889ada0>\n\n\n\n\n\n\nWhile the first section showed how siuba‚Äôs custom Over clauses can add grouping info to a translation, it is missing one key detail: expressions that generate Over clauses, like _.id.mean(), can be part of larger expressions. For example _.id.mean() + 1.\nIn this case, if we look at the call tree for that expression, the top operation is the addition‚Ä¶\n\n_.id.mean() + 1\n\n‚ñà‚îÄ+\n‚îú‚îÄ‚ñà‚îÄ'__call__'\n‚îÇ ‚îî‚îÄ‚ñà‚îÄ.\n‚îÇ   ‚îú‚îÄ‚ñà‚îÄ.\n‚îÇ   ‚îÇ ‚îú‚îÄ_\n‚îÇ   ‚îÇ ‚îî‚îÄ'id'\n‚îÇ   ‚îî‚îÄ'mean'\n‚îî‚îÄ1\n\n\nHow can we create the appropriate expression‚Ä¶\navg(some_col) OVER (PARTITION BY x, y) + 1\nwhen the piece that needs grouping info is not easily accessible? The answer is by using a tree visitor, which steps down every black rectangle in the call tree shown above, from top to bottom.\n\n\nBelow, we copy the code from the call shaping section..\n\nfrom siuba.sql.verbs import track_call_windows\nfrom siuba import _\nfrom siuba.sql.dialects.postgresql import funcs\n\nlocal_funcs = {**funcs['scalar'], **funcs['window']}\n\ncall_shaper = CallTreeLocal(\n    local_funcs,\n    call_sub_attr = ('dt',)\n    )\n\nsymbol3 = _.id.mean() + 1\ncall3 = strip_symbolic(symbol3)\nfunc_call3 = call_shaper.enter(call3)\n\nFinally, we pass the shaped call‚Ä¶\n\ncol, windows, window_cte = track_call_windows(\n    func_call3,\n    sel.columns,\n    group_by = ['x', 'y'],\n    order_by = [],\n    # note that this is optional, and results in window_cte being a\n    # copy of this select that contains the window clauses\n    window_cte = sel.select()\n    )\n\nprint(col)\nprint(windows)\nprint(window_cte)\n\navg(id) OVER (PARTITION BY x, y) + :param_1\n[<sqlalchemy.sql.elements.Label object at 0x11889ec50>, <sqlalchemy.sql.elements.Label object at 0x11889e1d0>]\nSELECT id, x, y, avg(id) OVER (PARTITION BY x, y) AS win1, avg(id) OVER (PARTITION BY x, y) + :param_1 AS win2 \nFROM (SELECT id, x, y)"
  },
  {
    "objectID": "contribute/index.html#open-leadership-roles",
    "href": "contribute/index.html#open-leadership-roles",
    "title": "Contribute",
    "section": "Open Leadership Roles",
    "text": "Open Leadership Roles"
  },
  {
    "objectID": "contribute/index.html#developing-siuba",
    "href": "contribute/index.html#developing-siuba",
    "title": "Contribute",
    "section": "Developing Siuba",
    "text": "Developing Siuba"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Siuba",
    "section": "",
    "text": "tutorial\n\n\npandas\n\n\n\n\n\n\n\n\n\n\n\nSep 21, 2022\n\n\nMichael Chow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/tremendous-tricks/index.html",
    "href": "blog/tremendous-tricks/index.html",
    "title": "10 Tremendous Tricks of the Siubaverse",
    "section": "",
    "text": "from gapminder import gapminder\nfrom siuba import _"
  },
  {
    "objectID": "blog/tremendous-tricks/index.html#count",
    "href": "blog/tremendous-tricks/index.html#count",
    "title": "10 Tremendous Tricks of the Siubaverse",
    "section": "count()",
    "text": "count()\n# uses phd data, need a wt argument\n#count(gapminder, _.continent, sort=True, wt=, name=\"graduates\")"
  },
  {
    "objectID": "blog/tremendous-tricks/index.html#count-or-group_by-with-expressions",
    "href": "blog/tremendous-tricks/index.html#count-or-group_by-with-expressions",
    "title": "10 Tremendous Tricks of the Siubaverse",
    "section": "count() or group_by() with expressions",
    "text": "count() or group_by() with expressions\n# maryland bridges\ncount(maryland_bridges, decade = 10 * (_.year // 10))\n\n# then plot"
  },
  {
    "objectID": "blog/tremendous-tricks/index.html#add_count",
    "href": "blog/tremendous-tricks/index.html#add_count",
    "title": "10 Tremendous Tricks of the Siubaverse",
    "section": "add_count()",
    "text": "add_count()\n# show manual counting approach\n# space launches\n# launches >> add_count(_.type) >> filter(_.n >= 20)"
  },
  {
    "objectID": "blog/tremendous-tricks/index.html#summarize-to-create-a-list-column",
    "href": "blog/tremendous-tricks/index.html#summarize-to-create-a-list-column",
    "title": "10 Tremendous Tricks of the Siubaverse",
    "section": "summarize() to create a list column",
    "text": "summarize() to create a list column\n# nyc restaurant data\n# all a t-test\n# (restaurants\n#     >> group_by(_.cuisine)\n#     >> summarize(test = list())\n# )"
  },
  {
    "objectID": "blog/tremendous-tricks/index.html#fct_reorder-geom_col-coord_flip",
    "href": "blog/tremendous-tricks/index.html#fct_reorder-geom_col-coord_flip",
    "title": "10 Tremendous Tricks of the Siubaverse",
    "section": "fct_reorder() + geom_col() + coord_flip()",
    "text": "fct_reorder() + geom_col() + coord_flip()\n# uses it across many datasets\n# bob ross\n# tennis grand slams\n# us majors?\n# common words in medium post titles\n# franchises"
  },
  {
    "objectID": "blog/tremendous-tricks/index.html#fct_lump",
    "href": "blog/tremendous-tricks/index.html#fct_lump",
    "title": "10 Tremendous Tricks of the Siubaverse",
    "section": "fct_lump()",
    "text": "fct_lump()\n# horror movies (but do space launches)\n#(horror_movies\n#    >> mutate(rating = fct_lump(rating, 5))\n#    >> mutate(rating = fct_reorder(_.rating, _.score))\n#    >> ggplot(aes(\"rating\", \"score\"))\n#    + geom_boxplot()\n#    + coord_flip()\n#)"
  },
  {
    "objectID": "blog/tremendous-tricks/index.html#scale_xy_log10",
    "href": "blog/tremendous-tricks/index.html#scale_xy_log10",
    "title": "10 Tremendous Tricks of the Siubaverse",
    "section": "scale_x/y_log10()",
    "text": "scale_x/y_log10()\n# y-axis co2 omissions\n# x-axis gdp"
  },
  {
    "objectID": "blog/tremendous-tricks/index.html#crossing",
    "href": "blog/tremendous-tricks/index.html#crossing",
    "title": "10 Tremendous Tricks of the Siubaverse",
    "section": "crossing()",
    "text": "crossing()\n# tougher to explain"
  },
  {
    "objectID": "blog/tremendous-tricks/index.html#separate",
    "href": "blog/tremendous-tricks/index.html#separate",
    "title": "10 Tremendous Tricks of the Siubaverse",
    "section": "separate()",
    "text": "separate()\n# find something with a funky separator"
  },
  {
    "objectID": "blog/tremendous-tricks/index.html#extract",
    "href": "blog/tremendous-tricks/index.html#extract",
    "title": "10 Tremendous Tricks of the Siubaverse",
    "section": "extract()",
    "text": "extract()\n# bob ross data extract season and episode"
  }
]