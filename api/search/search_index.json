{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Function reference Core verbs - one table arrange Sort rows based on one or more columns. count Count observations by group. distinct Count observations by group. filter Keep rows that match condition. head Keep the first n rows of data. mutate , transmute Create or replace columns. rename Rename columns. select Keep, drop, or rename specific columns. summarize Calculate a single number per grouping. group_by , ungroup Specify groups for splitting rows of data. Core verbs - two table inner_join, left_join, right_join, full_join Mutating joins semi_join , anti_join Filtering joins Query verbs collect Retrieve data into a DataFrame. show_query Print the query being generated. Tidy verbs complete Add rows for missing combinations in the data. extract Add new columns by matching a pattern on a column of strings. gather , spread Gather columns in to long format. Spread out to wide format. separate , unite Add new columns by splitting a character column. nest , unnest Create a column where each entry is a DataFrame. Column Operations Forcats fct_collapse Rename categories. Optionally group all others. fct_infreq Order categories by frequency (largest first) fct_inorder Order categories by when they first appear. fct_lump Lump infrequently observed categories together. fct_recode Rename categories. fct_reorder Reordered categories, using a calculation over another column. fct_rev Reverse category levels. Datetime floor_date, ceil_date Round datetimes down or up to a specific granularity (e.g. week). Vector between() Check whether values are in a specified range. case_when() , if_else() Generalized if statements. coalesce() Use first non-missing element across columns. cumall() , cumany() , cummean() Cumulative all, any, and mean. lag() , lead() Shift values later (lag) or earlier (lead) in time. n() Calculate the number of observations in a vector. n_distinct() Count the number of unique values. na_if() Convert a value to NA. near() Check whether every pair of values in two vectors are close. nth() , first() , last() Return the first, last, or nth value. row_number() , ntile() , min_rank() , dense_rank() , percent_rank() , cume_dist() Windowed rank functions.","title":"Overview"},{"location":"#function-reference","text":"","title":"Function reference"},{"location":"#core-verbs-one-table","text":"arrange Sort rows based on one or more columns. count Count observations by group. distinct Count observations by group. filter Keep rows that match condition. head Keep the first n rows of data. mutate , transmute Create or replace columns. rename Rename columns. select Keep, drop, or rename specific columns. summarize Calculate a single number per grouping. group_by , ungroup Specify groups for splitting rows of data.","title":"Core verbs - one table"},{"location":"#core-verbs-two-table","text":"inner_join, left_join, right_join, full_join Mutating joins semi_join , anti_join Filtering joins","title":"Core verbs - two table"},{"location":"#query-verbs","text":"collect Retrieve data into a DataFrame. show_query Print the query being generated.","title":"Query verbs"},{"location":"#tidy-verbs","text":"complete Add rows for missing combinations in the data. extract Add new columns by matching a pattern on a column of strings. gather , spread Gather columns in to long format. Spread out to wide format. separate , unite Add new columns by splitting a character column. nest , unnest Create a column where each entry is a DataFrame.","title":"Tidy verbs"},{"location":"#column-operations","text":"","title":"Column Operations"},{"location":"#forcats","text":"fct_collapse Rename categories. Optionally group all others. fct_infreq Order categories by frequency (largest first) fct_inorder Order categories by when they first appear. fct_lump Lump infrequently observed categories together. fct_recode Rename categories. fct_reorder Reordered categories, using a calculation over another column. fct_rev Reverse category levels.","title":"Forcats"},{"location":"#datetime","text":"floor_date, ceil_date Round datetimes down or up to a specific granularity (e.g. week).","title":"Datetime"},{"location":"#vector","text":"between() Check whether values are in a specified range. case_when() , if_else() Generalized if statements. coalesce() Use first non-missing element across columns. cumall() , cumany() , cummean() Cumulative all, any, and mean. lag() , lead() Shift values later (lag) or earlier (lead) in time. n() Calculate the number of observations in a vector. n_distinct() Count the number of unique values. na_if() Convert a value to NA. near() Check whether every pair of values in two vectors are close. nth() , first() , last() Return the first, last, or nth value. row_number() , ntile() , min_rank() , dense_rank() , percent_rank() , cume_dist() Windowed rank functions.","title":"Vector"},{"location":"experimental/","text":"floor_date(), ceil_date() from siuba.experimental.datetime import floor_date, ceil_date floor_date ( x , unit = 'S' ) floor_date and ceil_date return dates rounded to nearest specified unit. These functions address limitations in pandas' round() method, which cannot round down to, e.g., the month. Parameters: Name Type Description Default x a DatetimeIndex, PeriodIndex, or their underlying arrays or elements. required units a date or time unit for rounding (eg. \"MS\" rounds down or up to the start of a month) required Examples: >>> import pandas as pd >>> a_date = \"2020-02-02 02:02:02\" >>> dti = pd . DatetimeIndex ([ a_date ]) >>> dti . floor ( \"MS\" ) Traceback ( most recent call last ): ... ValueError : < MonthBegin > is a non - fixed frequency Month start will always go to the first day of a month. >>> floor_date ( dti , \"MS\" ) DatetimeIndex ([ '2020-02-01' ], dtype = 'datetime64[ns]' , freq = None ) >>> ceil_date ( dti , \"MS\" ) DatetimeIndex ([ '2020-03-01' ], dtype = 'datetime64[ns]' , freq = None ) On the other hand, here is month end. >>> floor_date ( dti , \"M\" ) DatetimeIndex ([ '2020-01-31' ], dtype = 'datetime64[ns]' , freq = None ) >>> ceil_date ( dti , \"M\" ) DatetimeIndex ([ '2020-02-29' ], dtype = 'datetime64[ns]' , freq = None ) It also works on things supported by the Series.dt.floor method, like hours. >>> floor_date ( dti , \"H\" ) DatetimeIndex ([ '2020-02-02 02:00:00' ], dtype = 'datetime64[ns]' , freq = None ) You can also use it on other types, like a PeriodIndex >>> per = pd . PeriodIndex ([ a_date ], freq = \"S\" ) >>> floor_date ( per , \"M\" ) PeriodIndex ([ '2020-02' ], dtype = 'period[M]' ... Source code in siuba/experimental/datetime.py @symbolic_dispatch def floor_date ( x , unit = \"S\" ): raise TypeError ( \"floor_date not implemented for class {} \" . format ( type ( x )))","title":"Datetime (experimental)"},{"location":"experimental/#floor_date-ceil_date","text":"from siuba.experimental.datetime import floor_date, ceil_date","title":"floor_date(), ceil_date()"},{"location":"experimental/#siuba.experimental.datetime.floor_date","text":"floor_date and ceil_date return dates rounded to nearest specified unit. These functions address limitations in pandas' round() method, which cannot round down to, e.g., the month. Parameters: Name Type Description Default x a DatetimeIndex, PeriodIndex, or their underlying arrays or elements. required units a date or time unit for rounding (eg. \"MS\" rounds down or up to the start of a month) required Examples: >>> import pandas as pd >>> a_date = \"2020-02-02 02:02:02\" >>> dti = pd . DatetimeIndex ([ a_date ]) >>> dti . floor ( \"MS\" ) Traceback ( most recent call last ): ... ValueError : < MonthBegin > is a non - fixed frequency Month start will always go to the first day of a month. >>> floor_date ( dti , \"MS\" ) DatetimeIndex ([ '2020-02-01' ], dtype = 'datetime64[ns]' , freq = None ) >>> ceil_date ( dti , \"MS\" ) DatetimeIndex ([ '2020-03-01' ], dtype = 'datetime64[ns]' , freq = None ) On the other hand, here is month end. >>> floor_date ( dti , \"M\" ) DatetimeIndex ([ '2020-01-31' ], dtype = 'datetime64[ns]' , freq = None ) >>> ceil_date ( dti , \"M\" ) DatetimeIndex ([ '2020-02-29' ], dtype = 'datetime64[ns]' , freq = None ) It also works on things supported by the Series.dt.floor method, like hours. >>> floor_date ( dti , \"H\" ) DatetimeIndex ([ '2020-02-02 02:00:00' ], dtype = 'datetime64[ns]' , freq = None ) You can also use it on other types, like a PeriodIndex >>> per = pd . PeriodIndex ([ a_date ], freq = \"S\" ) >>> floor_date ( per , \"M\" ) PeriodIndex ([ '2020-02' ], dtype = 'period[M]' ... Source code in siuba/experimental/datetime.py @symbolic_dispatch def floor_date ( x , unit = \"S\" ): raise TypeError ( \"floor_date not implemented for class {} \" . format ( type ( x )))","title":"floor_date()"},{"location":"forcats/","text":"Importing from siuba.dply.forcats import fct_collapse, fct_infreq siuba.dply.forcats fct_collapse ( fct , recat , group_other = None ) Return copy of fct with categories renamed. Optionally group all others. Parameters: Name Type Description Default fct A pandas.Categorical, or array(-like) used to create one. required recat Dictionary of form {new_cat_name: old_cat_name}. old_cat_name may be a list of existing categories, to be given the same name. required group_other An optional string, specifying what all other categories should be named. This will always be the last category level in the result. None Examples: >>> fct_collapse ([ 'a' , 'b' , 'c' ], { 'x' : 'a' }) [ 'x' , 'b' , 'c' ] Categories ( 3 , object ): [ 'x' , 'b' , 'c' ] >>> fct_collapse ([ 'a' , 'b' , 'c' ], { 'x' : 'a' }, group_other = 'others' ) [ 'x' , 'others' , 'others' ] Categories ( 2 , object ): [ 'x' , 'others' ] >>> fct_collapse ([ 'a' , 'b' , 'c' ], { 'ab' : [ 'a' , 'b' ]}) [ 'ab' , 'ab' , 'c' ] Categories ( 2 , object ): [ 'ab' , 'c' ] >>> fct_collapse ([ 'a' , 'b' , None ], { 'a' : [ 'b' ]}) [ 'a' , 'a' , NaN ] Categories ( 1 , object ): [ 'a' ] Source code in siuba/dply/forcats.py @symbolic_dispatch def fct_collapse ( fct , recat , group_other = None ) -> pd . Categorical : \"\"\"Return copy of fct with categories renamed. Optionally group all others. Parameters ---------- fct : A pandas.Categorical, or array(-like) used to create one. recat : Dictionary of form {new_cat_name: old_cat_name}. old_cat_name may be a list of existing categories, to be given the same name. group_other : An optional string, specifying what all other categories should be named. This will always be the last category level in the result. Notes ----- Resulting levels index is ordered according to the earliest level replaced. If we rename the first and last levels to \"c\", then \"c\" is the first level. Examples -------- >>> fct_collapse(['a', 'b', 'c'], {'x': 'a'}) ['x', 'b', 'c'] Categories (3, object): ['x', 'b', 'c'] >>> fct_collapse(['a', 'b', 'c'], {'x': 'a'}, group_other = 'others') ['x', 'others', 'others'] Categories (2, object): ['x', 'others'] >>> fct_collapse(['a', 'b', 'c'], {'ab': ['a', 'b']}) ['ab', 'ab', 'c'] Categories (2, object): ['ab', 'c'] >>> fct_collapse(['a', 'b', None], {'a': ['b']}) ['a', 'a', NaN] Categories (1, object): ['a'] \"\"\" if not isinstance ( fct , pd . Categorical ): new_fct = pd . Categorical ( fct ) else : new_fct = fct # each existing cat will map to a new one ---- # need to know existing to new cat # need to know new cat to new code cat_to_new = { k : None for k in new_fct . categories } for new_name , v in recat . items (): v = [ v ] if not np . ndim ( v ) else v for old_name in v : if cat_to_new [ old_name ] is not None : raise Exception ( \"category %s was already re-assigned\" % old_name ) cat_to_new [ old_name ] = new_name # collapse all unspecified cats to group_other if specified ---- for k , v in cat_to_new . items (): if v is None : if group_other is not None : cat_to_new [ k ] = group_other else : cat_to_new [ k ] = k # map from old cat to new code ---- # calculate new codes ordered_cats = { new : True for old , new in cat_to_new . items ()} # move the other group to last in the ordered set if group_other is not None : try : del ordered_cats [ group_other ] ordered_cats [ group_other ] = True except KeyError : pass # map new category name to code new_cat_set = { k : ii for ii , k in enumerate ( ordered_cats )} # at this point, we need remap codes to the other category # make an array, where the index is old code + 1 (so missing val index is 0) old_code_to_new = np . array ( [ - 1 ] + [ new_cat_set [ new_cat ] for new_cat in cat_to_new . values ()] ) # map old cats to new codes #remap_code = {old: new_cat_set[new] for old, new in cat_to_new.items()} new_codes = old_code_to_new [ new_fct . codes + 1 ] new_cats = list ( new_cat_set ) out = pd . Categorical . from_codes ( new_codes , new_cats ) return _maybe_upcast ( fct , out ) fct_infreq ( fct , ordered = None ) Return a copy of fct, with categories ordered by frequency (largest first) Parameters: Name Type Description Default fct list-like A pandas Series, Categorical, or list-like object required ordered bool Whether to return an ordered categorical. By default a Categorical inputs' ordered setting is respected. Use this to override it. None Examples: >>> fct_infreq ([ \"c\" , \"a\" , \"c\" , \"c\" , \"a\" , \"b\" ]) [ 'c' , 'a' , 'c' , 'c' , 'a' , 'b' ] Categories ( 3 , object ): [ 'c' , 'a' , 'b' ] Source code in siuba/dply/forcats.py @symbolic_dispatch def fct_infreq ( fct , ordered = None ): \"\"\"Return a copy of fct, with categories ordered by frequency (largest first) Parameters ---------- fct : list-like A pandas Series, Categorical, or list-like object ordered : bool Whether to return an ordered categorical. By default a Categorical inputs' ordered setting is respected. Use this to override it. See Also -------- fct_inorder : Order categories by when they're first observed. Examples -------- >>> fct_infreq([\"c\", \"a\", \"c\", \"c\", \"a\", \"b\"]) ['c', 'a', 'c', 'c', 'a', 'b'] Categories (3, object): ['c', 'a', 'b'] \"\"\" if ordered is None : ordered = _get_cat_order ( fct ) # sort and create new categorical ---- if isinstance ( fct , pd . Categorical ): # Categorical value counts are sorted in categories order # So to acheive the exact same result as the Series case below, # we need to use fct_inorder, so categories is in first-observed order. # This orders the final result by frequency, and then observed for ties. freq = fct_inorder ( fct ) . value_counts () . sort_values ( ascending = False ) # note that freq is a Series, but it has a CategoricalIndex. # we want the index values as shown, so we need to strip them out of # this nightmare index situation. categories = freq . index . categories [ freq . index . dropna () . codes ] return pd . Categorical ( fct , categories = categories , ordered = ordered ) else : # Series sorts in descending frequency order ser = pd . Series ( fct ) if not isinstance ( fct , pd . Series ) else fct freq = ser . value_counts () cat = pd . Categorical ( ser , categories = freq . index , ordered = ordered ) if isinstance ( fct , pd . Series ): return pd . Series ( cat ) return cat fct_inorder ( fct , ordered = None ) Return a copy of fct, with categories ordered by when they first appear. Parameters: Name Type Description Default fct list-like A pandas Series, Categorical, or list-like object required ordered bool Whether to return an ordered categorical. By default a Categorical inputs' ordered setting is respected. Use this to override it. None Examples: >>> fct = pd . Categorical ([ \"c\" , \"a\" , \"b\" ]) >>> fct [ 'c' , 'a' , 'b' ] Categories ( 3 , object ): [ 'a' , 'b' , 'c' ] Note that above the categories are sorted alphabetically. Use fct_inorder to keep the categories in first-observed order. >>> fct_inorder ( fct ) [ 'c' , 'a' , 'b' ] Categories ( 3 , object ): [ 'c' , 'a' , 'b' ] fct_inorder also accepts pd.Series and list objects: >>> fct_inorder ([ \"z\" , \"a\" ]) [ 'z' , 'a' ] Categories ( 2 , object ): [ 'z' , 'a' ] By default, the ordered setting of categoricals is respected. Use the ordered parameter to override it. >>> fct2 = pd . Categorical ([ \"z\" , \"a\" , \"b\" ], ordered = True ) >>> fct_inorder ( fct2 ) [ 'z' , 'a' , 'b' ] Categories ( 3 , object ): [ 'z' < 'a' < 'b' ] >>> fct_inorder ( fct2 , ordered = False ) [ 'z' , 'a' , 'b' ] Categories ( 3 , object ): [ 'z' , 'a' , 'b' ] Source code in siuba/dply/forcats.py @symbolic_dispatch def fct_inorder ( fct , ordered = None ): \"\"\"Return a copy of fct, with categories ordered by when they first appear. Parameters ---------- fct : list-like A pandas Series, Categorical, or list-like object ordered : bool Whether to return an ordered categorical. By default a Categorical inputs' ordered setting is respected. Use this to override it. See Also -------- fct_infreq : Order categories by value frequency count. Examples -------- >>> fct = pd.Categorical([\"c\", \"a\", \"b\"]) >>> fct ['c', 'a', 'b'] Categories (3, object): ['a', 'b', 'c'] Note that above the categories are sorted alphabetically. Use fct_inorder to keep the categories in first-observed order. >>> fct_inorder(fct) ['c', 'a', 'b'] Categories (3, object): ['c', 'a', 'b'] fct_inorder also accepts pd.Series and list objects: >>> fct_inorder([\"z\", \"a\"]) ['z', 'a'] Categories (2, object): ['z', 'a'] By default, the ordered setting of categoricals is respected. Use the ordered parameter to override it. >>> fct2 = pd.Categorical([\"z\", \"a\", \"b\"], ordered=True) >>> fct_inorder(fct2) ['z', 'a', 'b'] Categories (3, object): ['z' < 'a' < 'b'] >>> fct_inorder(fct2, ordered=False) ['z', 'a', 'b'] Categories (3, object): ['z', 'a', 'b'] \"\"\" if ordered is None : ordered = _get_cat_order ( fct ) if isinstance ( fct , ( pd . Series , pd . Categorical )): uniq = fct . dropna () . unique () if isinstance ( uniq , pd . Categorical ): # the result of .unique for a categorical is a new categorical # unsurprisingly, it also sorts the categories, so reorder manually # (note that this also applies to Series[Categorical].unique()) categories = uniq . categories [ uniq . dropna () . codes ] return pd . Categorical ( fct , categories , ordered = ordered ) # series in, so series out cat = pd . Categorical ( fct , uniq , ordered = ordered ) return pd . Series ( cat ) ser = pd . Series ( fct ) return pd . Categorical ( fct , categories = ser . dropna () . unique (), ordered = ordered ) fct_lump ( fct , n = None , prop = None , w = None , other_level = 'Other' , ties = None ) Return a copy of fct with categories lumped together. Parameters: Name Type Description Default fct A pandas.Categorical, or array(-like) used to create one. required n Number of categories to keep. None prop (not implemented) keep categories that occur prop proportion of the time. None w Array of weights corresponding to each value in fct. None other_level Name for all lumped together levels. 'Other' ties (not implemented) method to use in the case of ties. None Examples: >>> fct_lump ([ 'a' , 'a' , 'b' , 'c' ], n = 1 ) [ 'a' , 'a' , 'Other' , 'Other' ] Categories ( 2 , object ): [ 'a' , 'Other' ] >>> fct_lump ([ 'a' , 'a' , 'b' , 'b' , 'c' , 'd' ], prop = .2 ) [ 'a' , 'a' , 'b' , 'b' , 'Other' , 'Other' ] Categories ( 3 , object ): [ 'a' , 'b' , 'Other' ] Source code in siuba/dply/forcats.py @symbolic_dispatch def fct_lump ( fct , n = None , prop = None , w = None , other_level = \"Other\" , ties = None ) -> pd . Categorical : \"\"\"Return a copy of fct with categories lumped together. Parameters ---------- fct : A pandas.Categorical, or array(-like) used to create one. n : Number of categories to keep. prop : (not implemented) keep categories that occur prop proportion of the time. w : Array of weights corresponding to each value in fct. other_level : Name for all lumped together levels. ties : (not implemented) method to use in the case of ties. Notes ----- Currently, one of n and prop must be specified. Examples -------- >>> fct_lump(['a', 'a', 'b', 'c'], n = 1) ['a', 'a', 'Other', 'Other'] Categories (2, object): ['a', 'Other'] >>> fct_lump(['a', 'a', 'b', 'b', 'c', 'd'], prop = .2) ['a', 'a', 'b', 'b', 'Other', 'Other'] Categories (3, object): ['a', 'b', 'Other'] \"\"\" if ties is not None : raise NotImplementedError ( \"ties is not implemented\" ) if n is None and prop is None : raise NotImplementedError ( \"Either n or prop must be specified\" ) keep_cats = _fct_lump_n_cats ( fct , w , other_level , ties , n = n , prop = prop ) out = fct_collapse ( fct , { k : k for k in keep_cats }, group_other = other_level ) return _maybe_upcast ( fct , out ) fct_recode ( fct , recat = None , ** kwargs ) Return copy of fct with renamed categories. Parameters: Name Type Description Default fct A pandas.Categorical, or array(-like) used to create one. required **kwargs Arguments of form new_name = old_name. {} Examples: >>> cat = [ 'a' , 'b' , 'c' ] >>> fct_recode ( cat , z = 'c' ) [ 'a' , 'b' , 'z' ] Categories ( 3 , object ): [ 'a' , 'b' , 'z' ] >>> fct_recode ( cat , x = [ 'a' , 'b' ]) [ 'x' , 'x' , 'c' ] Categories ( 2 , object ): [ 'x' , 'c' ] >>> fct_recode ( cat , { \"x\" : [ 'a' , 'b' ]}) [ 'x' , 'x' , 'c' ] Categories ( 2 , object ): [ 'x' , 'c' ] Source code in siuba/dply/forcats.py @symbolic_dispatch def fct_recode ( fct , recat = None , ** kwargs ) -> pd . Categorical : \"\"\"Return copy of fct with renamed categories. Parameters ---------- fct : A pandas.Categorical, or array(-like) used to create one. **kwargs : Arguments of form new_name = old_name. Examples -------- >>> cat = ['a', 'b', 'c'] >>> fct_recode(cat, z = 'c') ['a', 'b', 'z'] Categories (3, object): ['a', 'b', 'z'] >>> fct_recode(cat, x = ['a', 'b']) ['x', 'x', 'c'] Categories (2, object): ['x', 'c'] >>> fct_recode(cat, {\"x\": ['a', 'b']}) ['x', 'x', 'c'] Categories (2, object): ['x', 'c'] \"\"\" if recat and not isinstance ( recat , dict ): raise TypeError ( \"fct_recode requires named args or a dict.\" ) if recat and kwargs : duplicate_keys = set ( recat ) . intersection ( set ( kwargs )) if duplicate_keys : raise ValueError ( \"The following recode name(s) were specified more than once: {} \" \\ . format ( duplicate_keys ) ) new_cats = { ** recat , ** kwargs } if recat else kwargs return _maybe_upcast ( fct , fct_collapse ( fct , new_cats )) fct_reorder ( fct , x , func =< function median at 0x7f931ea6dea0 > , desc = False ) Return copy of fct, with categories reordered according to values in x. Parameters: Name Type Description Default fct A pandas.Categorical, or array(-like) used to create one. required x Values used to reorder categorical. Must be same length as fct. required func Function run over all values within a level of the categorical. <function median at 0x7f931ea6dea0> desc Whether to sort in descending order. False Examples: >>> fct_reorder ([ 'a' , 'a' , 'b' ], [ 4 , 3 , 2 ]) [ 'a' , 'a' , 'b' ] Categories ( 2 , object ): [ 'b' , 'a' ] >>> fct_reorder ([ 'a' , 'a' , 'b' ], [ 4 , 3 , 2 ], desc = True ) [ 'a' , 'a' , 'b' ] Categories ( 2 , object ): [ 'a' , 'b' ] >>> fct_reorder ([ 'x' , 'x' , 'y' ], [ 4 , 0 , 2 ], np . max ) [ 'x' , 'x' , 'y' ] Categories ( 2 , object ): [ 'y' , 'x' ] Source code in siuba/dply/forcats.py @symbolic_dispatch def fct_reorder ( fct , x , func = np . median , desc = False ) -> pd . Categorical : \"\"\"Return copy of fct, with categories reordered according to values in x. Parameters ---------- fct : A pandas.Categorical, or array(-like) used to create one. x : Values used to reorder categorical. Must be same length as fct. func : Function run over all values within a level of the categorical. desc : Whether to sort in descending order. Notes ----- NaN categories can't be ordered. When func returns NaN, sorting is always done with NaNs last. Examples -------- >>> fct_reorder(['a', 'a', 'b'], [4, 3, 2]) ['a', 'a', 'b'] Categories (2, object): ['b', 'a'] >>> fct_reorder(['a', 'a', 'b'], [4, 3, 2], desc = True) ['a', 'a', 'b'] Categories (2, object): ['a', 'b'] >>> fct_reorder(['x', 'x', 'y'], [4, 0, 2], np.max) ['x', 'x', 'y'] Categories (2, object): ['y', 'x'] \"\"\" x_vals = x . values if isinstance ( x , pd . Series ) else x s = pd . Series ( x_vals , index = fct ) # sort groups by calculated agg func. note that groupby uses dropna=True by default, # but that's okay, since pandas categoricals can't order the NA category ordered = s . groupby ( level = 0 ) . agg ( func ) . sort_values ( ascending = not desc ) out = pd . Categorical ( fct , categories = ordered . index ) return _maybe_upcast ( fct , out ) fct_rev ( fct ) Return a copy of fct with category level order reversed.next Parameters: Name Type Description Default fct A pandas.Categorical, or array(-like) used to create one. required Examples: >>> fct = pd . Categorical ([ \"a\" , \"b\" , \"c\" ]) >>> fct [ 'a' , 'b' , 'c' ] Categories ( 3 , object ): [ 'a' , 'b' , 'c' ] >>> fct_rev ( fct ) [ 'a' , 'b' , 'c' ] Categories ( 3 , object ): [ 'c' , 'b' , 'a' ] Note that this function can also accept a list. >>> fct_rev ([ \"a\" , \"b\" , \"c\" ]) [ 'a' , 'b' , 'c' ] Categories ( 3 , object ): [ 'c' , 'b' , 'a' ] Source code in siuba/dply/forcats.py @symbolic_dispatch def fct_rev ( fct ) -> pd . Categorical : \"\"\"Return a copy of fct with category level order reversed.next Parameters ---------- fct : A pandas.Categorical, or array(-like) used to create one. Examples -------- >>> fct = pd.Categorical([\"a\", \"b\", \"c\"]) >>> fct ['a', 'b', 'c'] Categories (3, object): ['a', 'b', 'c'] >>> fct_rev(fct) ['a', 'b', 'c'] Categories (3, object): ['c', 'b', 'a'] Note that this function can also accept a list. >>> fct_rev([\"a\", \"b\", \"c\"]) ['a', 'b', 'c'] Categories (3, object): ['c', 'b', 'a'] \"\"\" if not isinstance ( fct , pd . Categorical ): fct = pd . Categorical ( fct ) rev_levels = list ( reversed ( fct . categories )) out = fct . reorder_categories ( rev_levels ) return _maybe_upcast ( fct , out )","title":"Forcats"},{"location":"forcats/#importing","text":"from siuba.dply.forcats import fct_collapse, fct_infreq","title":"Importing"},{"location":"forcats/#siuba.dply.forcats","text":"","title":"forcats"},{"location":"forcats/#siuba.dply.forcats.fct_collapse","text":"Return copy of fct with categories renamed. Optionally group all others. Parameters: Name Type Description Default fct A pandas.Categorical, or array(-like) used to create one. required recat Dictionary of form {new_cat_name: old_cat_name}. old_cat_name may be a list of existing categories, to be given the same name. required group_other An optional string, specifying what all other categories should be named. This will always be the last category level in the result. None Examples: >>> fct_collapse ([ 'a' , 'b' , 'c' ], { 'x' : 'a' }) [ 'x' , 'b' , 'c' ] Categories ( 3 , object ): [ 'x' , 'b' , 'c' ] >>> fct_collapse ([ 'a' , 'b' , 'c' ], { 'x' : 'a' }, group_other = 'others' ) [ 'x' , 'others' , 'others' ] Categories ( 2 , object ): [ 'x' , 'others' ] >>> fct_collapse ([ 'a' , 'b' , 'c' ], { 'ab' : [ 'a' , 'b' ]}) [ 'ab' , 'ab' , 'c' ] Categories ( 2 , object ): [ 'ab' , 'c' ] >>> fct_collapse ([ 'a' , 'b' , None ], { 'a' : [ 'b' ]}) [ 'a' , 'a' , NaN ] Categories ( 1 , object ): [ 'a' ] Source code in siuba/dply/forcats.py @symbolic_dispatch def fct_collapse ( fct , recat , group_other = None ) -> pd . Categorical : \"\"\"Return copy of fct with categories renamed. Optionally group all others. Parameters ---------- fct : A pandas.Categorical, or array(-like) used to create one. recat : Dictionary of form {new_cat_name: old_cat_name}. old_cat_name may be a list of existing categories, to be given the same name. group_other : An optional string, specifying what all other categories should be named. This will always be the last category level in the result. Notes ----- Resulting levels index is ordered according to the earliest level replaced. If we rename the first and last levels to \"c\", then \"c\" is the first level. Examples -------- >>> fct_collapse(['a', 'b', 'c'], {'x': 'a'}) ['x', 'b', 'c'] Categories (3, object): ['x', 'b', 'c'] >>> fct_collapse(['a', 'b', 'c'], {'x': 'a'}, group_other = 'others') ['x', 'others', 'others'] Categories (2, object): ['x', 'others'] >>> fct_collapse(['a', 'b', 'c'], {'ab': ['a', 'b']}) ['ab', 'ab', 'c'] Categories (2, object): ['ab', 'c'] >>> fct_collapse(['a', 'b', None], {'a': ['b']}) ['a', 'a', NaN] Categories (1, object): ['a'] \"\"\" if not isinstance ( fct , pd . Categorical ): new_fct = pd . Categorical ( fct ) else : new_fct = fct # each existing cat will map to a new one ---- # need to know existing to new cat # need to know new cat to new code cat_to_new = { k : None for k in new_fct . categories } for new_name , v in recat . items (): v = [ v ] if not np . ndim ( v ) else v for old_name in v : if cat_to_new [ old_name ] is not None : raise Exception ( \"category %s was already re-assigned\" % old_name ) cat_to_new [ old_name ] = new_name # collapse all unspecified cats to group_other if specified ---- for k , v in cat_to_new . items (): if v is None : if group_other is not None : cat_to_new [ k ] = group_other else : cat_to_new [ k ] = k # map from old cat to new code ---- # calculate new codes ordered_cats = { new : True for old , new in cat_to_new . items ()} # move the other group to last in the ordered set if group_other is not None : try : del ordered_cats [ group_other ] ordered_cats [ group_other ] = True except KeyError : pass # map new category name to code new_cat_set = { k : ii for ii , k in enumerate ( ordered_cats )} # at this point, we need remap codes to the other category # make an array, where the index is old code + 1 (so missing val index is 0) old_code_to_new = np . array ( [ - 1 ] + [ new_cat_set [ new_cat ] for new_cat in cat_to_new . values ()] ) # map old cats to new codes #remap_code = {old: new_cat_set[new] for old, new in cat_to_new.items()} new_codes = old_code_to_new [ new_fct . codes + 1 ] new_cats = list ( new_cat_set ) out = pd . Categorical . from_codes ( new_codes , new_cats ) return _maybe_upcast ( fct , out )","title":"fct_collapse()"},{"location":"forcats/#siuba.dply.forcats.fct_infreq","text":"Return a copy of fct, with categories ordered by frequency (largest first) Parameters: Name Type Description Default fct list-like A pandas Series, Categorical, or list-like object required ordered bool Whether to return an ordered categorical. By default a Categorical inputs' ordered setting is respected. Use this to override it. None Examples: >>> fct_infreq ([ \"c\" , \"a\" , \"c\" , \"c\" , \"a\" , \"b\" ]) [ 'c' , 'a' , 'c' , 'c' , 'a' , 'b' ] Categories ( 3 , object ): [ 'c' , 'a' , 'b' ] Source code in siuba/dply/forcats.py @symbolic_dispatch def fct_infreq ( fct , ordered = None ): \"\"\"Return a copy of fct, with categories ordered by frequency (largest first) Parameters ---------- fct : list-like A pandas Series, Categorical, or list-like object ordered : bool Whether to return an ordered categorical. By default a Categorical inputs' ordered setting is respected. Use this to override it. See Also -------- fct_inorder : Order categories by when they're first observed. Examples -------- >>> fct_infreq([\"c\", \"a\", \"c\", \"c\", \"a\", \"b\"]) ['c', 'a', 'c', 'c', 'a', 'b'] Categories (3, object): ['c', 'a', 'b'] \"\"\" if ordered is None : ordered = _get_cat_order ( fct ) # sort and create new categorical ---- if isinstance ( fct , pd . Categorical ): # Categorical value counts are sorted in categories order # So to acheive the exact same result as the Series case below, # we need to use fct_inorder, so categories is in first-observed order. # This orders the final result by frequency, and then observed for ties. freq = fct_inorder ( fct ) . value_counts () . sort_values ( ascending = False ) # note that freq is a Series, but it has a CategoricalIndex. # we want the index values as shown, so we need to strip them out of # this nightmare index situation. categories = freq . index . categories [ freq . index . dropna () . codes ] return pd . Categorical ( fct , categories = categories , ordered = ordered ) else : # Series sorts in descending frequency order ser = pd . Series ( fct ) if not isinstance ( fct , pd . Series ) else fct freq = ser . value_counts () cat = pd . Categorical ( ser , categories = freq . index , ordered = ordered ) if isinstance ( fct , pd . Series ): return pd . Series ( cat ) return cat","title":"fct_infreq()"},{"location":"forcats/#siuba.dply.forcats.fct_inorder","text":"Return a copy of fct, with categories ordered by when they first appear. Parameters: Name Type Description Default fct list-like A pandas Series, Categorical, or list-like object required ordered bool Whether to return an ordered categorical. By default a Categorical inputs' ordered setting is respected. Use this to override it. None Examples: >>> fct = pd . Categorical ([ \"c\" , \"a\" , \"b\" ]) >>> fct [ 'c' , 'a' , 'b' ] Categories ( 3 , object ): [ 'a' , 'b' , 'c' ] Note that above the categories are sorted alphabetically. Use fct_inorder to keep the categories in first-observed order. >>> fct_inorder ( fct ) [ 'c' , 'a' , 'b' ] Categories ( 3 , object ): [ 'c' , 'a' , 'b' ] fct_inorder also accepts pd.Series and list objects: >>> fct_inorder ([ \"z\" , \"a\" ]) [ 'z' , 'a' ] Categories ( 2 , object ): [ 'z' , 'a' ] By default, the ordered setting of categoricals is respected. Use the ordered parameter to override it. >>> fct2 = pd . Categorical ([ \"z\" , \"a\" , \"b\" ], ordered = True ) >>> fct_inorder ( fct2 ) [ 'z' , 'a' , 'b' ] Categories ( 3 , object ): [ 'z' < 'a' < 'b' ] >>> fct_inorder ( fct2 , ordered = False ) [ 'z' , 'a' , 'b' ] Categories ( 3 , object ): [ 'z' , 'a' , 'b' ] Source code in siuba/dply/forcats.py @symbolic_dispatch def fct_inorder ( fct , ordered = None ): \"\"\"Return a copy of fct, with categories ordered by when they first appear. Parameters ---------- fct : list-like A pandas Series, Categorical, or list-like object ordered : bool Whether to return an ordered categorical. By default a Categorical inputs' ordered setting is respected. Use this to override it. See Also -------- fct_infreq : Order categories by value frequency count. Examples -------- >>> fct = pd.Categorical([\"c\", \"a\", \"b\"]) >>> fct ['c', 'a', 'b'] Categories (3, object): ['a', 'b', 'c'] Note that above the categories are sorted alphabetically. Use fct_inorder to keep the categories in first-observed order. >>> fct_inorder(fct) ['c', 'a', 'b'] Categories (3, object): ['c', 'a', 'b'] fct_inorder also accepts pd.Series and list objects: >>> fct_inorder([\"z\", \"a\"]) ['z', 'a'] Categories (2, object): ['z', 'a'] By default, the ordered setting of categoricals is respected. Use the ordered parameter to override it. >>> fct2 = pd.Categorical([\"z\", \"a\", \"b\"], ordered=True) >>> fct_inorder(fct2) ['z', 'a', 'b'] Categories (3, object): ['z' < 'a' < 'b'] >>> fct_inorder(fct2, ordered=False) ['z', 'a', 'b'] Categories (3, object): ['z', 'a', 'b'] \"\"\" if ordered is None : ordered = _get_cat_order ( fct ) if isinstance ( fct , ( pd . Series , pd . Categorical )): uniq = fct . dropna () . unique () if isinstance ( uniq , pd . Categorical ): # the result of .unique for a categorical is a new categorical # unsurprisingly, it also sorts the categories, so reorder manually # (note that this also applies to Series[Categorical].unique()) categories = uniq . categories [ uniq . dropna () . codes ] return pd . Categorical ( fct , categories , ordered = ordered ) # series in, so series out cat = pd . Categorical ( fct , uniq , ordered = ordered ) return pd . Series ( cat ) ser = pd . Series ( fct ) return pd . Categorical ( fct , categories = ser . dropna () . unique (), ordered = ordered )","title":"fct_inorder()"},{"location":"forcats/#siuba.dply.forcats.fct_lump","text":"Return a copy of fct with categories lumped together. Parameters: Name Type Description Default fct A pandas.Categorical, or array(-like) used to create one. required n Number of categories to keep. None prop (not implemented) keep categories that occur prop proportion of the time. None w Array of weights corresponding to each value in fct. None other_level Name for all lumped together levels. 'Other' ties (not implemented) method to use in the case of ties. None Examples: >>> fct_lump ([ 'a' , 'a' , 'b' , 'c' ], n = 1 ) [ 'a' , 'a' , 'Other' , 'Other' ] Categories ( 2 , object ): [ 'a' , 'Other' ] >>> fct_lump ([ 'a' , 'a' , 'b' , 'b' , 'c' , 'd' ], prop = .2 ) [ 'a' , 'a' , 'b' , 'b' , 'Other' , 'Other' ] Categories ( 3 , object ): [ 'a' , 'b' , 'Other' ] Source code in siuba/dply/forcats.py @symbolic_dispatch def fct_lump ( fct , n = None , prop = None , w = None , other_level = \"Other\" , ties = None ) -> pd . Categorical : \"\"\"Return a copy of fct with categories lumped together. Parameters ---------- fct : A pandas.Categorical, or array(-like) used to create one. n : Number of categories to keep. prop : (not implemented) keep categories that occur prop proportion of the time. w : Array of weights corresponding to each value in fct. other_level : Name for all lumped together levels. ties : (not implemented) method to use in the case of ties. Notes ----- Currently, one of n and prop must be specified. Examples -------- >>> fct_lump(['a', 'a', 'b', 'c'], n = 1) ['a', 'a', 'Other', 'Other'] Categories (2, object): ['a', 'Other'] >>> fct_lump(['a', 'a', 'b', 'b', 'c', 'd'], prop = .2) ['a', 'a', 'b', 'b', 'Other', 'Other'] Categories (3, object): ['a', 'b', 'Other'] \"\"\" if ties is not None : raise NotImplementedError ( \"ties is not implemented\" ) if n is None and prop is None : raise NotImplementedError ( \"Either n or prop must be specified\" ) keep_cats = _fct_lump_n_cats ( fct , w , other_level , ties , n = n , prop = prop ) out = fct_collapse ( fct , { k : k for k in keep_cats }, group_other = other_level ) return _maybe_upcast ( fct , out )","title":"fct_lump()"},{"location":"forcats/#siuba.dply.forcats.fct_recode","text":"Return copy of fct with renamed categories. Parameters: Name Type Description Default fct A pandas.Categorical, or array(-like) used to create one. required **kwargs Arguments of form new_name = old_name. {} Examples: >>> cat = [ 'a' , 'b' , 'c' ] >>> fct_recode ( cat , z = 'c' ) [ 'a' , 'b' , 'z' ] Categories ( 3 , object ): [ 'a' , 'b' , 'z' ] >>> fct_recode ( cat , x = [ 'a' , 'b' ]) [ 'x' , 'x' , 'c' ] Categories ( 2 , object ): [ 'x' , 'c' ] >>> fct_recode ( cat , { \"x\" : [ 'a' , 'b' ]}) [ 'x' , 'x' , 'c' ] Categories ( 2 , object ): [ 'x' , 'c' ] Source code in siuba/dply/forcats.py @symbolic_dispatch def fct_recode ( fct , recat = None , ** kwargs ) -> pd . Categorical : \"\"\"Return copy of fct with renamed categories. Parameters ---------- fct : A pandas.Categorical, or array(-like) used to create one. **kwargs : Arguments of form new_name = old_name. Examples -------- >>> cat = ['a', 'b', 'c'] >>> fct_recode(cat, z = 'c') ['a', 'b', 'z'] Categories (3, object): ['a', 'b', 'z'] >>> fct_recode(cat, x = ['a', 'b']) ['x', 'x', 'c'] Categories (2, object): ['x', 'c'] >>> fct_recode(cat, {\"x\": ['a', 'b']}) ['x', 'x', 'c'] Categories (2, object): ['x', 'c'] \"\"\" if recat and not isinstance ( recat , dict ): raise TypeError ( \"fct_recode requires named args or a dict.\" ) if recat and kwargs : duplicate_keys = set ( recat ) . intersection ( set ( kwargs )) if duplicate_keys : raise ValueError ( \"The following recode name(s) were specified more than once: {} \" \\ . format ( duplicate_keys ) ) new_cats = { ** recat , ** kwargs } if recat else kwargs return _maybe_upcast ( fct , fct_collapse ( fct , new_cats ))","title":"fct_recode()"},{"location":"forcats/#siuba.dply.forcats.fct_reorder","text":"Return copy of fct, with categories reordered according to values in x. Parameters: Name Type Description Default fct A pandas.Categorical, or array(-like) used to create one. required x Values used to reorder categorical. Must be same length as fct. required func Function run over all values within a level of the categorical. <function median at 0x7f931ea6dea0> desc Whether to sort in descending order. False Examples: >>> fct_reorder ([ 'a' , 'a' , 'b' ], [ 4 , 3 , 2 ]) [ 'a' , 'a' , 'b' ] Categories ( 2 , object ): [ 'b' , 'a' ] >>> fct_reorder ([ 'a' , 'a' , 'b' ], [ 4 , 3 , 2 ], desc = True ) [ 'a' , 'a' , 'b' ] Categories ( 2 , object ): [ 'a' , 'b' ] >>> fct_reorder ([ 'x' , 'x' , 'y' ], [ 4 , 0 , 2 ], np . max ) [ 'x' , 'x' , 'y' ] Categories ( 2 , object ): [ 'y' , 'x' ] Source code in siuba/dply/forcats.py @symbolic_dispatch def fct_reorder ( fct , x , func = np . median , desc = False ) -> pd . Categorical : \"\"\"Return copy of fct, with categories reordered according to values in x. Parameters ---------- fct : A pandas.Categorical, or array(-like) used to create one. x : Values used to reorder categorical. Must be same length as fct. func : Function run over all values within a level of the categorical. desc : Whether to sort in descending order. Notes ----- NaN categories can't be ordered. When func returns NaN, sorting is always done with NaNs last. Examples -------- >>> fct_reorder(['a', 'a', 'b'], [4, 3, 2]) ['a', 'a', 'b'] Categories (2, object): ['b', 'a'] >>> fct_reorder(['a', 'a', 'b'], [4, 3, 2], desc = True) ['a', 'a', 'b'] Categories (2, object): ['a', 'b'] >>> fct_reorder(['x', 'x', 'y'], [4, 0, 2], np.max) ['x', 'x', 'y'] Categories (2, object): ['y', 'x'] \"\"\" x_vals = x . values if isinstance ( x , pd . Series ) else x s = pd . Series ( x_vals , index = fct ) # sort groups by calculated agg func. note that groupby uses dropna=True by default, # but that's okay, since pandas categoricals can't order the NA category ordered = s . groupby ( level = 0 ) . agg ( func ) . sort_values ( ascending = not desc ) out = pd . Categorical ( fct , categories = ordered . index ) return _maybe_upcast ( fct , out )","title":"fct_reorder()"},{"location":"forcats/#siuba.dply.forcats.fct_rev","text":"Return a copy of fct with category level order reversed.next Parameters: Name Type Description Default fct A pandas.Categorical, or array(-like) used to create one. required Examples: >>> fct = pd . Categorical ([ \"a\" , \"b\" , \"c\" ]) >>> fct [ 'a' , 'b' , 'c' ] Categories ( 3 , object ): [ 'a' , 'b' , 'c' ] >>> fct_rev ( fct ) [ 'a' , 'b' , 'c' ] Categories ( 3 , object ): [ 'c' , 'b' , 'a' ] Note that this function can also accept a list. >>> fct_rev ([ \"a\" , \"b\" , \"c\" ]) [ 'a' , 'b' , 'c' ] Categories ( 3 , object ): [ 'c' , 'b' , 'a' ] Source code in siuba/dply/forcats.py @symbolic_dispatch def fct_rev ( fct ) -> pd . Categorical : \"\"\"Return a copy of fct with category level order reversed.next Parameters ---------- fct : A pandas.Categorical, or array(-like) used to create one. Examples -------- >>> fct = pd.Categorical([\"a\", \"b\", \"c\"]) >>> fct ['a', 'b', 'c'] Categories (3, object): ['a', 'b', 'c'] >>> fct_rev(fct) ['a', 'b', 'c'] Categories (3, object): ['c', 'b', 'a'] Note that this function can also accept a list. >>> fct_rev([\"a\", \"b\", \"c\"]) ['a', 'b', 'c'] Categories (3, object): ['c', 'b', 'a'] \"\"\" if not isinstance ( fct , pd . Categorical ): fct = pd . Categorical ( fct ) rev_levels = list ( reversed ( fct . categories )) out = fct . reorder_categories ( rev_levels ) return _maybe_upcast ( fct , out )","title":"fct_rev()"},{"location":"vector/","text":"Importing from siuba.dply.vector import n, lead, lag siuba.dply.vector between ( x , left , right , default = False ) Return whether a value is between left and right (including either side). Examples: >>> between ( pd . Series ([ 1 , 2 , 3 ]), 0 , 2 ) 0 True 1 True 2 False dtype : bool Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def between ( x , left , right , default = False ): \"\"\"Return whether a value is between left and right (including either side). Example: >>> between(pd.Series([1,2,3]), 0, 2) 0 True 1 True 2 False dtype: bool Note: This is a thin wrapper around pd.Series.between(left, right) \"\"\" # note: NA -> False, in tidyverse NA -> NA if default is not False : raise TypeError ( \"between function must use default = False for pandas Series\" ) return x . between ( left , right ) coalesce ( x , * args ) Returns a copy of x, with NaN values filled in from *args. Ignores indexes. Parameters: Name Type Description Default x None a pandas Series object required *args None other Series that are the same length as x, or a scalar () Examples: >>> x = pd . Series ([ 1.1 , None , None ]) >>> abc = pd . Series ([ 'a' , 'b' , None ]) >>> xyz = pd . Series ([ 'x' , 'y' , 'z' ]) >>> coalesce ( x , abc ) 0 1.1 1 b 2 None dtype : object >>> coalesce ( x , abc , xyz ) 0 1.1 1 b 2 z dtype : object Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def coalesce ( x , * args ): \"\"\"Returns a copy of x, with NaN values filled in from \\*args. Ignores indexes. Arguments: x: a pandas Series object *args: other Series that are the same length as x, or a scalar Examples: >>> x = pd.Series([1.1, None, None]) >>> abc = pd.Series(['a', 'b', None]) >>> xyz = pd.Series(['x', 'y', 'z']) >>> coalesce(x, abc) 0 1.1 1 b 2 None dtype: object >>> coalesce(x, abc, xyz) 0 1.1 1 b 2 z dtype: object \"\"\" crnt = x . reset_index ( drop = True ) for other in args : if isinstance ( other , pd . Series ): other = other . reset_index ( drop = True ) crnt = crnt . where ( crnt . notna (), other ) crnt . index = x . index return crnt cumall ( x ) Return a same-length array. For each entry, indicates whether that entry and all previous are True-like. Examples: >>> cumall ( pd . Series ([ True , False , False ])) 0 True 1 False 2 False dtype : bool Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def cumall ( x ): \"\"\"Return a same-length array. For each entry, indicates whether that entry and all previous are True-like. Example: >>> cumall(pd.Series([True, False, False])) 0 True 1 False 2 False dtype: bool \"\"\" return _expand_bool ( x , np . all ) cumany ( x ) Return a same-length array. For each entry, indicates whether that entry or any previous are True-like. Examples: >>> cumany ( pd . Series ([ False , True , False ])) 0 False 1 True 2 True dtype : bool Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def cumany ( x ): \"\"\"Return a same-length array. For each entry, indicates whether that entry or any previous are True-like. Example: >>> cumany(pd.Series([False, True, False])) 0 False 1 True 2 True dtype: bool \"\"\" return _expand_bool ( x , np . any ) cume_dist ( x , na_option = 'keep' ) Return the cumulative distribution corresponding to each value in x. This reflects the proportion of values that are less than or equal to each value. Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def cume_dist ( x , na_option = \"keep\" ): \"\"\"Return the cumulative distribution corresponding to each value in x. This reflects the proportion of values that are less than or equal to each value. \"\"\" return x . rank ( method = \"max\" , na_option = na_option ) / x . count () cummean ( x ) Return a same-length array, containing the cumulative mean. Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def cummean ( x ): \"\"\"Return a same-length array, containing the cumulative mean.\"\"\" return x . expanding () . mean () dense_rank ( x , na_option = 'keep' ) Return the dense rank. This method of ranking returns values ranging from 1 to the number of unique entries. Ties are all given the same ranking. Examples: >>> dense_rank ( pd . Series ([ 1 , 3 , 3 , 5 ])) 0 1.0 1 2.0 2 2.0 3 3.0 dtype : float64 Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def dense_rank ( x , na_option = \"keep\" ): \"\"\"Return the dense rank. This method of ranking returns values ranging from 1 to the number of unique entries. Ties are all given the same ranking. Example: >>> dense_rank(pd.Series([1,3,3,5])) 0 1.0 1 2.0 2 2.0 3 3.0 dtype: float64 \"\"\" return x . rank ( method = \"dense\" , na_option = na_option ) desc ( x ) Return array sorted in descending order. Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def desc ( x ): \"\"\"Return array sorted in descending order.\"\"\" return x . sort_values ( ascending = False ) . reset_index ( drop = True ) first ( x , n , order_by = None , default = None ) Return the nth entry of x. Similar to x[n]. Note: first(x) and last(x) are nth(x, 0) and nth(x, -1). Parameters: Name Type Description Default x None series to get entry from. required n None position of entry to get from x (0 indicates first entry). required order_by None optional Series used to reorder x. None default None (not implemented) value to return if no entry at n. None Examples: >>> ser = pd . Series ([ 'a' , 'b' , 'c' ]) >>> nth ( ser , 1 ) 'b' >>> sorter = pd . Series ([ 1 , 2 , 0 ]) >>> nth ( ser , 1 , order_by = sorter ) 'a' >>> nth ( ser , 0 ), nth ( ser , - 1 ) ( 'a' , 'c' ) >>> first ( ser ), last ( ser ) ( 'a' , 'c' ) Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def nth ( x , n , order_by = None , default = None ): \"\"\"Return the nth entry of x. Similar to x[n]. Note: first(x) and last(x) are nth(x, 0) and nth(x, -1). Arguments: x: series to get entry from. n: position of entry to get from x (0 indicates first entry). order_by: optional Series used to reorder x. default: (not implemented) value to return if no entry at n. Examples: >>> ser = pd.Series(['a', 'b', 'c']) >>> nth(ser, 1) 'b' >>> sorter = pd.Series([1, 2, 0]) >>> nth(ser, 1, order_by = sorter) 'a' >>> nth(ser, 0), nth(ser, -1) ('a', 'c') >>> first(ser), last(ser) ('a', 'c') \"\"\" if default is not None : raise NotImplementedError ( \"default argument not implemented\" ) # check indexing is in range, handles positive and negative cases. # TODO: is returning None the correct behavior for an empty Series? if n >= len ( x ) or abs ( n ) > len ( x ): return default if order_by is None : return x . iloc [ n ] # case where order_by is specified and n in range ---- # TODO: ensure order_by is arraylike if not isinstance ( order_by , pd . Series ): raise NotImplementedError ( \"order_by argument is type %s , but currently only\" \"implemented for Series\" % type ( order_by ) ) if len ( x ) != len ( order_by ): raise ValueError ( \"x and order_by arguments must be same length\" ) order_indx = order_by . reset_index ( drop = True ) . sort_values () . index return x . iloc [ order_indx [ n ]] lag ( x , n = 1 , default = None ) Return an array with each value replaced by the previous (or further backward) value in the array. Parameters: Name Type Description Default x None a pandas Series object required n None number of next values backward to replace each value with 1 default None what to replace the n final values of the array with None Examples: >>> lag ( pd . Series ([ 1 , 2 , 3 ]), n = 1 ) 0 NaN 1 1.0 2 2.0 dtype : float64 >>> lag ( pd . Series ([ 1 , 2 , 3 ]), n = 1 , default = 99 ) 0 99.0 1 1.0 2 2.0 dtype : float64 Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def lag ( x , n = 1 , default = None ): \"\"\"Return an array with each value replaced by the previous (or further backward) value in the array. Arguments: x: a pandas Series object n: number of next values backward to replace each value with default: what to replace the n final values of the array with Example: >>> lag(pd.Series([1,2,3]), n=1) 0 NaN 1 1.0 2 2.0 dtype: float64 >>> lag(pd.Series([1,2,3]), n=1, default = 99) 0 99.0 1 1.0 2 2.0 dtype: float64 \"\"\" res = x . shift ( n ) if default is not None : res . iloc [: n ] = default return res last ( x , n , order_by = None , default = None ) Return the nth entry of x. Similar to x[n]. Note: first(x) and last(x) are nth(x, 0) and nth(x, -1). Parameters: Name Type Description Default x None series to get entry from. required n None position of entry to get from x (0 indicates first entry). required order_by None optional Series used to reorder x. None default None (not implemented) value to return if no entry at n. None Examples: >>> ser = pd . Series ([ 'a' , 'b' , 'c' ]) >>> nth ( ser , 1 ) 'b' >>> sorter = pd . Series ([ 1 , 2 , 0 ]) >>> nth ( ser , 1 , order_by = sorter ) 'a' >>> nth ( ser , 0 ), nth ( ser , - 1 ) ( 'a' , 'c' ) >>> first ( ser ), last ( ser ) ( 'a' , 'c' ) Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def nth ( x , n , order_by = None , default = None ): \"\"\"Return the nth entry of x. Similar to x[n]. Note: first(x) and last(x) are nth(x, 0) and nth(x, -1). Arguments: x: series to get entry from. n: position of entry to get from x (0 indicates first entry). order_by: optional Series used to reorder x. default: (not implemented) value to return if no entry at n. Examples: >>> ser = pd.Series(['a', 'b', 'c']) >>> nth(ser, 1) 'b' >>> sorter = pd.Series([1, 2, 0]) >>> nth(ser, 1, order_by = sorter) 'a' >>> nth(ser, 0), nth(ser, -1) ('a', 'c') >>> first(ser), last(ser) ('a', 'c') \"\"\" if default is not None : raise NotImplementedError ( \"default argument not implemented\" ) # check indexing is in range, handles positive and negative cases. # TODO: is returning None the correct behavior for an empty Series? if n >= len ( x ) or abs ( n ) > len ( x ): return default if order_by is None : return x . iloc [ n ] # case where order_by is specified and n in range ---- # TODO: ensure order_by is arraylike if not isinstance ( order_by , pd . Series ): raise NotImplementedError ( \"order_by argument is type %s , but currently only\" \"implemented for Series\" % type ( order_by ) ) if len ( x ) != len ( order_by ): raise ValueError ( \"x and order_by arguments must be same length\" ) order_indx = order_by . reset_index ( drop = True ) . sort_values () . index return x . iloc [ order_indx [ n ]] lead ( x , n = 1 , default = None ) Return an array with each value replaced by the next (or further forward) value in the array. Parameters: Name Type Description Default x None a pandas Series object required n None number of next values forward to replace each value with 1 default None what to replace the n final values of the array with None Examples: >>> lead ( pd . Series ([ 1 , 2 , 3 ]), n = 1 ) 0 2.0 1 3.0 2 NaN dtype : float64 >>> lead ( pd . Series ([ 1 , 2 , 3 ]), n = 1 , default = 99 ) 0 2 1 3 2 99 dtype : int64 Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def lead ( x , n = 1 , default = None ): \"\"\"Return an array with each value replaced by the next (or further forward) value in the array. Arguments: x: a pandas Series object n: number of next values forward to replace each value with default: what to replace the n final values of the array with Example: >>> lead(pd.Series([1,2,3]), n=1) 0 2.0 1 3.0 2 NaN dtype: float64 >>> lead(pd.Series([1,2,3]), n=1, default = 99) 0 2 1 3 2 99 dtype: int64 \"\"\" res = x . shift ( - 1 * n , fill_value = default ) return res min_rank ( x , na_option = 'keep' ) Return the min rank. See pd.Series.rank with method=\"min\" for details. Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def min_rank ( x , na_option = \"keep\" ): \"\"\"Return the min rank. See pd.Series.rank with method=\"min\" for details. \"\"\" return x . rank ( method = \"min\" , na_option = na_option ) n ( x ) Return the total number of elements in the array (or rows in a DataFrame). Examples: >>> ser = pd . Series ([ 1 , 2 , 3 ]) >>> n ( ser ) 3 >>> df = pd . DataFrame ({ 'x' : ser }) >>> n ( df ) 3 Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = NDFrame ) def n ( x ): \"\"\"Return the total number of elements in the array (or rows in a DataFrame). Example: >>> ser = pd.Series([1,2,3]) >>> n(ser) 3 >>> df = pd.DataFrame({'x': ser}) >>> n(df) 3 \"\"\" if isinstance ( x , pd . DataFrame ): return x . shape [ 0 ] return len ( x ) n_distinct ( x ) Return the total number of distinct (i.e. unique) elements in an array. Examples: >>> n_distinct ( pd . Series ([ 1 , 1 , 2 , 2 ])) 2 Source code in siuba/dply/vector.py @alias_series_agg ( 'nunique' ) @symbolic_dispatch ( cls = Series ) def n_distinct ( x ): \"\"\"Return the total number of distinct (i.e. unique) elements in an array. Example: >>> n_distinct(pd.Series([1,1,2,2])) 2 \"\"\" return x . nunique () na_if ( x , y ) Return a array like x, but with values in y replaced by NAs. Examples: >>> na_if ( pd . Series ([ 1 , 2 , 3 ]), [ 1 , 3 ]) 0 NaN 1 2.0 2 NaN dtype : float64 Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def na_if ( x , y ): \"\"\"Return a array like x, but with values in y replaced by NAs. Examples: >>> na_if(pd.Series([1,2,3]), [1,3]) 0 NaN 1 2.0 2 NaN dtype: float64 \"\"\" y = [ y ] if not np . ndim ( y ) else y tmp_x = x . copy ( deep = True ) tmp_x [ x . isin ( y )] = np . nan return tmp_x near ( x ) TODO: Not Implemented Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def near ( x ): \"\"\"TODO: Not Implemented\"\"\" raise NotImplementedError ( \"near not implemented\" ) nth ( x , n , order_by = None , default = None ) Return the nth entry of x. Similar to x[n]. Note: first(x) and last(x) are nth(x, 0) and nth(x, -1). Parameters: Name Type Description Default x None series to get entry from. required n None position of entry to get from x (0 indicates first entry). required order_by None optional Series used to reorder x. None default None (not implemented) value to return if no entry at n. None Examples: >>> ser = pd . Series ([ 'a' , 'b' , 'c' ]) >>> nth ( ser , 1 ) 'b' >>> sorter = pd . Series ([ 1 , 2 , 0 ]) >>> nth ( ser , 1 , order_by = sorter ) 'a' >>> nth ( ser , 0 ), nth ( ser , - 1 ) ( 'a' , 'c' ) >>> first ( ser ), last ( ser ) ( 'a' , 'c' ) Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def nth ( x , n , order_by = None , default = None ): \"\"\"Return the nth entry of x. Similar to x[n]. Note: first(x) and last(x) are nth(x, 0) and nth(x, -1). Arguments: x: series to get entry from. n: position of entry to get from x (0 indicates first entry). order_by: optional Series used to reorder x. default: (not implemented) value to return if no entry at n. Examples: >>> ser = pd.Series(['a', 'b', 'c']) >>> nth(ser, 1) 'b' >>> sorter = pd.Series([1, 2, 0]) >>> nth(ser, 1, order_by = sorter) 'a' >>> nth(ser, 0), nth(ser, -1) ('a', 'c') >>> first(ser), last(ser) ('a', 'c') \"\"\" if default is not None : raise NotImplementedError ( \"default argument not implemented\" ) # check indexing is in range, handles positive and negative cases. # TODO: is returning None the correct behavior for an empty Series? if n >= len ( x ) or abs ( n ) > len ( x ): return default if order_by is None : return x . iloc [ n ] # case where order_by is specified and n in range ---- # TODO: ensure order_by is arraylike if not isinstance ( order_by , pd . Series ): raise NotImplementedError ( \"order_by argument is type %s , but currently only\" \"implemented for Series\" % type ( order_by ) ) if len ( x ) != len ( order_by ): raise ValueError ( \"x and order_by arguments must be same length\" ) order_indx = order_by . reset_index ( drop = True ) . sort_values () . index return x . iloc [ order_indx [ n ]] ntile ( x , n ) TODO: Not Implemented Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def ntile ( x , n ): \"\"\"TODO: Not Implemented\"\"\" raise NotImplementedError ( \"ntile not implemented\" ) percent_rank ( x , na_option = 'keep' ) Return the percent rank. Note: Uses minimum rank, and reports the proportion of unique ranks each entry is greater than. Examples: >>> percent_rank ( pd . Series ([ 1 , 2 , 3 ])) 0 0.0 1 0.5 2 1.0 dtype : float64 >>> percent_rank ( pd . Series ([ 1 , 2 , 2 ])) 0 0.0 1 0.5 2 0.5 dtype : float64 >>> percent_rank ( pd . Series ([ 1 ])) 0 NaN dtype : float64 Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def percent_rank ( x , na_option = \"keep\" ): \"\"\"Return the percent rank. Note: Uses minimum rank, and reports the proportion of unique ranks each entry is greater than. Examples: >>> percent_rank(pd.Series([1, 2, 3])) 0 0.0 1 0.5 2 1.0 dtype: float64 >>> percent_rank(pd.Series([1, 2, 2])) 0 0.0 1 0.5 2 0.5 dtype: float64 >>> percent_rank(pd.Series([1])) 0 NaN dtype: float64 \"\"\" return ( min_rank ( x ) - 1 ) / ( x . count () - 1 ) row_number ( x ) Return the row number (position) for each value in x, beginning with 1. Examples: >>> ser = pd . Series ([ 7 , 8 ]) >>> row_number ( ser ) 0 1 1 2 dtype : int64 >>> row_number ( pd . DataFrame ({ 'a' : ser })) 0 1 1 2 dtype : int64 >>> row_number ( pd . Series ([ 7 , 8 ], index = [ 3 , 4 ])) 3 1 4 2 dtype : int64 Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = NDFrame ) def row_number ( x ): \"\"\"Return the row number (position) for each value in x, beginning with 1. Example: >>> ser = pd.Series([7,8]) >>> row_number(ser) 0 1 1 2 dtype: int64 >>> row_number(pd.DataFrame({'a': ser})) 0 1 1 2 dtype: int64 >>> row_number(pd.Series([7,8], index = [3, 4])) 3 1 4 2 dtype: int64 \"\"\" if isinstance ( x , pd . DataFrame ): n = x . shape [ 0 ] else : n = len ( x ) arr = np . arange ( 1 , n + 1 ) # could use single dispatch, but for now ensure output data type matches input if isinstance ( x , pd . Series ): return x . _constructor ( arr , x . index , fastpath = True ) return pd . Series ( arr , x . index , fastpath = True ) Conditionals Note that these functions currently can be imported from the top level: from siuba import case_when, if_else They will be moved into the siuba.dply.vector module, but the above import will continue to work for backwards compatibility. case_when ( __data , cases ) Generalized, vectorized if statement. Parameters: Name Type Description Default __data The input data. required cases dict A mapping of condition : value. required Examples: >>> import pandas as pd >>> from siuba import _ , case_when >>> df = pd . DataFrame ({ \"x\" : [ 1 , 2 , 3 ]}) >>> case_when ( df , { _ . x == 1 : \"one\" , _ . x == 2 : \"two\" }) 0 one 1 two 2 None dtype : object >>> df >> case_when ({ _ . x == 1 : \"one\" , _ . x == 2 : \"two\" }) 0 one 1 two 2 None dtype : object >>> df >> case_when ({ _ . x == 1 : \"one\" , _ . x == 2 : \"two\" , True : \"other\" }) 0 one 1 two 2 other dtype : object Source code in siuba/dply/verbs.py @singledispatch2 (( pd . DataFrame , pd . Series )) def case_when ( __data , cases : dict ): \"\"\"Generalized, vectorized if statement. Parameters ---------- __data: The input data. cases: dict A mapping of condition : value. See Also -------- if_else : Handles the special case of two conditions. Examples -------- >>> import pandas as pd >>> from siuba import _, case_when >>> df = pd.DataFrame({\"x\": [1, 2, 3]}) >>> case_when(df, {_.x == 1: \"one\", _.x == 2: \"two\"}) 0 one 1 two 2 None dtype: object >>> df >> case_when({_.x == 1: \"one\", _.x == 2: \"two\"}) 0 one 1 two 2 None dtype: object >>> df >> case_when({_.x == 1: \"one\", _.x == 2: \"two\", True: \"other\"}) 0 one 1 two 2 other dtype: object \"\"\" if isinstance ( cases , Call ): cases = cases ( __data ) # TODO: handle when receive list of (k,v) pairs for py < 3.5 compat? stripped_cases = { strip_symbolic ( k ): strip_symbolic ( v ) for k , v in cases . items ()} n = len ( __data ) out = np . repeat ( None , n ) for k , v in reversed ( list ( stripped_cases . items ())): if callable ( k ): result = _val_call ( k , __data , n ) indx = np . where ( result )[ 0 ] val_res = _val_call ( v , __data , n , indx ) out [ indx ] = val_res elif k : # e.g. k is just True, etc.. val_res = _val_call ( v , __data , n ) out [:] = val_res # by recreating an array, attempts to cast as best dtype return pd . Series ( list ( out )) if_else ( condition , true , false ) Parameters: Name Type Description Default condition Logical vector (or lazy expression). required true Values to be used when condition is True. required false Values to be used when condition is False. required Examples: >>> ser1 = pd . Series ([ 1 , 2 , 3 ]) >>> if_else ( ser1 > 2 , np . nan , ser1 ) 0 1.0 1 2.0 2 NaN dtype : float64 >>> from siuba import _ >>> f = if_else ( _ < 2 , _ , 2 ) >>> f ( ser1 ) 0 1 1 2 2 2 dtype : int64 >>> import numpy as np >>> ser2 = pd . Series ([ 'NA' , 'a' , 'b' ]) >>> if_else ( ser2 == 'NA' , np . nan , ser2 ) 0 NaN 1 a 2 b dtype : object Source code in siuba/dply/verbs.py @singledispatch def if_else ( condition , true , false ): \"\"\" Parameters ---------- condition: Logical vector (or lazy expression). true: Values to be used when condition is True. false: Values to be used when condition is False. See Also -------- case_when : Generalized if_else, for handling many cases. Examples -------- >>> ser1 = pd.Series([1,2,3]) >>> if_else(ser1 > 2, np.nan, ser1) 0 1.0 1 2.0 2 NaN dtype: float64 >>> from siuba import _ >>> f = if_else(_ < 2, _, 2) >>> f(ser1) 0 1 1 2 2 2 dtype: int64 >>> import numpy as np >>> ser2 = pd.Series(['NA', 'a', 'b']) >>> if_else(ser2 == 'NA', np.nan, ser2) 0 NaN 1 a 2 b dtype: object \"\"\" raise_type_error ( condition )","title":"General"},{"location":"vector/#importing","text":"from siuba.dply.vector import n, lead, lag","title":"Importing"},{"location":"vector/#siuba.dply.vector","text":"","title":"vector"},{"location":"vector/#siuba.dply.vector.between","text":"Return whether a value is between left and right (including either side). Examples: >>> between ( pd . Series ([ 1 , 2 , 3 ]), 0 , 2 ) 0 True 1 True 2 False dtype : bool Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def between ( x , left , right , default = False ): \"\"\"Return whether a value is between left and right (including either side). Example: >>> between(pd.Series([1,2,3]), 0, 2) 0 True 1 True 2 False dtype: bool Note: This is a thin wrapper around pd.Series.between(left, right) \"\"\" # note: NA -> False, in tidyverse NA -> NA if default is not False : raise TypeError ( \"between function must use default = False for pandas Series\" ) return x . between ( left , right )","title":"between()"},{"location":"vector/#siuba.dply.vector.coalesce","text":"Returns a copy of x, with NaN values filled in from *args. Ignores indexes. Parameters: Name Type Description Default x None a pandas Series object required *args None other Series that are the same length as x, or a scalar () Examples: >>> x = pd . Series ([ 1.1 , None , None ]) >>> abc = pd . Series ([ 'a' , 'b' , None ]) >>> xyz = pd . Series ([ 'x' , 'y' , 'z' ]) >>> coalesce ( x , abc ) 0 1.1 1 b 2 None dtype : object >>> coalesce ( x , abc , xyz ) 0 1.1 1 b 2 z dtype : object Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def coalesce ( x , * args ): \"\"\"Returns a copy of x, with NaN values filled in from \\*args. Ignores indexes. Arguments: x: a pandas Series object *args: other Series that are the same length as x, or a scalar Examples: >>> x = pd.Series([1.1, None, None]) >>> abc = pd.Series(['a', 'b', None]) >>> xyz = pd.Series(['x', 'y', 'z']) >>> coalesce(x, abc) 0 1.1 1 b 2 None dtype: object >>> coalesce(x, abc, xyz) 0 1.1 1 b 2 z dtype: object \"\"\" crnt = x . reset_index ( drop = True ) for other in args : if isinstance ( other , pd . Series ): other = other . reset_index ( drop = True ) crnt = crnt . where ( crnt . notna (), other ) crnt . index = x . index return crnt","title":"coalesce()"},{"location":"vector/#siuba.dply.vector.cumall","text":"Return a same-length array. For each entry, indicates whether that entry and all previous are True-like. Examples: >>> cumall ( pd . Series ([ True , False , False ])) 0 True 1 False 2 False dtype : bool Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def cumall ( x ): \"\"\"Return a same-length array. For each entry, indicates whether that entry and all previous are True-like. Example: >>> cumall(pd.Series([True, False, False])) 0 True 1 False 2 False dtype: bool \"\"\" return _expand_bool ( x , np . all )","title":"cumall()"},{"location":"vector/#siuba.dply.vector.cumany","text":"Return a same-length array. For each entry, indicates whether that entry or any previous are True-like. Examples: >>> cumany ( pd . Series ([ False , True , False ])) 0 False 1 True 2 True dtype : bool Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def cumany ( x ): \"\"\"Return a same-length array. For each entry, indicates whether that entry or any previous are True-like. Example: >>> cumany(pd.Series([False, True, False])) 0 False 1 True 2 True dtype: bool \"\"\" return _expand_bool ( x , np . any )","title":"cumany()"},{"location":"vector/#siuba.dply.vector.cume_dist","text":"Return the cumulative distribution corresponding to each value in x. This reflects the proportion of values that are less than or equal to each value. Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def cume_dist ( x , na_option = \"keep\" ): \"\"\"Return the cumulative distribution corresponding to each value in x. This reflects the proportion of values that are less than or equal to each value. \"\"\" return x . rank ( method = \"max\" , na_option = na_option ) / x . count ()","title":"cume_dist()"},{"location":"vector/#siuba.dply.vector.cummean","text":"Return a same-length array, containing the cumulative mean. Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def cummean ( x ): \"\"\"Return a same-length array, containing the cumulative mean.\"\"\" return x . expanding () . mean ()","title":"cummean()"},{"location":"vector/#siuba.dply.vector.dense_rank","text":"Return the dense rank. This method of ranking returns values ranging from 1 to the number of unique entries. Ties are all given the same ranking. Examples: >>> dense_rank ( pd . Series ([ 1 , 3 , 3 , 5 ])) 0 1.0 1 2.0 2 2.0 3 3.0 dtype : float64 Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def dense_rank ( x , na_option = \"keep\" ): \"\"\"Return the dense rank. This method of ranking returns values ranging from 1 to the number of unique entries. Ties are all given the same ranking. Example: >>> dense_rank(pd.Series([1,3,3,5])) 0 1.0 1 2.0 2 2.0 3 3.0 dtype: float64 \"\"\" return x . rank ( method = \"dense\" , na_option = na_option )","title":"dense_rank()"},{"location":"vector/#siuba.dply.vector.desc","text":"Return array sorted in descending order. Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def desc ( x ): \"\"\"Return array sorted in descending order.\"\"\" return x . sort_values ( ascending = False ) . reset_index ( drop = True )","title":"desc()"},{"location":"vector/#siuba.dply.vector.first","text":"Return the nth entry of x. Similar to x[n]. Note: first(x) and last(x) are nth(x, 0) and nth(x, -1). Parameters: Name Type Description Default x None series to get entry from. required n None position of entry to get from x (0 indicates first entry). required order_by None optional Series used to reorder x. None default None (not implemented) value to return if no entry at n. None Examples: >>> ser = pd . Series ([ 'a' , 'b' , 'c' ]) >>> nth ( ser , 1 ) 'b' >>> sorter = pd . Series ([ 1 , 2 , 0 ]) >>> nth ( ser , 1 , order_by = sorter ) 'a' >>> nth ( ser , 0 ), nth ( ser , - 1 ) ( 'a' , 'c' ) >>> first ( ser ), last ( ser ) ( 'a' , 'c' ) Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def nth ( x , n , order_by = None , default = None ): \"\"\"Return the nth entry of x. Similar to x[n]. Note: first(x) and last(x) are nth(x, 0) and nth(x, -1). Arguments: x: series to get entry from. n: position of entry to get from x (0 indicates first entry). order_by: optional Series used to reorder x. default: (not implemented) value to return if no entry at n. Examples: >>> ser = pd.Series(['a', 'b', 'c']) >>> nth(ser, 1) 'b' >>> sorter = pd.Series([1, 2, 0]) >>> nth(ser, 1, order_by = sorter) 'a' >>> nth(ser, 0), nth(ser, -1) ('a', 'c') >>> first(ser), last(ser) ('a', 'c') \"\"\" if default is not None : raise NotImplementedError ( \"default argument not implemented\" ) # check indexing is in range, handles positive and negative cases. # TODO: is returning None the correct behavior for an empty Series? if n >= len ( x ) or abs ( n ) > len ( x ): return default if order_by is None : return x . iloc [ n ] # case where order_by is specified and n in range ---- # TODO: ensure order_by is arraylike if not isinstance ( order_by , pd . Series ): raise NotImplementedError ( \"order_by argument is type %s , but currently only\" \"implemented for Series\" % type ( order_by ) ) if len ( x ) != len ( order_by ): raise ValueError ( \"x and order_by arguments must be same length\" ) order_indx = order_by . reset_index ( drop = True ) . sort_values () . index return x . iloc [ order_indx [ n ]]","title":"first()"},{"location":"vector/#siuba.dply.vector.lag","text":"Return an array with each value replaced by the previous (or further backward) value in the array. Parameters: Name Type Description Default x None a pandas Series object required n None number of next values backward to replace each value with 1 default None what to replace the n final values of the array with None Examples: >>> lag ( pd . Series ([ 1 , 2 , 3 ]), n = 1 ) 0 NaN 1 1.0 2 2.0 dtype : float64 >>> lag ( pd . Series ([ 1 , 2 , 3 ]), n = 1 , default = 99 ) 0 99.0 1 1.0 2 2.0 dtype : float64 Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def lag ( x , n = 1 , default = None ): \"\"\"Return an array with each value replaced by the previous (or further backward) value in the array. Arguments: x: a pandas Series object n: number of next values backward to replace each value with default: what to replace the n final values of the array with Example: >>> lag(pd.Series([1,2,3]), n=1) 0 NaN 1 1.0 2 2.0 dtype: float64 >>> lag(pd.Series([1,2,3]), n=1, default = 99) 0 99.0 1 1.0 2 2.0 dtype: float64 \"\"\" res = x . shift ( n ) if default is not None : res . iloc [: n ] = default return res","title":"lag()"},{"location":"vector/#siuba.dply.vector.last","text":"Return the nth entry of x. Similar to x[n]. Note: first(x) and last(x) are nth(x, 0) and nth(x, -1). Parameters: Name Type Description Default x None series to get entry from. required n None position of entry to get from x (0 indicates first entry). required order_by None optional Series used to reorder x. None default None (not implemented) value to return if no entry at n. None Examples: >>> ser = pd . Series ([ 'a' , 'b' , 'c' ]) >>> nth ( ser , 1 ) 'b' >>> sorter = pd . Series ([ 1 , 2 , 0 ]) >>> nth ( ser , 1 , order_by = sorter ) 'a' >>> nth ( ser , 0 ), nth ( ser , - 1 ) ( 'a' , 'c' ) >>> first ( ser ), last ( ser ) ( 'a' , 'c' ) Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def nth ( x , n , order_by = None , default = None ): \"\"\"Return the nth entry of x. Similar to x[n]. Note: first(x) and last(x) are nth(x, 0) and nth(x, -1). Arguments: x: series to get entry from. n: position of entry to get from x (0 indicates first entry). order_by: optional Series used to reorder x. default: (not implemented) value to return if no entry at n. Examples: >>> ser = pd.Series(['a', 'b', 'c']) >>> nth(ser, 1) 'b' >>> sorter = pd.Series([1, 2, 0]) >>> nth(ser, 1, order_by = sorter) 'a' >>> nth(ser, 0), nth(ser, -1) ('a', 'c') >>> first(ser), last(ser) ('a', 'c') \"\"\" if default is not None : raise NotImplementedError ( \"default argument not implemented\" ) # check indexing is in range, handles positive and negative cases. # TODO: is returning None the correct behavior for an empty Series? if n >= len ( x ) or abs ( n ) > len ( x ): return default if order_by is None : return x . iloc [ n ] # case where order_by is specified and n in range ---- # TODO: ensure order_by is arraylike if not isinstance ( order_by , pd . Series ): raise NotImplementedError ( \"order_by argument is type %s , but currently only\" \"implemented for Series\" % type ( order_by ) ) if len ( x ) != len ( order_by ): raise ValueError ( \"x and order_by arguments must be same length\" ) order_indx = order_by . reset_index ( drop = True ) . sort_values () . index return x . iloc [ order_indx [ n ]]","title":"last()"},{"location":"vector/#siuba.dply.vector.lead","text":"Return an array with each value replaced by the next (or further forward) value in the array. Parameters: Name Type Description Default x None a pandas Series object required n None number of next values forward to replace each value with 1 default None what to replace the n final values of the array with None Examples: >>> lead ( pd . Series ([ 1 , 2 , 3 ]), n = 1 ) 0 2.0 1 3.0 2 NaN dtype : float64 >>> lead ( pd . Series ([ 1 , 2 , 3 ]), n = 1 , default = 99 ) 0 2 1 3 2 99 dtype : int64 Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def lead ( x , n = 1 , default = None ): \"\"\"Return an array with each value replaced by the next (or further forward) value in the array. Arguments: x: a pandas Series object n: number of next values forward to replace each value with default: what to replace the n final values of the array with Example: >>> lead(pd.Series([1,2,3]), n=1) 0 2.0 1 3.0 2 NaN dtype: float64 >>> lead(pd.Series([1,2,3]), n=1, default = 99) 0 2 1 3 2 99 dtype: int64 \"\"\" res = x . shift ( - 1 * n , fill_value = default ) return res","title":"lead()"},{"location":"vector/#siuba.dply.vector.min_rank","text":"Return the min rank. See pd.Series.rank with method=\"min\" for details. Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def min_rank ( x , na_option = \"keep\" ): \"\"\"Return the min rank. See pd.Series.rank with method=\"min\" for details. \"\"\" return x . rank ( method = \"min\" , na_option = na_option )","title":"min_rank()"},{"location":"vector/#siuba.dply.vector.n","text":"Return the total number of elements in the array (or rows in a DataFrame). Examples: >>> ser = pd . Series ([ 1 , 2 , 3 ]) >>> n ( ser ) 3 >>> df = pd . DataFrame ({ 'x' : ser }) >>> n ( df ) 3 Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = NDFrame ) def n ( x ): \"\"\"Return the total number of elements in the array (or rows in a DataFrame). Example: >>> ser = pd.Series([1,2,3]) >>> n(ser) 3 >>> df = pd.DataFrame({'x': ser}) >>> n(df) 3 \"\"\" if isinstance ( x , pd . DataFrame ): return x . shape [ 0 ] return len ( x )","title":"n()"},{"location":"vector/#siuba.dply.vector.n_distinct","text":"Return the total number of distinct (i.e. unique) elements in an array. Examples: >>> n_distinct ( pd . Series ([ 1 , 1 , 2 , 2 ])) 2 Source code in siuba/dply/vector.py @alias_series_agg ( 'nunique' ) @symbolic_dispatch ( cls = Series ) def n_distinct ( x ): \"\"\"Return the total number of distinct (i.e. unique) elements in an array. Example: >>> n_distinct(pd.Series([1,1,2,2])) 2 \"\"\" return x . nunique ()","title":"n_distinct()"},{"location":"vector/#siuba.dply.vector.na_if","text":"Return a array like x, but with values in y replaced by NAs. Examples: >>> na_if ( pd . Series ([ 1 , 2 , 3 ]), [ 1 , 3 ]) 0 NaN 1 2.0 2 NaN dtype : float64 Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def na_if ( x , y ): \"\"\"Return a array like x, but with values in y replaced by NAs. Examples: >>> na_if(pd.Series([1,2,3]), [1,3]) 0 NaN 1 2.0 2 NaN dtype: float64 \"\"\" y = [ y ] if not np . ndim ( y ) else y tmp_x = x . copy ( deep = True ) tmp_x [ x . isin ( y )] = np . nan return tmp_x","title":"na_if()"},{"location":"vector/#siuba.dply.vector.near","text":"TODO: Not Implemented Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def near ( x ): \"\"\"TODO: Not Implemented\"\"\" raise NotImplementedError ( \"near not implemented\" )","title":"near()"},{"location":"vector/#siuba.dply.vector.nth","text":"Return the nth entry of x. Similar to x[n]. Note: first(x) and last(x) are nth(x, 0) and nth(x, -1). Parameters: Name Type Description Default x None series to get entry from. required n None position of entry to get from x (0 indicates first entry). required order_by None optional Series used to reorder x. None default None (not implemented) value to return if no entry at n. None Examples: >>> ser = pd . Series ([ 'a' , 'b' , 'c' ]) >>> nth ( ser , 1 ) 'b' >>> sorter = pd . Series ([ 1 , 2 , 0 ]) >>> nth ( ser , 1 , order_by = sorter ) 'a' >>> nth ( ser , 0 ), nth ( ser , - 1 ) ( 'a' , 'c' ) >>> first ( ser ), last ( ser ) ( 'a' , 'c' ) Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def nth ( x , n , order_by = None , default = None ): \"\"\"Return the nth entry of x. Similar to x[n]. Note: first(x) and last(x) are nth(x, 0) and nth(x, -1). Arguments: x: series to get entry from. n: position of entry to get from x (0 indicates first entry). order_by: optional Series used to reorder x. default: (not implemented) value to return if no entry at n. Examples: >>> ser = pd.Series(['a', 'b', 'c']) >>> nth(ser, 1) 'b' >>> sorter = pd.Series([1, 2, 0]) >>> nth(ser, 1, order_by = sorter) 'a' >>> nth(ser, 0), nth(ser, -1) ('a', 'c') >>> first(ser), last(ser) ('a', 'c') \"\"\" if default is not None : raise NotImplementedError ( \"default argument not implemented\" ) # check indexing is in range, handles positive and negative cases. # TODO: is returning None the correct behavior for an empty Series? if n >= len ( x ) or abs ( n ) > len ( x ): return default if order_by is None : return x . iloc [ n ] # case where order_by is specified and n in range ---- # TODO: ensure order_by is arraylike if not isinstance ( order_by , pd . Series ): raise NotImplementedError ( \"order_by argument is type %s , but currently only\" \"implemented for Series\" % type ( order_by ) ) if len ( x ) != len ( order_by ): raise ValueError ( \"x and order_by arguments must be same length\" ) order_indx = order_by . reset_index ( drop = True ) . sort_values () . index return x . iloc [ order_indx [ n ]]","title":"nth()"},{"location":"vector/#siuba.dply.vector.ntile","text":"TODO: Not Implemented Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def ntile ( x , n ): \"\"\"TODO: Not Implemented\"\"\" raise NotImplementedError ( \"ntile not implemented\" )","title":"ntile()"},{"location":"vector/#siuba.dply.vector.percent_rank","text":"Return the percent rank. Note: Uses minimum rank, and reports the proportion of unique ranks each entry is greater than. Examples: >>> percent_rank ( pd . Series ([ 1 , 2 , 3 ])) 0 0.0 1 0.5 2 1.0 dtype : float64 >>> percent_rank ( pd . Series ([ 1 , 2 , 2 ])) 0 0.0 1 0.5 2 0.5 dtype : float64 >>> percent_rank ( pd . Series ([ 1 ])) 0 NaN dtype : float64 Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = Series ) def percent_rank ( x , na_option = \"keep\" ): \"\"\"Return the percent rank. Note: Uses minimum rank, and reports the proportion of unique ranks each entry is greater than. Examples: >>> percent_rank(pd.Series([1, 2, 3])) 0 0.0 1 0.5 2 1.0 dtype: float64 >>> percent_rank(pd.Series([1, 2, 2])) 0 0.0 1 0.5 2 0.5 dtype: float64 >>> percent_rank(pd.Series([1])) 0 NaN dtype: float64 \"\"\" return ( min_rank ( x ) - 1 ) / ( x . count () - 1 )","title":"percent_rank()"},{"location":"vector/#siuba.dply.vector.row_number","text":"Return the row number (position) for each value in x, beginning with 1. Examples: >>> ser = pd . Series ([ 7 , 8 ]) >>> row_number ( ser ) 0 1 1 2 dtype : int64 >>> row_number ( pd . DataFrame ({ 'a' : ser })) 0 1 1 2 dtype : int64 >>> row_number ( pd . Series ([ 7 , 8 ], index = [ 3 , 4 ])) 3 1 4 2 dtype : int64 Source code in siuba/dply/vector.py @symbolic_dispatch ( cls = NDFrame ) def row_number ( x ): \"\"\"Return the row number (position) for each value in x, beginning with 1. Example: >>> ser = pd.Series([7,8]) >>> row_number(ser) 0 1 1 2 dtype: int64 >>> row_number(pd.DataFrame({'a': ser})) 0 1 1 2 dtype: int64 >>> row_number(pd.Series([7,8], index = [3, 4])) 3 1 4 2 dtype: int64 \"\"\" if isinstance ( x , pd . DataFrame ): n = x . shape [ 0 ] else : n = len ( x ) arr = np . arange ( 1 , n + 1 ) # could use single dispatch, but for now ensure output data type matches input if isinstance ( x , pd . Series ): return x . _constructor ( arr , x . index , fastpath = True ) return pd . Series ( arr , x . index , fastpath = True )","title":"row_number()"},{"location":"vector/#conditionals","text":"Note that these functions currently can be imported from the top level: from siuba import case_when, if_else They will be moved into the siuba.dply.vector module, but the above import will continue to work for backwards compatibility.","title":"Conditionals"},{"location":"vector/#siuba.dply.verbs.case_when","text":"Generalized, vectorized if statement. Parameters: Name Type Description Default __data The input data. required cases dict A mapping of condition : value. required Examples: >>> import pandas as pd >>> from siuba import _ , case_when >>> df = pd . DataFrame ({ \"x\" : [ 1 , 2 , 3 ]}) >>> case_when ( df , { _ . x == 1 : \"one\" , _ . x == 2 : \"two\" }) 0 one 1 two 2 None dtype : object >>> df >> case_when ({ _ . x == 1 : \"one\" , _ . x == 2 : \"two\" }) 0 one 1 two 2 None dtype : object >>> df >> case_when ({ _ . x == 1 : \"one\" , _ . x == 2 : \"two\" , True : \"other\" }) 0 one 1 two 2 other dtype : object Source code in siuba/dply/verbs.py @singledispatch2 (( pd . DataFrame , pd . Series )) def case_when ( __data , cases : dict ): \"\"\"Generalized, vectorized if statement. Parameters ---------- __data: The input data. cases: dict A mapping of condition : value. See Also -------- if_else : Handles the special case of two conditions. Examples -------- >>> import pandas as pd >>> from siuba import _, case_when >>> df = pd.DataFrame({\"x\": [1, 2, 3]}) >>> case_when(df, {_.x == 1: \"one\", _.x == 2: \"two\"}) 0 one 1 two 2 None dtype: object >>> df >> case_when({_.x == 1: \"one\", _.x == 2: \"two\"}) 0 one 1 two 2 None dtype: object >>> df >> case_when({_.x == 1: \"one\", _.x == 2: \"two\", True: \"other\"}) 0 one 1 two 2 other dtype: object \"\"\" if isinstance ( cases , Call ): cases = cases ( __data ) # TODO: handle when receive list of (k,v) pairs for py < 3.5 compat? stripped_cases = { strip_symbolic ( k ): strip_symbolic ( v ) for k , v in cases . items ()} n = len ( __data ) out = np . repeat ( None , n ) for k , v in reversed ( list ( stripped_cases . items ())): if callable ( k ): result = _val_call ( k , __data , n ) indx = np . where ( result )[ 0 ] val_res = _val_call ( v , __data , n , indx ) out [ indx ] = val_res elif k : # e.g. k is just True, etc.. val_res = _val_call ( v , __data , n ) out [:] = val_res # by recreating an array, attempts to cast as best dtype return pd . Series ( list ( out ))","title":"case_when()"},{"location":"vector/#siuba.dply.verbs.if_else","text":"Parameters: Name Type Description Default condition Logical vector (or lazy expression). required true Values to be used when condition is True. required false Values to be used when condition is False. required Examples: >>> ser1 = pd . Series ([ 1 , 2 , 3 ]) >>> if_else ( ser1 > 2 , np . nan , ser1 ) 0 1.0 1 2.0 2 NaN dtype : float64 >>> from siuba import _ >>> f = if_else ( _ < 2 , _ , 2 ) >>> f ( ser1 ) 0 1 1 2 2 2 dtype : int64 >>> import numpy as np >>> ser2 = pd . Series ([ 'NA' , 'a' , 'b' ]) >>> if_else ( ser2 == 'NA' , np . nan , ser2 ) 0 NaN 1 a 2 b dtype : object Source code in siuba/dply/verbs.py @singledispatch def if_else ( condition , true , false ): \"\"\" Parameters ---------- condition: Logical vector (or lazy expression). true: Values to be used when condition is True. false: Values to be used when condition is False. See Also -------- case_when : Generalized if_else, for handling many cases. Examples -------- >>> ser1 = pd.Series([1,2,3]) >>> if_else(ser1 > 2, np.nan, ser1) 0 1.0 1 2.0 2 NaN dtype: float64 >>> from siuba import _ >>> f = if_else(_ < 2, _, 2) >>> f(ser1) 0 1 1 2 2 2 dtype: int64 >>> import numpy as np >>> ser2 = pd.Series(['NA', 'a', 'b']) >>> if_else(ser2 == 'NA', np.nan, ser2) 0 NaN 1 a 2 b dtype: object \"\"\" raise_type_error ( condition )","title":"if_else()"},{"location":"verb-add_count/","text":"add_count ( __data , * args , * , wt = None , sort = False , ** kwargs ) Add a column that is the number of observations for each grouping of data. Note that this function is similar to count(), but does not aggregate. It's useful combined with filter(). Parameters: Name Type Description Default __data A DataFrame. required *args The names of columns to be used for grouping. Passed to group_by. () wt The name of a column to use as a weighted for each row. None sort Whether to sort the results in descending order. False **kwargs Creates a new named column, and uses for grouping. Passed to group_by. {} Examples: >>> import pandas as pd >>> from siuba import _ , add_count , group_by , ungroup , mutate >>> from siuba.data import mtcars >>> df = pd . DataFrame ({ \"x\" : [ \"a\" , \"a\" , \"b\" ], \"y\" : [ 1 , 2 , 3 ]}) >>> df >> add_count ( _ . x ) x y n 0 a 1 2 1 a 2 2 2 b 3 1 This is useful if you want to see data associated with some count: >>> df >> add_count ( _ . x ) >> filter ( _ . n == 1 ) x y n 2 b 3 1 Note that add_count is equivalent to a grouped mutate: >>> df >> group_by ( _ . x ) >> mutate ( n = _ . shape [ 0 ]) >> ungroup () x y n 0 a 1 2 1 a 2 2 2 b 3 1 Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def add_count ( __data , * args , wt = None , sort = False , ** kwargs ): \"\"\"Add a column that is the number of observations for each grouping of data. Note that this function is similar to count(), but does not aggregate. It's useful combined with filter(). Parameters ---------- __data: A DataFrame. *args: The names of columns to be used for grouping. Passed to group_by. wt: The name of a column to use as a weighted for each row. sort: Whether to sort the results in descending order. **kwargs: Creates a new named column, and uses for grouping. Passed to group_by. Examples -------- >>> import pandas as pd >>> from siuba import _, add_count, group_by, ungroup, mutate >>> from siuba.data import mtcars >>> df = pd.DataFrame({\"x\": [\"a\", \"a\", \"b\"], \"y\": [1, 2, 3]}) >>> df >> add_count(_.x) x y n 0 a 1 2 1 a 2 2 2 b 3 1 This is useful if you want to see data associated with some count: >>> df >> add_count(_.x) >> filter(_.n == 1) x y n 2 b 3 1 Note that add_count is equivalent to a grouped mutate: >>> df >> group_by(_.x) >> mutate(n = _.shape[0]) >> ungroup() x y n 0 a 1 2 1 a 2 2 2 b 3 1 \"\"\" counts = count ( __data , * args , wt = wt , sort = sort , ** kwargs ) on = list ( counts . columns )[: - 1 ] return __data . merge ( counts , on = on )","title":"Verb add count"},{"location":"verb-add_count/#siuba.dply.verbs.add_count","text":"Add a column that is the number of observations for each grouping of data. Note that this function is similar to count(), but does not aggregate. It's useful combined with filter(). Parameters: Name Type Description Default __data A DataFrame. required *args The names of columns to be used for grouping. Passed to group_by. () wt The name of a column to use as a weighted for each row. None sort Whether to sort the results in descending order. False **kwargs Creates a new named column, and uses for grouping. Passed to group_by. {} Examples: >>> import pandas as pd >>> from siuba import _ , add_count , group_by , ungroup , mutate >>> from siuba.data import mtcars >>> df = pd . DataFrame ({ \"x\" : [ \"a\" , \"a\" , \"b\" ], \"y\" : [ 1 , 2 , 3 ]}) >>> df >> add_count ( _ . x ) x y n 0 a 1 2 1 a 2 2 2 b 3 1 This is useful if you want to see data associated with some count: >>> df >> add_count ( _ . x ) >> filter ( _ . n == 1 ) x y n 2 b 3 1 Note that add_count is equivalent to a grouped mutate: >>> df >> group_by ( _ . x ) >> mutate ( n = _ . shape [ 0 ]) >> ungroup () x y n 0 a 1 2 1 a 2 2 2 b 3 1 Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def add_count ( __data , * args , wt = None , sort = False , ** kwargs ): \"\"\"Add a column that is the number of observations for each grouping of data. Note that this function is similar to count(), but does not aggregate. It's useful combined with filter(). Parameters ---------- __data: A DataFrame. *args: The names of columns to be used for grouping. Passed to group_by. wt: The name of a column to use as a weighted for each row. sort: Whether to sort the results in descending order. **kwargs: Creates a new named column, and uses for grouping. Passed to group_by. Examples -------- >>> import pandas as pd >>> from siuba import _, add_count, group_by, ungroup, mutate >>> from siuba.data import mtcars >>> df = pd.DataFrame({\"x\": [\"a\", \"a\", \"b\"], \"y\": [1, 2, 3]}) >>> df >> add_count(_.x) x y n 0 a 1 2 1 a 2 2 2 b 3 1 This is useful if you want to see data associated with some count: >>> df >> add_count(_.x) >> filter(_.n == 1) x y n 2 b 3 1 Note that add_count is equivalent to a grouped mutate: >>> df >> group_by(_.x) >> mutate(n = _.shape[0]) >> ungroup() x y n 0 a 1 2 1 a 2 2 2 b 3 1 \"\"\" counts = count ( __data , * args , wt = wt , sort = sort , ** kwargs ) on = list ( counts . columns )[: - 1 ] return __data . merge ( counts , on = on )","title":"add_count()"},{"location":"verb-anti_join/","text":"anti_join ( left , right = None , on = None ) Return the left table with every row that would not be kept in an inner join. Parameters: Name Type Description Default left The left-hand table. required right The right-hand table. None on How to match them. By default it uses matches all columns with the same name across the two tables. None Examples: >>> import pandas as pd >>> from siuba import _ , semi_join , anti_join >>> df1 = pd . DataFrame ({ \"id\" : [ 1 , 2 , 3 ], \"x\" : [ \"a\" , \"b\" , \"c\" ]}) >>> df2 = pd . DataFrame ({ \"id\" : [ 2 , 3 , 3 ], \"y\" : [ \"l\" , \"m\" , \"n\" ]}) >>> df1 >> semi_join ( _ , df2 ) id x 1 2 b 2 3 c >>> df1 >> anti_join ( _ , df2 ) id x 0 1 a Generally, it's a good idea to explicitly specify the on argument. >>> df1 >> anti_join ( _ , df2 , on = \"id\" ) id x 0 1 a Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def anti_join ( left , right = None , on = None ): \"\"\"Return the left table with every row that would *not* be kept in an inner join. Parameters ---------- left : The left-hand table. right : The right-hand table. on : How to match them. By default it uses matches all columns with the same name across the two tables. Examples -------- >>> import pandas as pd >>> from siuba import _, semi_join, anti_join >>> df1 = pd.DataFrame({\"id\": [1, 2, 3], \"x\": [\"a\", \"b\", \"c\"]}) >>> df2 = pd.DataFrame({\"id\": [2, 3, 3], \"y\": [\"l\", \"m\", \"n\"]}) >>> df1 >> semi_join(_, df2) id x 1 2 b 2 3 c >>> df1 >> anti_join(_, df2) id x 0 1 a Generally, it's a good idea to explicitly specify the on argument. >>> df1 >> anti_join(_, df2, on=\"id\") id x 0 1 a \"\"\" # copied from semi_join if isinstance ( on , Mapping ): left_on , right_on = zip ( * on . items ()) else : left_on = right_on = on # manually perform merge, up to getting pieces need for indexing merger = _MergeOperation ( left , right , left_on = left_on , right_on = right_on ) _ , l_indx , _ = merger . _get_join_info () # use the left table's indexer to exclude those rows range_indx = pd . RangeIndex ( len ( left )) return left . iloc [ range_indx . difference ( l_indx ),:]","title":"Verb anti join"},{"location":"verb-anti_join/#siuba.dply.verbs.anti_join","text":"Return the left table with every row that would not be kept in an inner join. Parameters: Name Type Description Default left The left-hand table. required right The right-hand table. None on How to match them. By default it uses matches all columns with the same name across the two tables. None Examples: >>> import pandas as pd >>> from siuba import _ , semi_join , anti_join >>> df1 = pd . DataFrame ({ \"id\" : [ 1 , 2 , 3 ], \"x\" : [ \"a\" , \"b\" , \"c\" ]}) >>> df2 = pd . DataFrame ({ \"id\" : [ 2 , 3 , 3 ], \"y\" : [ \"l\" , \"m\" , \"n\" ]}) >>> df1 >> semi_join ( _ , df2 ) id x 1 2 b 2 3 c >>> df1 >> anti_join ( _ , df2 ) id x 0 1 a Generally, it's a good idea to explicitly specify the on argument. >>> df1 >> anti_join ( _ , df2 , on = \"id\" ) id x 0 1 a Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def anti_join ( left , right = None , on = None ): \"\"\"Return the left table with every row that would *not* be kept in an inner join. Parameters ---------- left : The left-hand table. right : The right-hand table. on : How to match them. By default it uses matches all columns with the same name across the two tables. Examples -------- >>> import pandas as pd >>> from siuba import _, semi_join, anti_join >>> df1 = pd.DataFrame({\"id\": [1, 2, 3], \"x\": [\"a\", \"b\", \"c\"]}) >>> df2 = pd.DataFrame({\"id\": [2, 3, 3], \"y\": [\"l\", \"m\", \"n\"]}) >>> df1 >> semi_join(_, df2) id x 1 2 b 2 3 c >>> df1 >> anti_join(_, df2) id x 0 1 a Generally, it's a good idea to explicitly specify the on argument. >>> df1 >> anti_join(_, df2, on=\"id\") id x 0 1 a \"\"\" # copied from semi_join if isinstance ( on , Mapping ): left_on , right_on = zip ( * on . items ()) else : left_on = right_on = on # manually perform merge, up to getting pieces need for indexing merger = _MergeOperation ( left , right , left_on = left_on , right_on = right_on ) _ , l_indx , _ = merger . _get_join_info () # use the left table's indexer to exclude those rows range_indx = pd . RangeIndex ( len ( left )) return left . iloc [ range_indx . difference ( l_indx ),:]","title":"anti_join()"},{"location":"verb-arrange/","text":"arrange ( __data , * args ) Re-order the rows of a DataFrame using the values of specified columns. Parameters: Name Type Description Default __data The input table. required *args Columns or expressions used to sort the rows. () Examples: >>> import pandas as pd >>> from siuba import _ , arrange , mutate >>> df = pd . DataFrame ({ \"x\" : [ 2 , 1 , 1 ], \"y\" : [ \"aa\" , \"b\" , \"aa\" ]}) >>> df x y 0 2 aa 1 1 b 2 1 aa Arrange sorts on the first argument, then the second, etc.. >>> df >> arrange ( _ . x , _ . y ) x y 2 1 aa 1 1 b 0 2 aa Use a minus sign ( - ) to sort is descending order. >>> df >> arrange ( - _ . x ) x y 0 2 aa 1 1 b 2 1 aa Note that arrange can sort on complex expressions: >>> df >> arrange ( - _ . y . str . len ()) x y 0 2 aa 2 1 aa 1 1 b The case above is equivalent to running a mutate before arrange: >>> df >> mutate ( res = - _ . y . str . len ()) >> arrange ( _ . res ) x y res 0 2 aa - 2 2 1 aa - 2 1 1 b - 1 Source code in siuba/dply/verbs.py @singledispatch2 ( DataFrame ) def arrange ( __data , * args ): \"\"\"Re-order the rows of a DataFrame using the values of specified columns. Parameters ---------- __data: The input table. *args: Columns or expressions used to sort the rows. Examples -------- >>> import pandas as pd >>> from siuba import _, arrange, mutate >>> df = pd.DataFrame({\"x\": [2, 1, 1], \"y\": [\"aa\", \"b\", \"aa\"]}) >>> df x y 0 2 aa 1 1 b 2 1 aa Arrange sorts on the first argument, then the second, etc.. >>> df >> arrange(_.x, _.y) x y 2 1 aa 1 1 b 0 2 aa Use a minus sign (`-`) to sort is descending order. >>> df >> arrange(-_.x) x y 0 2 aa 1 1 b 2 1 aa Note that arrange can sort on complex expressions: >>> df >> arrange(-_.y.str.len()) x y 0 2 aa 2 1 aa 1 1 b The case above is equivalent to running a mutate before arrange: >>> df >> mutate(res = -_.y.str.len()) >> arrange(_.res) x y res 0 2 aa -2 2 1 aa -2 1 1 b -1 \"\"\" # TODO: # - add arguments to pass to sort_values (e.g. ascending, kind) # # basically need some (1) select behavior, (2) mutate-like behavior # df.sort_values is the obvious candidate, but only takes names, not expressions # to work around this, we make a shallow copy of data, and add sorting columns # then drop them at the end # # sort order is determined by using a unary w/ Call e.g. -_.repo df = __data . copy ( deep = False ) n_cols = len ( df . columns ) n_args = len ( args ) #kwargs = {n_cols + ii: arg for ii,arg in enumerate(args)} # TODO: more careful handling of arg types (true across library :/ ).. tmp_cols = [] sort_cols = [] ascending = [] for ii , arg in enumerate ( args ): f , asc = _call_strip_ascending ( arg ) ascending . append ( asc ) col = simple_varname ( f ) if col is not None : sort_cols . append ( col ) else : # TODO: could screw up if user has columns names that are ints... sort_cols . append ( n_cols + ii ) tmp_cols . append ( n_cols + ii ) df [ n_cols + ii ] = f ( df ) return df . sort_values ( by = sort_cols , kind = \"mergesort\" , ascending = ascending ) \\ . drop ( tmp_cols , axis = 1 )","title":"arrange"},{"location":"verb-arrange/#siuba.dply.verbs.arrange","text":"Re-order the rows of a DataFrame using the values of specified columns. Parameters: Name Type Description Default __data The input table. required *args Columns or expressions used to sort the rows. () Examples: >>> import pandas as pd >>> from siuba import _ , arrange , mutate >>> df = pd . DataFrame ({ \"x\" : [ 2 , 1 , 1 ], \"y\" : [ \"aa\" , \"b\" , \"aa\" ]}) >>> df x y 0 2 aa 1 1 b 2 1 aa Arrange sorts on the first argument, then the second, etc.. >>> df >> arrange ( _ . x , _ . y ) x y 2 1 aa 1 1 b 0 2 aa Use a minus sign ( - ) to sort is descending order. >>> df >> arrange ( - _ . x ) x y 0 2 aa 1 1 b 2 1 aa Note that arrange can sort on complex expressions: >>> df >> arrange ( - _ . y . str . len ()) x y 0 2 aa 2 1 aa 1 1 b The case above is equivalent to running a mutate before arrange: >>> df >> mutate ( res = - _ . y . str . len ()) >> arrange ( _ . res ) x y res 0 2 aa - 2 2 1 aa - 2 1 1 b - 1 Source code in siuba/dply/verbs.py @singledispatch2 ( DataFrame ) def arrange ( __data , * args ): \"\"\"Re-order the rows of a DataFrame using the values of specified columns. Parameters ---------- __data: The input table. *args: Columns or expressions used to sort the rows. Examples -------- >>> import pandas as pd >>> from siuba import _, arrange, mutate >>> df = pd.DataFrame({\"x\": [2, 1, 1], \"y\": [\"aa\", \"b\", \"aa\"]}) >>> df x y 0 2 aa 1 1 b 2 1 aa Arrange sorts on the first argument, then the second, etc.. >>> df >> arrange(_.x, _.y) x y 2 1 aa 1 1 b 0 2 aa Use a minus sign (`-`) to sort is descending order. >>> df >> arrange(-_.x) x y 0 2 aa 1 1 b 2 1 aa Note that arrange can sort on complex expressions: >>> df >> arrange(-_.y.str.len()) x y 0 2 aa 2 1 aa 1 1 b The case above is equivalent to running a mutate before arrange: >>> df >> mutate(res = -_.y.str.len()) >> arrange(_.res) x y res 0 2 aa -2 2 1 aa -2 1 1 b -1 \"\"\" # TODO: # - add arguments to pass to sort_values (e.g. ascending, kind) # # basically need some (1) select behavior, (2) mutate-like behavior # df.sort_values is the obvious candidate, but only takes names, not expressions # to work around this, we make a shallow copy of data, and add sorting columns # then drop them at the end # # sort order is determined by using a unary w/ Call e.g. -_.repo df = __data . copy ( deep = False ) n_cols = len ( df . columns ) n_args = len ( args ) #kwargs = {n_cols + ii: arg for ii,arg in enumerate(args)} # TODO: more careful handling of arg types (true across library :/ ).. tmp_cols = [] sort_cols = [] ascending = [] for ii , arg in enumerate ( args ): f , asc = _call_strip_ascending ( arg ) ascending . append ( asc ) col = simple_varname ( f ) if col is not None : sort_cols . append ( col ) else : # TODO: could screw up if user has columns names that are ints... sort_cols . append ( n_cols + ii ) tmp_cols . append ( n_cols + ii ) df [ n_cols + ii ] = f ( df ) return df . sort_values ( by = sort_cols , kind = \"mergesort\" , ascending = ascending ) \\ . drop ( tmp_cols , axis = 1 )","title":"arrange()"},{"location":"verb-collect/","text":"collect ( __data , * args , ** kwargs ) Retrieve data as a local DataFrame. Source code in siuba/dply/verbs.py @pipe_no_args @singledispatch2 (( DataFrame , DataFrameGroupBy )) def collect ( __data , * args , ** kwargs ): \"\"\"Retrieve data as a local DataFrame. \"\"\" # simply return DataFrame, since requires no execution return __data","title":"collect"},{"location":"verb-collect/#siuba.dply.verbs.collect","text":"Retrieve data as a local DataFrame. Source code in siuba/dply/verbs.py @pipe_no_args @singledispatch2 (( DataFrame , DataFrameGroupBy )) def collect ( __data , * args , ** kwargs ): \"\"\"Retrieve data as a local DataFrame. \"\"\" # simply return DataFrame, since requires no execution return __data","title":"collect()"},{"location":"verb-complete/","text":"complete ( __data , * args , * , fill = None ) Add rows to fill in missing combinations in the data. This is a wrapper around expand(), right_join(), along with filling NAs. Parameters: Name Type Description Default __data The input data. required *args Columns to cross and expand. () fill A dictionary specifying what to use for missing values in each column. If a column is not specified, missing values are left as is. None Examples: >>> import pandas as pd >>> from siuba import _ , expand , count , anti_join , right_join >>> df = pd . DataFrame ({ \"x\" : [ 1 , 2 , 2 ], \"y\" : [ \"a\" , \"a\" , \"b\" ], \"z\" : 1 }) >>> df x y z 0 1 a 1 1 2 a 1 2 2 b 1 >>> df >> complete ( _ . x , _ . y ) x y z 0 1.0 a 1.0 1 1 b NaN 2 2.0 a 1.0 3 2.0 b 1.0 Use the fill argument to replace missing values: >>> df >> complete ( _ . x , _ . y , fill = { \"z\" : 999 }) x y z 0 1.0 a 1.0 1 1 b 999.0 2 2.0 a 1.0 3 2.0 b 1.0 A common use of complete is to make zero counts explicit (e.g. for charting): >>> df >> count ( _ . x , _ . y ) >> complete ( _ . x , _ . y , fill = { \"n\" : 0 }) x y n 0 1.0 a 1.0 1 1 b 0.0 2 2.0 a 1.0 3 2.0 b 1.0 Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def complete ( __data , * args , fill = None ): \"\"\"Add rows to fill in missing combinations in the data. This is a wrapper around expand(), right_join(), along with filling NAs. Parameters ---------- __data: The input data. *args: Columns to cross and expand. fill: A dictionary specifying what to use for missing values in each column. If a column is not specified, missing values are left as is. Examples -------- >>> import pandas as pd >>> from siuba import _, expand, count, anti_join, right_join >>> df = pd.DataFrame({\"x\": [1, 2, 2], \"y\": [\"a\", \"a\", \"b\"], \"z\": 1}) >>> df x y z 0 1 a 1 1 2 a 1 2 2 b 1 >>> df >> complete(_.x, _.y) x y z 0 1.0 a 1.0 1 1 b NaN 2 2.0 a 1.0 3 2.0 b 1.0 Use the fill argument to replace missing values: >>> df >> complete(_.x, _.y, fill={\"z\": 999}) x y z 0 1.0 a 1.0 1 1 b 999.0 2 2.0 a 1.0 3 2.0 b 1.0 A common use of complete is to make zero counts explicit (e.g. for charting): >>> df >> count(_.x, _.y) >> complete(_.x, _.y, fill={\"n\": 0}) x y n 0 1.0 a 1.0 1 1 b 0.0 2 2.0 a 1.0 3 2.0 b 1.0 \"\"\" expanded = expand ( __data , * args , fill = fill ) # TODO: should we attempt to coerce cols back to original types? # e.g. NAs will turn int -> float on_cols = list ( expanded . columns ) df = __data . merge ( expanded , how = \"right\" , on = on_cols ) if fill is not None : for col_name , val in fill . items (): df [ col_name ] . fillna ( val , inplace = True ) return df","title":"complete"},{"location":"verb-complete/#siuba.dply.verbs.complete","text":"Add rows to fill in missing combinations in the data. This is a wrapper around expand(), right_join(), along with filling NAs. Parameters: Name Type Description Default __data The input data. required *args Columns to cross and expand. () fill A dictionary specifying what to use for missing values in each column. If a column is not specified, missing values are left as is. None Examples: >>> import pandas as pd >>> from siuba import _ , expand , count , anti_join , right_join >>> df = pd . DataFrame ({ \"x\" : [ 1 , 2 , 2 ], \"y\" : [ \"a\" , \"a\" , \"b\" ], \"z\" : 1 }) >>> df x y z 0 1 a 1 1 2 a 1 2 2 b 1 >>> df >> complete ( _ . x , _ . y ) x y z 0 1.0 a 1.0 1 1 b NaN 2 2.0 a 1.0 3 2.0 b 1.0 Use the fill argument to replace missing values: >>> df >> complete ( _ . x , _ . y , fill = { \"z\" : 999 }) x y z 0 1.0 a 1.0 1 1 b 999.0 2 2.0 a 1.0 3 2.0 b 1.0 A common use of complete is to make zero counts explicit (e.g. for charting): >>> df >> count ( _ . x , _ . y ) >> complete ( _ . x , _ . y , fill = { \"n\" : 0 }) x y n 0 1.0 a 1.0 1 1 b 0.0 2 2.0 a 1.0 3 2.0 b 1.0 Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def complete ( __data , * args , fill = None ): \"\"\"Add rows to fill in missing combinations in the data. This is a wrapper around expand(), right_join(), along with filling NAs. Parameters ---------- __data: The input data. *args: Columns to cross and expand. fill: A dictionary specifying what to use for missing values in each column. If a column is not specified, missing values are left as is. Examples -------- >>> import pandas as pd >>> from siuba import _, expand, count, anti_join, right_join >>> df = pd.DataFrame({\"x\": [1, 2, 2], \"y\": [\"a\", \"a\", \"b\"], \"z\": 1}) >>> df x y z 0 1 a 1 1 2 a 1 2 2 b 1 >>> df >> complete(_.x, _.y) x y z 0 1.0 a 1.0 1 1 b NaN 2 2.0 a 1.0 3 2.0 b 1.0 Use the fill argument to replace missing values: >>> df >> complete(_.x, _.y, fill={\"z\": 999}) x y z 0 1.0 a 1.0 1 1 b 999.0 2 2.0 a 1.0 3 2.0 b 1.0 A common use of complete is to make zero counts explicit (e.g. for charting): >>> df >> count(_.x, _.y) >> complete(_.x, _.y, fill={\"n\": 0}) x y n 0 1.0 a 1.0 1 1 b 0.0 2 2.0 a 1.0 3 2.0 b 1.0 \"\"\" expanded = expand ( __data , * args , fill = fill ) # TODO: should we attempt to coerce cols back to original types? # e.g. NAs will turn int -> float on_cols = list ( expanded . columns ) df = __data . merge ( expanded , how = \"right\" , on = on_cols ) if fill is not None : for col_name , val in fill . items (): df [ col_name ] . fillna ( val , inplace = True ) return df","title":"complete()"},{"location":"verb-count/","text":"count ( __data , * args , * , wt = None , sort = False , ** kwargs ) Summarize data with the number of rows for each grouping of data. Parameters: Name Type Description Default __data A DataFrame. required *args The names of columns to be used for grouping. Passed to group_by. () wt The name of a column to use as a weighted for each row. None sort Whether to sort the results in descending order. False **kwargs Creates a new named column, and uses for grouping. Passed to group_by. {} Examples: >>> from siuba import _ , count , group_by , summarize , arrange >>> from siuba.data import mtcars >>> count ( mtcars , _ . cyl , high_mpg = _ . mpg > 30 ) cyl high_mpg n 0 4 False 7 1 4 True 4 2 6 False 7 3 8 False 14 Use sort to order results by number of observations (in descending order). >>> count ( mtcars , _ . cyl , sort = True ) cyl n 0 8 14 1 4 11 2 6 7 count is equivalent to doing a grouped summarize: >>> mtcars >> group_by ( _ . cyl ) >> summarize ( n = _ . shape [ 0 ]) >> arrange ( - _ . n ) cyl n 2 8 14 0 4 11 1 6 7 Source code in siuba/dply/verbs.py @singledispatch2 (( pd . DataFrame , DataFrameGroupBy )) def count ( __data , * args , wt = None , sort = False , ** kwargs ): \"\"\"Summarize data with the number of rows for each grouping of data. Parameters ---------- __data: A DataFrame. *args: The names of columns to be used for grouping. Passed to group_by. wt: The name of a column to use as a weighted for each row. sort: Whether to sort the results in descending order. **kwargs: Creates a new named column, and uses for grouping. Passed to group_by. Examples -------- >>> from siuba import _, count, group_by, summarize, arrange >>> from siuba.data import mtcars >>> count(mtcars, _.cyl, high_mpg = _.mpg > 30) cyl high_mpg n 0 4 False 7 1 4 True 4 2 6 False 7 3 8 False 14 Use sort to order results by number of observations (in descending order). >>> count(mtcars, _.cyl, sort=True) cyl n 0 8 14 1 4 11 2 6 7 count is equivalent to doing a grouped summarize: >>> mtcars >> group_by(_.cyl) >> summarize(n = _.shape[0]) >> arrange(-_.n) cyl n 2 8 14 0 4 11 1 6 7 \"\"\" no_grouping_vars = not args and not kwargs and isinstance ( __data , pd . DataFrame ) if wt is None : if no_grouping_vars : # no groups, just use number of rows counts = pd . DataFrame ({ 'tmp' : [ __data . shape [ 0 ]]}) else : # tally rows for each group counts = group_by ( __data , * args , add = True , ** kwargs ) . size () . reset_index () else : wt_col = simple_varname ( wt ) if wt_col is None : raise Exception ( \"wt argument has to be simple column name\" ) if no_grouping_vars : # no groups, sum weights counts = pd . DataFrame ({ 'tmp' : [ __data [ wt_col ] . sum ()]}) else : # do weighted tally counts = group_by ( __data , * args , add = True , ** kwargs )[ wt_col ] . sum () . reset_index () # count col named, n. If that col already exists, add more \"n\"s... crnt_cols = set ( counts . columns ) out_col = \"n\" while out_col in crnt_cols : out_col = out_col + \"n\" # rename the tally column to correct name counts . rename ( columns = { counts . columns [ - 1 ]: out_col }, inplace = True ) if sort : return counts . sort_values ( out_col , ascending = False ) . reset_index ( drop = True ) return counts","title":"count"},{"location":"verb-count/#siuba.dply.verbs.count","text":"Summarize data with the number of rows for each grouping of data. Parameters: Name Type Description Default __data A DataFrame. required *args The names of columns to be used for grouping. Passed to group_by. () wt The name of a column to use as a weighted for each row. None sort Whether to sort the results in descending order. False **kwargs Creates a new named column, and uses for grouping. Passed to group_by. {} Examples: >>> from siuba import _ , count , group_by , summarize , arrange >>> from siuba.data import mtcars >>> count ( mtcars , _ . cyl , high_mpg = _ . mpg > 30 ) cyl high_mpg n 0 4 False 7 1 4 True 4 2 6 False 7 3 8 False 14 Use sort to order results by number of observations (in descending order). >>> count ( mtcars , _ . cyl , sort = True ) cyl n 0 8 14 1 4 11 2 6 7 count is equivalent to doing a grouped summarize: >>> mtcars >> group_by ( _ . cyl ) >> summarize ( n = _ . shape [ 0 ]) >> arrange ( - _ . n ) cyl n 2 8 14 0 4 11 1 6 7 Source code in siuba/dply/verbs.py @singledispatch2 (( pd . DataFrame , DataFrameGroupBy )) def count ( __data , * args , wt = None , sort = False , ** kwargs ): \"\"\"Summarize data with the number of rows for each grouping of data. Parameters ---------- __data: A DataFrame. *args: The names of columns to be used for grouping. Passed to group_by. wt: The name of a column to use as a weighted for each row. sort: Whether to sort the results in descending order. **kwargs: Creates a new named column, and uses for grouping. Passed to group_by. Examples -------- >>> from siuba import _, count, group_by, summarize, arrange >>> from siuba.data import mtcars >>> count(mtcars, _.cyl, high_mpg = _.mpg > 30) cyl high_mpg n 0 4 False 7 1 4 True 4 2 6 False 7 3 8 False 14 Use sort to order results by number of observations (in descending order). >>> count(mtcars, _.cyl, sort=True) cyl n 0 8 14 1 4 11 2 6 7 count is equivalent to doing a grouped summarize: >>> mtcars >> group_by(_.cyl) >> summarize(n = _.shape[0]) >> arrange(-_.n) cyl n 2 8 14 0 4 11 1 6 7 \"\"\" no_grouping_vars = not args and not kwargs and isinstance ( __data , pd . DataFrame ) if wt is None : if no_grouping_vars : # no groups, just use number of rows counts = pd . DataFrame ({ 'tmp' : [ __data . shape [ 0 ]]}) else : # tally rows for each group counts = group_by ( __data , * args , add = True , ** kwargs ) . size () . reset_index () else : wt_col = simple_varname ( wt ) if wt_col is None : raise Exception ( \"wt argument has to be simple column name\" ) if no_grouping_vars : # no groups, sum weights counts = pd . DataFrame ({ 'tmp' : [ __data [ wt_col ] . sum ()]}) else : # do weighted tally counts = group_by ( __data , * args , add = True , ** kwargs )[ wt_col ] . sum () . reset_index () # count col named, n. If that col already exists, add more \"n\"s... crnt_cols = set ( counts . columns ) out_col = \"n\" while out_col in crnt_cols : out_col = out_col + \"n\" # rename the tally column to correct name counts . rename ( columns = { counts . columns [ - 1 ]: out_col }, inplace = True ) if sort : return counts . sort_values ( out_col , ascending = False ) . reset_index ( drop = True ) return counts","title":"count()"},{"location":"verb-distinct/","text":"distinct ( __data , * args , * , _keep_all = False , ** kwargs ) Keep only distinct (unique) rows from a table. Parameters: Name Type Description Default __data The input data. required *args Columns to use when determining which rows are unique. () _keep_all Whether to keep all columns of the original data, not just *args. False **kwargs If specified, arguments passed to the verb mutate(), and then being used in distinct(). {} Examples: >>> from siuba import _ , distinct , select >>> from siuba.data import penguins >>> penguins >> distinct ( _ . species , _ . island ) species island 0 Adelie Torgersen 1 Adelie Biscoe 2 Adelie Dream 3 Gentoo Biscoe 4 Chinstrap Dream Use _keep_all=True, to keep all columns in each distinct row. This lets you peak at the values of the first unique row. >>> small_penguins = penguins >> select ( _ [: 4 ]) >>> small_penguins >> distinct ( _ . species , _keep_all = True ) species island bill_length_mm bill_depth_mm 0 Adelie Torgersen 39.1 18.7 1 Gentoo Biscoe 46.1 13.2 2 Chinstrap Dream 46.5 17.9 Source code in siuba/dply/verbs.py @singledispatch2 ( DataFrame ) def distinct ( __data , * args , _keep_all = False , ** kwargs ): \"\"\"Keep only distinct (unique) rows from a table. Parameters ---------- __data: The input data. *args: Columns to use when determining which rows are unique. _keep_all: Whether to keep all columns of the original data, not just *args. **kwargs: If specified, arguments passed to the verb mutate(), and then being used in distinct(). See Also -------- count : keep distinct rows, and count their number of observations. Examples -------- >>> from siuba import _, distinct, select >>> from siuba.data import penguins >>> penguins >> distinct(_.species, _.island) species island 0 Adelie Torgersen 1 Adelie Biscoe 2 Adelie Dream 3 Gentoo Biscoe 4 Chinstrap Dream Use _keep_all=True, to keep all columns in each distinct row. This lets you peak at the values of the first unique row. >>> small_penguins = penguins >> select(_[:4]) >>> small_penguins >> distinct(_.species, _keep_all = True) species island bill_length_mm bill_depth_mm 0 Adelie Torgersen 39.1 18.7 1 Gentoo Biscoe 46.1 13.2 2 Chinstrap Dream 46.5 17.9 \"\"\" # using dict as ordered set cols = { simple_varname ( x ): True for x in args } if None in cols : raise Exception ( \"positional arguments must be simple column, \" \"e.g. _.colname or _['colname']\" ) # mutate kwargs cols . update ( kwargs ) # special case: use all variables when none are specified if not len ( cols ): cols = __data . columns tmp_data = mutate ( __data , ** kwargs ) . drop_duplicates ( list ( cols )) . reset_index ( drop = True ) if not _keep_all : return tmp_data [ list ( cols )] return tmp_data","title":"distinct"},{"location":"verb-distinct/#siuba.dply.verbs.distinct","text":"Keep only distinct (unique) rows from a table. Parameters: Name Type Description Default __data The input data. required *args Columns to use when determining which rows are unique. () _keep_all Whether to keep all columns of the original data, not just *args. False **kwargs If specified, arguments passed to the verb mutate(), and then being used in distinct(). {} Examples: >>> from siuba import _ , distinct , select >>> from siuba.data import penguins >>> penguins >> distinct ( _ . species , _ . island ) species island 0 Adelie Torgersen 1 Adelie Biscoe 2 Adelie Dream 3 Gentoo Biscoe 4 Chinstrap Dream Use _keep_all=True, to keep all columns in each distinct row. This lets you peak at the values of the first unique row. >>> small_penguins = penguins >> select ( _ [: 4 ]) >>> small_penguins >> distinct ( _ . species , _keep_all = True ) species island bill_length_mm bill_depth_mm 0 Adelie Torgersen 39.1 18.7 1 Gentoo Biscoe 46.1 13.2 2 Chinstrap Dream 46.5 17.9 Source code in siuba/dply/verbs.py @singledispatch2 ( DataFrame ) def distinct ( __data , * args , _keep_all = False , ** kwargs ): \"\"\"Keep only distinct (unique) rows from a table. Parameters ---------- __data: The input data. *args: Columns to use when determining which rows are unique. _keep_all: Whether to keep all columns of the original data, not just *args. **kwargs: If specified, arguments passed to the verb mutate(), and then being used in distinct(). See Also -------- count : keep distinct rows, and count their number of observations. Examples -------- >>> from siuba import _, distinct, select >>> from siuba.data import penguins >>> penguins >> distinct(_.species, _.island) species island 0 Adelie Torgersen 1 Adelie Biscoe 2 Adelie Dream 3 Gentoo Biscoe 4 Chinstrap Dream Use _keep_all=True, to keep all columns in each distinct row. This lets you peak at the values of the first unique row. >>> small_penguins = penguins >> select(_[:4]) >>> small_penguins >> distinct(_.species, _keep_all = True) species island bill_length_mm bill_depth_mm 0 Adelie Torgersen 39.1 18.7 1 Gentoo Biscoe 46.1 13.2 2 Chinstrap Dream 46.5 17.9 \"\"\" # using dict as ordered set cols = { simple_varname ( x ): True for x in args } if None in cols : raise Exception ( \"positional arguments must be simple column, \" \"e.g. _.colname or _['colname']\" ) # mutate kwargs cols . update ( kwargs ) # special case: use all variables when none are specified if not len ( cols ): cols = __data . columns tmp_data = mutate ( __data , ** kwargs ) . drop_duplicates ( list ( cols )) . reset_index ( drop = True ) if not _keep_all : return tmp_data [ list ( cols )] return tmp_data","title":"distinct()"},{"location":"verb-expand/","text":"expand ( __data , * args , * , fill = None ) Return table with unique crossings of specified columns. Parameters: Name Type Description Default __data The input data. required *args Column names to cross and de-duplicate. () Examples: >>> import pandas as pd >>> from siuba import _ , expand , count , anti_join , right_join >>> df = pd . DataFrame ({ \"x\" : [ 1 , 2 , 2 ], \"y\" : [ \"a\" , \"a\" , \"b\" ], \"z\" : 1 }) >>> df x y z 0 1 a 1 1 2 a 1 2 2 b 1 >>> combos = df >> expand ( _ . x , _ . y ) >>> combos x y 0 1 a 1 1 b 2 2 a 3 2 b >>> df >> right_join ( _ , combos ) x y z 0 1.0 a 1.0 1 1 b NaN 2 2.0 a 1.0 3 2.0 b 1.0 >>> combos >> anti_join ( _ , df ) x y 1 1 b Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def expand ( __data , * args , fill = None ): \"\"\"Return table with unique crossings of specified columns. Parameters ---------- __data: The input data. *args: Column names to cross and de-duplicate. Examples -------- >>> import pandas as pd >>> from siuba import _, expand, count, anti_join, right_join >>> df = pd.DataFrame({\"x\": [1, 2, 2], \"y\": [\"a\", \"a\", \"b\"], \"z\": 1}) >>> df x y z 0 1 a 1 1 2 a 1 2 2 b 1 >>> combos = df >> expand(_.x, _.y) >>> combos x y 0 1 a 1 1 b 2 2 a 3 2 b >>> df >> right_join(_, combos) x y z 0 1.0 a 1.0 1 1 b NaN 2 2.0 a 1.0 3 2.0 b 1.0 >>> combos >> anti_join(_, df) x y 1 1 b \"\"\" var_names = list ( map ( simple_varname , args )) cols = [ __data [ name ] . unique () for name in var_names ] # see https://stackoverflow.com/a/25636395/1144523 cprod = cartesian_product ( cols ) expanded = pd . DataFrame ( np . array ( cprod ) . T ) expanded . columns = var_names return expanded","title":"Verb expand"},{"location":"verb-expand/#siuba.dply.verbs.expand","text":"Return table with unique crossings of specified columns. Parameters: Name Type Description Default __data The input data. required *args Column names to cross and de-duplicate. () Examples: >>> import pandas as pd >>> from siuba import _ , expand , count , anti_join , right_join >>> df = pd . DataFrame ({ \"x\" : [ 1 , 2 , 2 ], \"y\" : [ \"a\" , \"a\" , \"b\" ], \"z\" : 1 }) >>> df x y z 0 1 a 1 1 2 a 1 2 2 b 1 >>> combos = df >> expand ( _ . x , _ . y ) >>> combos x y 0 1 a 1 1 b 2 2 a 3 2 b >>> df >> right_join ( _ , combos ) x y z 0 1.0 a 1.0 1 1 b NaN 2 2.0 a 1.0 3 2.0 b 1.0 >>> combos >> anti_join ( _ , df ) x y 1 1 b Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def expand ( __data , * args , fill = None ): \"\"\"Return table with unique crossings of specified columns. Parameters ---------- __data: The input data. *args: Column names to cross and de-duplicate. Examples -------- >>> import pandas as pd >>> from siuba import _, expand, count, anti_join, right_join >>> df = pd.DataFrame({\"x\": [1, 2, 2], \"y\": [\"a\", \"a\", \"b\"], \"z\": 1}) >>> df x y z 0 1 a 1 1 2 a 1 2 2 b 1 >>> combos = df >> expand(_.x, _.y) >>> combos x y 0 1 a 1 1 b 2 2 a 3 2 b >>> df >> right_join(_, combos) x y z 0 1.0 a 1.0 1 1 b NaN 2 2.0 a 1.0 3 2.0 b 1.0 >>> combos >> anti_join(_, df) x y 1 1 b \"\"\" var_names = list ( map ( simple_varname , args )) cols = [ __data [ name ] . unique () for name in var_names ] # see https://stackoverflow.com/a/25636395/1144523 cprod = cartesian_product ( cols ) expanded = pd . DataFrame ( np . array ( cprod ) . T ) expanded . columns = var_names return expanded","title":"expand()"},{"location":"verb-extract/","text":"extract ( __data , col , into , regex = '( \\\\ w+)' , remove = True , convert = False , flags = 0 ) Pull out len(into) fields from character strings. Returns a DataFrame with a column added for each piece. Parameters: Name Type Description Default __data a DataFrame required col name of column to split (either string, or siu expression). required into names of resulting columns holding each entry in pulled out fields. required regex regular expression used to extract field. Passed to col.str.extract method. '(\\\\w+)' remove whether to remove col from the returned DataFrame. True convert whether to attempt to convert the split columns to numerics. False flags flags from the re module, passed to col.str.extract. 0 Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def extract ( __data , col , into , regex = r \"(\\w+)\" , remove = True , convert = False , flags = 0 ): \"\"\"Pull out len(into) fields from character strings. Returns a DataFrame with a column added for each piece. Parameters ---------- __data: a DataFrame col: name of column to split (either string, or siu expression). into: names of resulting columns holding each entry in pulled out fields. regex: regular expression used to extract field. Passed to col.str.extract method. remove: whether to remove col from the returned DataFrame. convert: whether to attempt to convert the split columns to numerics. flags: flags from the re module, passed to col.str.extract. \"\"\" col_name = simple_varname ( col ) n_into = len ( into ) all_splits = __data [ col_name ] . str . extract ( regex , flags ) n_split_cols = len ( all_splits . columns ) if n_split_cols != n_into : raise ValueError ( \"Split into %s pieces, but expected %s \" % ( n_split_cols , n_into )) # end up with only the into columns, correctly named ---- new_names = dict ( zip ( all_splits . columns , into )) keep_splits = all_splits . rename ( columns = new_names ) # attempt to convert columns to numeric ---- if convert : # TODO: better strategy here? for k in keep_splits : try : keep_splits [ k ] = pd . to_numeric ( keep_splits [ k ]) except ValueError : pass out = pd . concat ([ __data , keep_splits ], axis = 1 ) if remove : return out . drop ( columns = col_name ) return out","title":"extract"},{"location":"verb-extract/#siuba.dply.verbs.extract","text":"Pull out len(into) fields from character strings. Returns a DataFrame with a column added for each piece. Parameters: Name Type Description Default __data a DataFrame required col name of column to split (either string, or siu expression). required into names of resulting columns holding each entry in pulled out fields. required regex regular expression used to extract field. Passed to col.str.extract method. '(\\\\w+)' remove whether to remove col from the returned DataFrame. True convert whether to attempt to convert the split columns to numerics. False flags flags from the re module, passed to col.str.extract. 0 Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def extract ( __data , col , into , regex = r \"(\\w+)\" , remove = True , convert = False , flags = 0 ): \"\"\"Pull out len(into) fields from character strings. Returns a DataFrame with a column added for each piece. Parameters ---------- __data: a DataFrame col: name of column to split (either string, or siu expression). into: names of resulting columns holding each entry in pulled out fields. regex: regular expression used to extract field. Passed to col.str.extract method. remove: whether to remove col from the returned DataFrame. convert: whether to attempt to convert the split columns to numerics. flags: flags from the re module, passed to col.str.extract. \"\"\" col_name = simple_varname ( col ) n_into = len ( into ) all_splits = __data [ col_name ] . str . extract ( regex , flags ) n_split_cols = len ( all_splits . columns ) if n_split_cols != n_into : raise ValueError ( \"Split into %s pieces, but expected %s \" % ( n_split_cols , n_into )) # end up with only the into columns, correctly named ---- new_names = dict ( zip ( all_splits . columns , into )) keep_splits = all_splits . rename ( columns = new_names ) # attempt to convert columns to numeric ---- if convert : # TODO: better strategy here? for k in keep_splits : try : keep_splits [ k ] = pd . to_numeric ( keep_splits [ k ]) except ValueError : pass out = pd . concat ([ __data , keep_splits ], axis = 1 ) if remove : return out . drop ( columns = col_name ) return out","title":"extract()"},{"location":"verb-filter/","text":"filter ( __data , * args ) Keep rows where conditions are true. Parameters: Name Type Description Default __data The data being filtered. required *args conditions that must be met to keep a column. () Examples: >>> from siuba import _ , filter >>> from siuba.data import cars Keep rows where cyl is 4 and mpg is less than 25. >>> cars >> filter ( _ . cyl == 4 , _ . mpg < 22 ) cyl mpg hp 20 4 21.5 97 31 4 21.4 109 Use | to represent an OR condition. For example, the code below keeps rows where hp is over 250 or mpg is over 32. >>> cars >> filter (( _ . hp > 300 ) | ( _ . mpg > 32 )) cyl mpg hp 17 4 32.4 66 19 4 33.9 65 30 8 15.0 335 Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def filter ( __data , * args ): \"\"\"Keep rows where conditions are true. Parameters ---------- __data: The data being filtered. *args: conditions that must be met to keep a column. Examples -------- >>> from siuba import _, filter >>> from siuba.data import cars Keep rows where cyl is 4 *and* mpg is less than 25. >>> cars >> filter(_.cyl == 4, _.mpg < 22) cyl mpg hp 20 4 21.5 97 31 4 21.4 109 Use `|` to represent an OR condition. For example, the code below keeps rows where hp is over 250 *or* mpg is over 32. >>> cars >> filter((_.hp > 300) | (_.mpg > 32)) cyl mpg hp 17 4 32.4 66 19 4 33.9 65 30 8 15.0 335 \"\"\" crnt_indx = True for arg in args : crnt_indx &= arg ( __data ) if callable ( arg ) else arg # use loc or iloc to subset, depending on crnt_indx ---- # the main issue here is that loc can't remove all rows using a slice # and iloc can't use a boolean series if isinstance ( crnt_indx , bool ) or isinstance ( crnt_indx , np . bool_ ): # iloc can do slice, but not a bool series result = __data . iloc [ slice ( None ) if crnt_indx else slice ( 0 ),:] else : result = __data . loc [ crnt_indx ,:] return result","title":"filter"},{"location":"verb-filter/#siuba.dply.verbs.filter","text":"Keep rows where conditions are true. Parameters: Name Type Description Default __data The data being filtered. required *args conditions that must be met to keep a column. () Examples: >>> from siuba import _ , filter >>> from siuba.data import cars Keep rows where cyl is 4 and mpg is less than 25. >>> cars >> filter ( _ . cyl == 4 , _ . mpg < 22 ) cyl mpg hp 20 4 21.5 97 31 4 21.4 109 Use | to represent an OR condition. For example, the code below keeps rows where hp is over 250 or mpg is over 32. >>> cars >> filter (( _ . hp > 300 ) | ( _ . mpg > 32 )) cyl mpg hp 17 4 32.4 66 19 4 33.9 65 30 8 15.0 335 Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def filter ( __data , * args ): \"\"\"Keep rows where conditions are true. Parameters ---------- __data: The data being filtered. *args: conditions that must be met to keep a column. Examples -------- >>> from siuba import _, filter >>> from siuba.data import cars Keep rows where cyl is 4 *and* mpg is less than 25. >>> cars >> filter(_.cyl == 4, _.mpg < 22) cyl mpg hp 20 4 21.5 97 31 4 21.4 109 Use `|` to represent an OR condition. For example, the code below keeps rows where hp is over 250 *or* mpg is over 32. >>> cars >> filter((_.hp > 300) | (_.mpg > 32)) cyl mpg hp 17 4 32.4 66 19 4 33.9 65 30 8 15.0 335 \"\"\" crnt_indx = True for arg in args : crnt_indx &= arg ( __data ) if callable ( arg ) else arg # use loc or iloc to subset, depending on crnt_indx ---- # the main issue here is that loc can't remove all rows using a slice # and iloc can't use a boolean series if isinstance ( crnt_indx , bool ) or isinstance ( crnt_indx , np . bool_ ): # iloc can do slice, but not a bool series result = __data . iloc [ slice ( None ) if crnt_indx else slice ( 0 ),:] else : result = __data . loc [ crnt_indx ,:] return result","title":"filter()"},{"location":"verb-gather/","text":"gather ( __data , key = 'key' , value = 'value' , * args , * , drop_na = False , convert = False ) Reshape table by gathering it in to long format. Parameters: Name Type Description Default __data The input data. required key Name of the key (or measure) column, which holds the names of the columns that were turned into rows. 'key' value Name of the value column, which holds the values from the columns that were turned into rows. 'value' *args A selection of columns. If unspecified, all columns are selected. Any arguments you could pass to the select() verb are allowed. () drop_na bool Whether to remove any rows where the value column is NA. False Examples: >>> import pandas as pd >>> from siuba import _ , gather >>> df = pd . DataFrame ({ \"id\" : [ \"a\" , \"b\" ], \"x\" : [ 1 , 2 ], \"y\" : [ 3 , None ]}) The code below gathers in all columns, except id: >>> gather ( df , \"key\" , \"value\" , - _ . id ) id key value 0 a x 1.0 1 b x 2.0 2 a y 3.0 3 b y NaN >>> gather ( df , \"measure\" , \"result\" , _ . x , _ . y , drop_na = True ) id measure result 0 a x 1.0 1 b x 2.0 2 a y 3.0 Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def gather ( __data , key = \"key\" , value = \"value\" , * args , drop_na = False , convert = False ): \"\"\"Reshape table by gathering it in to long format. Parameters ---------- __data: The input data. key: Name of the key (or measure) column, which holds the names of the columns that were turned into rows. value: Name of the value column, which holds the values from the columns that were turned into rows. *args: A selection of columns. If unspecified, all columns are selected. Any arguments you could pass to the select() verb are allowed. drop_na: bool Whether to remove any rows where the value column is NA. Examples -------- >>> import pandas as pd >>> from siuba import _, gather >>> df = pd.DataFrame({\"id\": [\"a\", \"b\"], \"x\": [1, 2], \"y\": [3, None]}) The code below gathers in all columns, except id: >>> gather(df, \"key\", \"value\", -_.id) id key value 0 a x 1.0 1 b x 2.0 2 a y 3.0 3 b y NaN >>> gather(df, \"measure\", \"result\", _.x, _.y, drop_na=True) id measure result 0 a x 1.0 1 b x 2.0 2 a y 3.0 \"\"\" # TODO: implement var selection over *args if convert : raise NotImplementedError ( \"convert not yet implemented\" ) # TODO: copied from nest and select var_list = var_create ( * args ) od = var_select ( __data . columns , * var_list ) value_vars = list ( od ) or None id_vars = [ col for col in __data . columns if col not in od ] long = pd . melt ( __data , id_vars , value_vars , key , value ) if drop_na : return long [ ~ long [ value ] . isna ()] . reset_index ( drop = True ) return long spread ( __data , key , value , fill = None , reset_index = True ) Reshape table by spreading it out to wide format. Parameters: Name Type Description Default __data The input data. required key Column whose values will be used as new column names. required value Column whose values will fill the new column entries. required fill Value to set for any missing values. By default keeps them as missing values. None Examples: >>> import pandas as pd >>> from siuba import _ , gather >>> df = pd . DataFrame ({ \"id\" : [ \"a\" , \"b\" ], \"x\" : [ 1 , 2 ], \"y\" : [ 3 , None ]}) >>> long = gather ( df , \"key\" , \"value\" , - _ . id , drop_na = True ) >>> long id key value 0 a x 1.0 1 b x 2.0 2 a y 3.0 >>> spread ( long , \"key\" , \"value\" ) id x y 0 a 1.0 3.0 1 b 2.0 NaN Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def spread ( __data , key , value , fill = None , reset_index = True ): \"\"\"Reshape table by spreading it out to wide format. Parameters ---------- __data: The input data. key: Column whose values will be used as new column names. value: Column whose values will fill the new column entries. fill: Value to set for any missing values. By default keeps them as missing values. Examples -------- >>> import pandas as pd >>> from siuba import _, gather >>> df = pd.DataFrame({\"id\": [\"a\", \"b\"], \"x\": [1, 2], \"y\": [3, None]}) >>> long = gather(df, \"key\", \"value\", -_.id, drop_na=True) >>> long id key value 0 a x 1.0 1 b x 2.0 2 a y 3.0 >>> spread(long, \"key\", \"value\") id x y 0 a 1.0 3.0 1 b 2.0 NaN \"\"\" key_col = _get_single_var_select ( __data . columns , key ) val_col = _get_single_var_select ( __data . columns , value ) id_cols = [ col for col in __data . columns if col not in ( key_col , val_col )] wide = __data . set_index ( id_cols + [ key_col ]) . unstack ( level = - 1 ) if fill is not None : wide . fillna ( fill , inplace = True ) # remove multi-index from both rows and cols wide . columns = wide . columns . droplevel () . rename ( None ) if reset_index : wide . reset_index ( inplace = True ) return wide","title":"gather, spread"},{"location":"verb-gather/#siuba.dply.verbs.gather","text":"Reshape table by gathering it in to long format. Parameters: Name Type Description Default __data The input data. required key Name of the key (or measure) column, which holds the names of the columns that were turned into rows. 'key' value Name of the value column, which holds the values from the columns that were turned into rows. 'value' *args A selection of columns. If unspecified, all columns are selected. Any arguments you could pass to the select() verb are allowed. () drop_na bool Whether to remove any rows where the value column is NA. False Examples: >>> import pandas as pd >>> from siuba import _ , gather >>> df = pd . DataFrame ({ \"id\" : [ \"a\" , \"b\" ], \"x\" : [ 1 , 2 ], \"y\" : [ 3 , None ]}) The code below gathers in all columns, except id: >>> gather ( df , \"key\" , \"value\" , - _ . id ) id key value 0 a x 1.0 1 b x 2.0 2 a y 3.0 3 b y NaN >>> gather ( df , \"measure\" , \"result\" , _ . x , _ . y , drop_na = True ) id measure result 0 a x 1.0 1 b x 2.0 2 a y 3.0 Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def gather ( __data , key = \"key\" , value = \"value\" , * args , drop_na = False , convert = False ): \"\"\"Reshape table by gathering it in to long format. Parameters ---------- __data: The input data. key: Name of the key (or measure) column, which holds the names of the columns that were turned into rows. value: Name of the value column, which holds the values from the columns that were turned into rows. *args: A selection of columns. If unspecified, all columns are selected. Any arguments you could pass to the select() verb are allowed. drop_na: bool Whether to remove any rows where the value column is NA. Examples -------- >>> import pandas as pd >>> from siuba import _, gather >>> df = pd.DataFrame({\"id\": [\"a\", \"b\"], \"x\": [1, 2], \"y\": [3, None]}) The code below gathers in all columns, except id: >>> gather(df, \"key\", \"value\", -_.id) id key value 0 a x 1.0 1 b x 2.0 2 a y 3.0 3 b y NaN >>> gather(df, \"measure\", \"result\", _.x, _.y, drop_na=True) id measure result 0 a x 1.0 1 b x 2.0 2 a y 3.0 \"\"\" # TODO: implement var selection over *args if convert : raise NotImplementedError ( \"convert not yet implemented\" ) # TODO: copied from nest and select var_list = var_create ( * args ) od = var_select ( __data . columns , * var_list ) value_vars = list ( od ) or None id_vars = [ col for col in __data . columns if col not in od ] long = pd . melt ( __data , id_vars , value_vars , key , value ) if drop_na : return long [ ~ long [ value ] . isna ()] . reset_index ( drop = True ) return long","title":"gather()"},{"location":"verb-gather/#siuba.dply.verbs.spread","text":"Reshape table by spreading it out to wide format. Parameters: Name Type Description Default __data The input data. required key Column whose values will be used as new column names. required value Column whose values will fill the new column entries. required fill Value to set for any missing values. By default keeps them as missing values. None Examples: >>> import pandas as pd >>> from siuba import _ , gather >>> df = pd . DataFrame ({ \"id\" : [ \"a\" , \"b\" ], \"x\" : [ 1 , 2 ], \"y\" : [ 3 , None ]}) >>> long = gather ( df , \"key\" , \"value\" , - _ . id , drop_na = True ) >>> long id key value 0 a x 1.0 1 b x 2.0 2 a y 3.0 >>> spread ( long , \"key\" , \"value\" ) id x y 0 a 1.0 3.0 1 b 2.0 NaN Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def spread ( __data , key , value , fill = None , reset_index = True ): \"\"\"Reshape table by spreading it out to wide format. Parameters ---------- __data: The input data. key: Column whose values will be used as new column names. value: Column whose values will fill the new column entries. fill: Value to set for any missing values. By default keeps them as missing values. Examples -------- >>> import pandas as pd >>> from siuba import _, gather >>> df = pd.DataFrame({\"id\": [\"a\", \"b\"], \"x\": [1, 2], \"y\": [3, None]}) >>> long = gather(df, \"key\", \"value\", -_.id, drop_na=True) >>> long id key value 0 a x 1.0 1 b x 2.0 2 a y 3.0 >>> spread(long, \"key\", \"value\") id x y 0 a 1.0 3.0 1 b 2.0 NaN \"\"\" key_col = _get_single_var_select ( __data . columns , key ) val_col = _get_single_var_select ( __data . columns , value ) id_cols = [ col for col in __data . columns if col not in ( key_col , val_col )] wide = __data . set_index ( id_cols + [ key_col ]) . unstack ( level = - 1 ) if fill is not None : wide . fillna ( fill , inplace = True ) # remove multi-index from both rows and cols wide . columns = wide . columns . droplevel () . rename ( None ) if reset_index : wide . reset_index ( inplace = True ) return wide","title":"spread()"},{"location":"verb-group_by/","text":"group_by ( __data , * args , * , add = False , ** kwargs ) Return a grouped DataFrame, using columns or expressions to define groups. Any operations (e.g. summarize, mutate, filter) performed on grouped data will be performed \"by group\". Use ungroup() to remove the groupings. Parameters: Name Type Description Default __data The data being grouped. required *args Lazy expressions used to select the grouping columns. Currently, each arg must refer to a single columns (e.g. .cyl, .mpg). () add bool If the data is already grouped, whether to add these groupings on top of those. False **kwargs Keyword arguments define new columns used to group the data. {} Examples: >>> from siuba import _ , group_by , summarize , filter , mutate , head >>> from siuba.data import cars >>> by_cyl = cars >> group_by ( _ . cyl ) >>> by_cyl >> summarize ( max_mpg = _ . mpg . max (), max_hp = _ . hp . max ()) cyl max_mpg max_hp 0 4 33.9 113 1 6 21.4 175 2 8 19.2 335 >>> by_cyl >> filter ( _ . mpg == _ . mpg . max ()) ( grouped data frame ) cyl mpg hp 3 6 21.4 110 19 4 33.9 65 24 8 19.2 175 >>> cars >> group_by ( cyl2 = _ . cyl + 1 ) >> head ( 2 ) ( grouped data frame ) cyl mpg hp cyl2 0 6 21.0 110 7 1 6 21.0 110 7 Note that creating the new grouping column is always performed on ungrouped data. Use an explicit mutate on the grouped data perform the operation within groups. For example, the code below calls pd.cut on the mpg column, within each cyl group. >>> from siuba.siu import call >>> ( cars ... >> group_by ( _ . cyl ) ... >> mutate ( mpg_bin = call ( pd . cut , _ . mpg , 3 )) ... >> group_by ( _ . mpg_bin , add = True ) ... >> head ( 2 ) ... ) ( grouped data frame ) cyl mpg hp mpg_bin 0 6 21.0 110 ( 20.2 , 21.4 ] 1 6 21.0 110 ( 20.2 , 21.4 ] Source code in siuba/dply/verbs.py @singledispatch2 (( pd . DataFrame , DataFrameGroupBy )) def group_by ( __data , * args , add = False , ** kwargs ): \"\"\"Return a grouped DataFrame, using columns or expressions to define groups. Any operations (e.g. summarize, mutate, filter) performed on grouped data will be performed \"by group\". Use `ungroup()` to remove the groupings. Parameters ---------- __data: The data being grouped. *args: Lazy expressions used to select the grouping columns. Currently, each arg must refer to a single columns (e.g. _.cyl, _.mpg). add: bool If the data is already grouped, whether to add these groupings on top of those. **kwargs: Keyword arguments define new columns used to group the data. Examples -------- >>> from siuba import _, group_by, summarize, filter, mutate, head >>> from siuba.data import cars >>> by_cyl = cars >> group_by(_.cyl) >>> by_cyl >> summarize(max_mpg = _.mpg.max(), max_hp = _.hp.max()) cyl max_mpg max_hp 0 4 33.9 113 1 6 21.4 175 2 8 19.2 335 >>> by_cyl >> filter(_.mpg == _.mpg.max()) (grouped data frame) cyl mpg hp 3 6 21.4 110 19 4 33.9 65 24 8 19.2 175 >>> cars >> group_by(cyl2 = _.cyl + 1) >> head(2) (grouped data frame) cyl mpg hp cyl2 0 6 21.0 110 7 1 6 21.0 110 7 Note that creating the new grouping column is always performed on ungrouped data. Use an explicit mutate on the grouped data perform the operation within groups. For example, the code below calls pd.cut on the mpg column, within each cyl group. >>> from siuba.siu import call >>> (cars ... >> group_by(_.cyl) ... >> mutate(mpg_bin = call(pd.cut, _.mpg, 3)) ... >> group_by(_.mpg_bin, add=True) ... >> head(2) ... ) (grouped data frame) cyl mpg hp mpg_bin 0 6 21.0 110 (20.2, 21.4] 1 6 21.0 110 (20.2, 21.4] \"\"\" tmp_df = mutate ( __data , ** kwargs ) if kwargs else __data by_vars = list ( map ( simple_varname , args )) for ii , name in enumerate ( by_vars ): if name is None : raise Exception ( \"group by variable %s is not a column name\" % ii ) by_vars . extend ( kwargs . keys ()) if isinstance ( tmp_df , DataFrameGroupBy ) and add : prior_groups = [ el . name for el in __data . grouper . groupings ] all_groups = ordered_union ( prior_groups , by_vars ) return tmp_df . obj . groupby ( list ( all_groups )) return tmp_df . groupby ( by = by_vars ) ungroup ( __data ) Return an ungrouped DataFrame. Parameters: Name Type Description Default __data The data being ungrouped. required Examples: >>> from siuba import _ , group_by , ungroup >>> from siuba.data import cars >>> g_cyl = cars . groupby ( \"cyl\" ) >>> res1 = ungroup ( g_cyl ) >>> res2 = cars >> group_by ( _ . cyl ) >> ungroup () Source code in siuba/dply/verbs.py @singledispatch2 (( pd . DataFrame , DataFrameGroupBy )) def ungroup ( __data ): \"\"\"Return an ungrouped DataFrame. Parameters ---------- __data: The data being ungrouped. Examples -------- >>> from siuba import _, group_by, ungroup >>> from siuba.data import cars >>> g_cyl = cars.groupby(\"cyl\") >>> res1 = ungroup(g_cyl) >>> res2 = cars >> group_by(_.cyl) >> ungroup() \"\"\" # TODO: can we somehow just restore the original df used to construct # the groupby? if isinstance ( __data , pd . DataFrame ): return __data if isinstance ( __data , pd . Series ): return __data . reset_index () return __data . obj . reset_index ( drop = True )","title":"group_by, ungroup"},{"location":"verb-group_by/#siuba.dply.verbs.group_by","text":"Return a grouped DataFrame, using columns or expressions to define groups. Any operations (e.g. summarize, mutate, filter) performed on grouped data will be performed \"by group\". Use ungroup() to remove the groupings. Parameters: Name Type Description Default __data The data being grouped. required *args Lazy expressions used to select the grouping columns. Currently, each arg must refer to a single columns (e.g. .cyl, .mpg). () add bool If the data is already grouped, whether to add these groupings on top of those. False **kwargs Keyword arguments define new columns used to group the data. {} Examples: >>> from siuba import _ , group_by , summarize , filter , mutate , head >>> from siuba.data import cars >>> by_cyl = cars >> group_by ( _ . cyl ) >>> by_cyl >> summarize ( max_mpg = _ . mpg . max (), max_hp = _ . hp . max ()) cyl max_mpg max_hp 0 4 33.9 113 1 6 21.4 175 2 8 19.2 335 >>> by_cyl >> filter ( _ . mpg == _ . mpg . max ()) ( grouped data frame ) cyl mpg hp 3 6 21.4 110 19 4 33.9 65 24 8 19.2 175 >>> cars >> group_by ( cyl2 = _ . cyl + 1 ) >> head ( 2 ) ( grouped data frame ) cyl mpg hp cyl2 0 6 21.0 110 7 1 6 21.0 110 7 Note that creating the new grouping column is always performed on ungrouped data. Use an explicit mutate on the grouped data perform the operation within groups. For example, the code below calls pd.cut on the mpg column, within each cyl group. >>> from siuba.siu import call >>> ( cars ... >> group_by ( _ . cyl ) ... >> mutate ( mpg_bin = call ( pd . cut , _ . mpg , 3 )) ... >> group_by ( _ . mpg_bin , add = True ) ... >> head ( 2 ) ... ) ( grouped data frame ) cyl mpg hp mpg_bin 0 6 21.0 110 ( 20.2 , 21.4 ] 1 6 21.0 110 ( 20.2 , 21.4 ] Source code in siuba/dply/verbs.py @singledispatch2 (( pd . DataFrame , DataFrameGroupBy )) def group_by ( __data , * args , add = False , ** kwargs ): \"\"\"Return a grouped DataFrame, using columns or expressions to define groups. Any operations (e.g. summarize, mutate, filter) performed on grouped data will be performed \"by group\". Use `ungroup()` to remove the groupings. Parameters ---------- __data: The data being grouped. *args: Lazy expressions used to select the grouping columns. Currently, each arg must refer to a single columns (e.g. _.cyl, _.mpg). add: bool If the data is already grouped, whether to add these groupings on top of those. **kwargs: Keyword arguments define new columns used to group the data. Examples -------- >>> from siuba import _, group_by, summarize, filter, mutate, head >>> from siuba.data import cars >>> by_cyl = cars >> group_by(_.cyl) >>> by_cyl >> summarize(max_mpg = _.mpg.max(), max_hp = _.hp.max()) cyl max_mpg max_hp 0 4 33.9 113 1 6 21.4 175 2 8 19.2 335 >>> by_cyl >> filter(_.mpg == _.mpg.max()) (grouped data frame) cyl mpg hp 3 6 21.4 110 19 4 33.9 65 24 8 19.2 175 >>> cars >> group_by(cyl2 = _.cyl + 1) >> head(2) (grouped data frame) cyl mpg hp cyl2 0 6 21.0 110 7 1 6 21.0 110 7 Note that creating the new grouping column is always performed on ungrouped data. Use an explicit mutate on the grouped data perform the operation within groups. For example, the code below calls pd.cut on the mpg column, within each cyl group. >>> from siuba.siu import call >>> (cars ... >> group_by(_.cyl) ... >> mutate(mpg_bin = call(pd.cut, _.mpg, 3)) ... >> group_by(_.mpg_bin, add=True) ... >> head(2) ... ) (grouped data frame) cyl mpg hp mpg_bin 0 6 21.0 110 (20.2, 21.4] 1 6 21.0 110 (20.2, 21.4] \"\"\" tmp_df = mutate ( __data , ** kwargs ) if kwargs else __data by_vars = list ( map ( simple_varname , args )) for ii , name in enumerate ( by_vars ): if name is None : raise Exception ( \"group by variable %s is not a column name\" % ii ) by_vars . extend ( kwargs . keys ()) if isinstance ( tmp_df , DataFrameGroupBy ) and add : prior_groups = [ el . name for el in __data . grouper . groupings ] all_groups = ordered_union ( prior_groups , by_vars ) return tmp_df . obj . groupby ( list ( all_groups )) return tmp_df . groupby ( by = by_vars )","title":"group_by()"},{"location":"verb-group_by/#siuba.dply.verbs.ungroup","text":"Return an ungrouped DataFrame. Parameters: Name Type Description Default __data The data being ungrouped. required Examples: >>> from siuba import _ , group_by , ungroup >>> from siuba.data import cars >>> g_cyl = cars . groupby ( \"cyl\" ) >>> res1 = ungroup ( g_cyl ) >>> res2 = cars >> group_by ( _ . cyl ) >> ungroup () Source code in siuba/dply/verbs.py @singledispatch2 (( pd . DataFrame , DataFrameGroupBy )) def ungroup ( __data ): \"\"\"Return an ungrouped DataFrame. Parameters ---------- __data: The data being ungrouped. Examples -------- >>> from siuba import _, group_by, ungroup >>> from siuba.data import cars >>> g_cyl = cars.groupby(\"cyl\") >>> res1 = ungroup(g_cyl) >>> res2 = cars >> group_by(_.cyl) >> ungroup() \"\"\" # TODO: can we somehow just restore the original df used to construct # the groupby? if isinstance ( __data , pd . DataFrame ): return __data if isinstance ( __data , pd . Series ): return __data . reset_index () return __data . obj . reset_index ( drop = True )","title":"ungroup()"},{"location":"verb-head/","text":"head ( __data , n = 5 ) Return the first n rows of the data. Parameters: Name Type Description Default __data a DataFrame. required n The number of rows of data to keep. 5 Examples: >>> from siuba import head >>> from siuba.data import cars >>> cars >> head ( 2 ) cyl mpg hp 0 6 21.0 110 1 6 21.0 110 Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def head ( __data , n = 5 ): \"\"\"Return the first n rows of the data. Parameters ---------- __data: a DataFrame. n: The number of rows of data to keep. Examples -------- >>> from siuba import head >>> from siuba.data import cars >>> cars >> head(2) cyl mpg hp 0 6 21.0 110 1 6 21.0 110 \"\"\" return __data . head ( n )","title":"Verb head"},{"location":"verb-head/#siuba.dply.verbs.head","text":"Return the first n rows of the data. Parameters: Name Type Description Default __data a DataFrame. required n The number of rows of data to keep. 5 Examples: >>> from siuba import head >>> from siuba.data import cars >>> cars >> head ( 2 ) cyl mpg hp 0 6 21.0 110 1 6 21.0 110 Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def head ( __data , n = 5 ): \"\"\"Return the first n rows of the data. Parameters ---------- __data: a DataFrame. n: The number of rows of data to keep. Examples -------- >>> from siuba import head >>> from siuba.data import cars >>> cars >> head(2) cyl mpg hp 0 6 21.0 110 1 6 21.0 110 \"\"\" return __data . head ( n )","title":"head()"},{"location":"verb-join/","text":"join ( left , right , on = None , how = None , * args , * , by = None , ** kwargs ) Join two tables together, by matching on specified columns. The functions inner_join, left_join, right_join, and full_join are provided as wrappers around join, and are used in the examples. Parameters: Name Type Description Default left The left-hand table. required right The right-hand table. required on How to match them. Note that the keyword \"by\" can also be used for this parameter, in order to support compatibility with dplyr. None how The type of join to perform (inner, full, left, right). None *args Additional postition arguments. Currently not supported. () **kwargs Additional keyword arguments. Currently not supported. {} Examples: >>> from siuba import _ , inner_join , left_join , full_join , right_join >>> from siuba.data import band_members , band_instruments , band_instruments2 >>> band_members name band 0 Mick Stones 1 John Beatles 2 Paul Beatles >>> band_instruments name plays 0 John guitar 1 Paul bass 2 Keith guitar Notice that above, only John and Paul have entries for band instruments. This means that they will be the only two rows in the inner_join result: >>> band_members >> inner_join ( _ , band_instruments ) name band plays 0 John Beatles guitar 1 Paul Beatles bass A left join ensures all original rows of the left hand data are included. >>> band_members >> left_join ( _ , band_instruments ) name band plays 0 Mick Stones NaN 1 John Beatles guitar 2 Paul Beatles bass A full join is similar, but ensures all rows of both data are included. >>> band_members >> full_join ( _ , band_instruments ) name band plays 0 Mick Stones NaN 1 John Beatles guitar 2 Paul Beatles bass 3 Keith NaN guitar You can explicilty specify columns to join on using the \"by\" argument: >>> band_members >> inner_join ( _ , band_instruments , by = \"name\" ) n ... Use a dictionary for the by argument, to match up columns with different names: >>> band_members >> full_join ( _ , band_instruments2 , { \"name\" : \"artist\" }) n ... Joins create a new row for each pair of matches. For example, the value 1 is in two rows on the left, and 2 rows on the right so 4 rows will be created. >>> df1 = pd . DataFrame ({ \"x\" : [ 1 , 1 , 3 ]}) >>> df2 = pd . DataFrame ({ \"x\" : [ 1 , 1 , 2 ], \"y\" : [ \"first\" , \"second\" , \"third\" ]}) >>> df1 >> left_join ( _ , df2 ) x y 0 1 first 1 1 second 2 1 first 3 1 second 4 3 NaN Missing values count as matches to eachother by default: >>> df3 = pd . DataFrame ({ \"x\" : [ 1 , None ], \"y\" : 2 }) >>> df4 = pd . DataFrame ({ \"x\" : [ 1 , None ], \"z\" : 3 }) >>> left_join ( df3 , df4 ) x y z 0 1.0 2 3 1 NaN 2 3 Returns: Type Description pd.DataFrame Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def join ( left , right , on = None , how = None , * args , by = None , ** kwargs ): \"\"\"Join two tables together, by matching on specified columns. The functions inner_join, left_join, right_join, and full_join are provided as wrappers around join, and are used in the examples. Parameters ---------- left : The left-hand table. right : The right-hand table. on : How to match them. Note that the keyword \"by\" can also be used for this parameter, in order to support compatibility with dplyr. how : The type of join to perform (inner, full, left, right). *args: Additional postition arguments. Currently not supported. **kwargs: Additional keyword arguments. Currently not supported. Returns ------- pd.DataFrame Examples -------- >>> from siuba import _, inner_join, left_join, full_join, right_join >>> from siuba.data import band_members, band_instruments, band_instruments2 >>> band_members name band 0 Mick Stones 1 John Beatles 2 Paul Beatles >>> band_instruments name plays 0 John guitar 1 Paul bass 2 Keith guitar Notice that above, only John and Paul have entries for band instruments. This means that they will be the only two rows in the inner_join result: >>> band_members >> inner_join(_, band_instruments) name band plays 0 John Beatles guitar 1 Paul Beatles bass A left join ensures all original rows of the left hand data are included. >>> band_members >> left_join(_, band_instruments) name band plays 0 Mick Stones NaN 1 John Beatles guitar 2 Paul Beatles bass A full join is similar, but ensures all rows of both data are included. >>> band_members >> full_join(_, band_instruments) name band plays 0 Mick Stones NaN 1 John Beatles guitar 2 Paul Beatles bass 3 Keith NaN guitar You can explicilty specify columns to join on using the \"by\" argument: >>> band_members >> inner_join(_, band_instruments, by = \"name\") n... Use a dictionary for the by argument, to match up columns with different names: >>> band_members >> full_join(_, band_instruments2, {\"name\": \"artist\"}) n... Joins create a new row for each pair of matches. For example, the value 1 is in two rows on the left, and 2 rows on the right so 4 rows will be created. >>> df1 = pd.DataFrame({\"x\": [1, 1, 3]}) >>> df2 = pd.DataFrame({\"x\": [1, 1, 2], \"y\": [\"first\", \"second\", \"third\"]}) >>> df1 >> left_join(_, df2) x y 0 1 first 1 1 second 2 1 first 3 1 second 4 3 NaN Missing values count as matches to eachother by default: >>> df3 = pd.DataFrame({\"x\": [1, None], \"y\": 2}) >>> df4 = pd.DataFrame({\"x\": [1, None], \"z\": 3}) >>> left_join(df3, df4) x y z 0 1.0 2 3 1 NaN 2 3 \"\"\" if not isinstance ( right , DataFrame ): raise Exception ( \"right hand table must be a DataFrame\" ) if how is None : raise Exception ( \"Must specify how argument\" ) if len ( args ) or len ( kwargs ): raise NotImplementedError ( \"extra arguments to pandas join not currently supported\" ) if on is None and by is not None : on = by # pandas uses outer, but dplyr uses term full if how == \"full\" : how = \"outer\" if isinstance ( on , Mapping ): left_on , right_on = zip ( * on . items ()) return left . merge ( right , how = how , left_on = left_on , right_on = right_on ) return left . merge ( right , how = how , on = on )","title":"Verb join"},{"location":"verb-join/#siuba.dply.verbs.join","text":"Join two tables together, by matching on specified columns. The functions inner_join, left_join, right_join, and full_join are provided as wrappers around join, and are used in the examples. Parameters: Name Type Description Default left The left-hand table. required right The right-hand table. required on How to match them. Note that the keyword \"by\" can also be used for this parameter, in order to support compatibility with dplyr. None how The type of join to perform (inner, full, left, right). None *args Additional postition arguments. Currently not supported. () **kwargs Additional keyword arguments. Currently not supported. {} Examples: >>> from siuba import _ , inner_join , left_join , full_join , right_join >>> from siuba.data import band_members , band_instruments , band_instruments2 >>> band_members name band 0 Mick Stones 1 John Beatles 2 Paul Beatles >>> band_instruments name plays 0 John guitar 1 Paul bass 2 Keith guitar Notice that above, only John and Paul have entries for band instruments. This means that they will be the only two rows in the inner_join result: >>> band_members >> inner_join ( _ , band_instruments ) name band plays 0 John Beatles guitar 1 Paul Beatles bass A left join ensures all original rows of the left hand data are included. >>> band_members >> left_join ( _ , band_instruments ) name band plays 0 Mick Stones NaN 1 John Beatles guitar 2 Paul Beatles bass A full join is similar, but ensures all rows of both data are included. >>> band_members >> full_join ( _ , band_instruments ) name band plays 0 Mick Stones NaN 1 John Beatles guitar 2 Paul Beatles bass 3 Keith NaN guitar You can explicilty specify columns to join on using the \"by\" argument: >>> band_members >> inner_join ( _ , band_instruments , by = \"name\" ) n ... Use a dictionary for the by argument, to match up columns with different names: >>> band_members >> full_join ( _ , band_instruments2 , { \"name\" : \"artist\" }) n ... Joins create a new row for each pair of matches. For example, the value 1 is in two rows on the left, and 2 rows on the right so 4 rows will be created. >>> df1 = pd . DataFrame ({ \"x\" : [ 1 , 1 , 3 ]}) >>> df2 = pd . DataFrame ({ \"x\" : [ 1 , 1 , 2 ], \"y\" : [ \"first\" , \"second\" , \"third\" ]}) >>> df1 >> left_join ( _ , df2 ) x y 0 1 first 1 1 second 2 1 first 3 1 second 4 3 NaN Missing values count as matches to eachother by default: >>> df3 = pd . DataFrame ({ \"x\" : [ 1 , None ], \"y\" : 2 }) >>> df4 = pd . DataFrame ({ \"x\" : [ 1 , None ], \"z\" : 3 }) >>> left_join ( df3 , df4 ) x y z 0 1.0 2 3 1 NaN 2 3 Returns: Type Description pd.DataFrame Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def join ( left , right , on = None , how = None , * args , by = None , ** kwargs ): \"\"\"Join two tables together, by matching on specified columns. The functions inner_join, left_join, right_join, and full_join are provided as wrappers around join, and are used in the examples. Parameters ---------- left : The left-hand table. right : The right-hand table. on : How to match them. Note that the keyword \"by\" can also be used for this parameter, in order to support compatibility with dplyr. how : The type of join to perform (inner, full, left, right). *args: Additional postition arguments. Currently not supported. **kwargs: Additional keyword arguments. Currently not supported. Returns ------- pd.DataFrame Examples -------- >>> from siuba import _, inner_join, left_join, full_join, right_join >>> from siuba.data import band_members, band_instruments, band_instruments2 >>> band_members name band 0 Mick Stones 1 John Beatles 2 Paul Beatles >>> band_instruments name plays 0 John guitar 1 Paul bass 2 Keith guitar Notice that above, only John and Paul have entries for band instruments. This means that they will be the only two rows in the inner_join result: >>> band_members >> inner_join(_, band_instruments) name band plays 0 John Beatles guitar 1 Paul Beatles bass A left join ensures all original rows of the left hand data are included. >>> band_members >> left_join(_, band_instruments) name band plays 0 Mick Stones NaN 1 John Beatles guitar 2 Paul Beatles bass A full join is similar, but ensures all rows of both data are included. >>> band_members >> full_join(_, band_instruments) name band plays 0 Mick Stones NaN 1 John Beatles guitar 2 Paul Beatles bass 3 Keith NaN guitar You can explicilty specify columns to join on using the \"by\" argument: >>> band_members >> inner_join(_, band_instruments, by = \"name\") n... Use a dictionary for the by argument, to match up columns with different names: >>> band_members >> full_join(_, band_instruments2, {\"name\": \"artist\"}) n... Joins create a new row for each pair of matches. For example, the value 1 is in two rows on the left, and 2 rows on the right so 4 rows will be created. >>> df1 = pd.DataFrame({\"x\": [1, 1, 3]}) >>> df2 = pd.DataFrame({\"x\": [1, 1, 2], \"y\": [\"first\", \"second\", \"third\"]}) >>> df1 >> left_join(_, df2) x y 0 1 first 1 1 second 2 1 first 3 1 second 4 3 NaN Missing values count as matches to eachother by default: >>> df3 = pd.DataFrame({\"x\": [1, None], \"y\": 2}) >>> df4 = pd.DataFrame({\"x\": [1, None], \"z\": 3}) >>> left_join(df3, df4) x y z 0 1.0 2 3 1 NaN 2 3 \"\"\" if not isinstance ( right , DataFrame ): raise Exception ( \"right hand table must be a DataFrame\" ) if how is None : raise Exception ( \"Must specify how argument\" ) if len ( args ) or len ( kwargs ): raise NotImplementedError ( \"extra arguments to pandas join not currently supported\" ) if on is None and by is not None : on = by # pandas uses outer, but dplyr uses term full if how == \"full\" : how = \"outer\" if isinstance ( on , Mapping ): left_on , right_on = zip ( * on . items ()) return left . merge ( right , how = how , left_on = left_on , right_on = right_on ) return left . merge ( right , how = how , on = on )","title":"join()"},{"location":"verb-nest/","text":"nest ( __data , * args , * , key = 'data' ) Nest columns within a DataFrame. Parameters: Name Type Description Default __data A DataFrame. required *args The names of columns to be nested. May use any syntax used by the select function. () key The name of the column that will hold the nested columns. 'data' Examples: >>> from siuba import _ , nest >>> from siuba.data import cars >>> nested_cars = cars >> nest ( - _ . cyl ) Note that pandas with nested DataFrames looks okay in juypter notebooks, but has a weird representation in the IPython console, so the example below shows that each entry in the data column is a DataFrame. >>> nested_cars . shape ( 3 , 2 ) >>> type ( nested_cars . data [ 0 ]) < class ' pandas . core . frame . DataFrame '> Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def nest ( __data , * args , key = \"data\" ): \"\"\"Nest columns within a DataFrame. Parameters ---------- __data: A DataFrame. *args: The names of columns to be nested. May use any syntax used by the `select` function. key: The name of the column that will hold the nested columns. Examples -------- >>> from siuba import _, nest >>> from siuba.data import cars >>> nested_cars = cars >> nest(-_.cyl) Note that pandas with nested DataFrames looks okay in juypter notebooks, but has a weird representation in the IPython console, so the example below shows that each entry in the data column is a DataFrame. >>> nested_cars.shape (3, 2) >>> type(nested_cars.data[0]) <class 'pandas.core.frame.DataFrame'> \"\"\" # TODO: copied from select function var_list = var_create ( * args ) od = var_select ( __data . columns , * var_list ) # unselected columns are treated similar to using groupby grp_keys = list ( k for k in __data . columns if k not in set ( od )) nest_keys = list ( od ) # split into sub DataFrames, with only nest_keys as columns g_df = __data . groupby ( grp_keys ) splitter = g_df . grouper . _get_splitter ( g_df . obj [ nest_keys ]) # TODO: iterating over splitter now only produces 1 item (the dataframe) # check backwards compat def _extract_subdf_pandas_1_3 ( entry ): # in pandas < 1.3, splitter.__iter__ returns tuple entries (ii, df) if isinstance ( entry , tuple ): return entry [ 1 ] # in pandas 1.3, each entry is just the dataframe return entry result_index = g_df . grouper . result_index nested_dfs = [ _extract_subdf_pandas_1_3 ( x ) for x in splitter ] out = pd . DataFrame ({ key : nested_dfs }, index = result_index ) . reset_index () return out unnest ( __data , key = 'data' ) Unnest a column holding nested data (e.g. Series of lists or DataFrames). Parameters: Name Type Description Default ___data A DataFrame. required key The name of the column to be unnested. 'data' Examples: >>> import pandas as pd >>> df = pd . DataFrame ({ 'id' : [ 1 , 2 ], 'data' : [[ 'a' , 'b' ], [ 'c' ]]}) >>> df >> unnest () id data 0 1 a 1 1 b 2 2 c Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def unnest ( __data , key = \"data\" ): \"\"\"Unnest a column holding nested data (e.g. Series of lists or DataFrames). Parameters ---------- ___data: A DataFrame. key: The name of the column to be unnested. Examples -------- >>> import pandas as pd >>> df = pd.DataFrame({'id': [1,2], 'data': [['a', 'b'], ['c']]}) >>> df >> unnest() id data 0 1 a 1 1 b 2 2 c \"\"\" # TODO: currently only takes key, not expressions nrows_nested = __data [ key ] . apply ( len , convert_dtype = True ) indx_nested = nrows_nested . index . repeat ( nrows_nested ) grp_keys = list ( __data . columns [ __data . columns != key ]) # flatten nested data data_entries = map ( _convert_nested_entry , __data [ key ]) long_data = pd . concat ( data_entries , ignore_index = True ) long_data . name = key # may be a better approach using a multi-index long_grp = __data . loc [ indx_nested , grp_keys ] . reset_index ( drop = True ) return long_grp . join ( long_data )","title":"nest, unnest"},{"location":"verb-nest/#siuba.dply.verbs.nest","text":"Nest columns within a DataFrame. Parameters: Name Type Description Default __data A DataFrame. required *args The names of columns to be nested. May use any syntax used by the select function. () key The name of the column that will hold the nested columns. 'data' Examples: >>> from siuba import _ , nest >>> from siuba.data import cars >>> nested_cars = cars >> nest ( - _ . cyl ) Note that pandas with nested DataFrames looks okay in juypter notebooks, but has a weird representation in the IPython console, so the example below shows that each entry in the data column is a DataFrame. >>> nested_cars . shape ( 3 , 2 ) >>> type ( nested_cars . data [ 0 ]) < class ' pandas . core . frame . DataFrame '> Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def nest ( __data , * args , key = \"data\" ): \"\"\"Nest columns within a DataFrame. Parameters ---------- __data: A DataFrame. *args: The names of columns to be nested. May use any syntax used by the `select` function. key: The name of the column that will hold the nested columns. Examples -------- >>> from siuba import _, nest >>> from siuba.data import cars >>> nested_cars = cars >> nest(-_.cyl) Note that pandas with nested DataFrames looks okay in juypter notebooks, but has a weird representation in the IPython console, so the example below shows that each entry in the data column is a DataFrame. >>> nested_cars.shape (3, 2) >>> type(nested_cars.data[0]) <class 'pandas.core.frame.DataFrame'> \"\"\" # TODO: copied from select function var_list = var_create ( * args ) od = var_select ( __data . columns , * var_list ) # unselected columns are treated similar to using groupby grp_keys = list ( k for k in __data . columns if k not in set ( od )) nest_keys = list ( od ) # split into sub DataFrames, with only nest_keys as columns g_df = __data . groupby ( grp_keys ) splitter = g_df . grouper . _get_splitter ( g_df . obj [ nest_keys ]) # TODO: iterating over splitter now only produces 1 item (the dataframe) # check backwards compat def _extract_subdf_pandas_1_3 ( entry ): # in pandas < 1.3, splitter.__iter__ returns tuple entries (ii, df) if isinstance ( entry , tuple ): return entry [ 1 ] # in pandas 1.3, each entry is just the dataframe return entry result_index = g_df . grouper . result_index nested_dfs = [ _extract_subdf_pandas_1_3 ( x ) for x in splitter ] out = pd . DataFrame ({ key : nested_dfs }, index = result_index ) . reset_index () return out","title":"nest()"},{"location":"verb-nest/#siuba.dply.verbs.unnest","text":"Unnest a column holding nested data (e.g. Series of lists or DataFrames). Parameters: Name Type Description Default ___data A DataFrame. required key The name of the column to be unnested. 'data' Examples: >>> import pandas as pd >>> df = pd . DataFrame ({ 'id' : [ 1 , 2 ], 'data' : [[ 'a' , 'b' ], [ 'c' ]]}) >>> df >> unnest () id data 0 1 a 1 1 b 2 2 c Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def unnest ( __data , key = \"data\" ): \"\"\"Unnest a column holding nested data (e.g. Series of lists or DataFrames). Parameters ---------- ___data: A DataFrame. key: The name of the column to be unnested. Examples -------- >>> import pandas as pd >>> df = pd.DataFrame({'id': [1,2], 'data': [['a', 'b'], ['c']]}) >>> df >> unnest() id data 0 1 a 1 1 b 2 2 c \"\"\" # TODO: currently only takes key, not expressions nrows_nested = __data [ key ] . apply ( len , convert_dtype = True ) indx_nested = nrows_nested . index . repeat ( nrows_nested ) grp_keys = list ( __data . columns [ __data . columns != key ]) # flatten nested data data_entries = map ( _convert_nested_entry , __data [ key ]) long_data = pd . concat ( data_entries , ignore_index = True ) long_data . name = key # may be a better approach using a multi-index long_grp = __data . loc [ indx_nested , grp_keys ] . reset_index ( drop = True ) return long_grp . join ( long_data )","title":"unnest()"},{"location":"verb-rename/","text":"rename ( __data , ** kwargs ) Rename columns of a table. Parameters: Name Type Description Default __data The input table. required **kwargs Keyword arguments of the form new_name = _.old_name, or new_name = \"old_name\". {} Examples: >>> import pandas as pd >>> from siuba import _ , rename , select >>> df = pd . DataFrame ({ \"zzz\" : [ 1 ], \"b\" : [ 2 ]}) >>> df >> rename ( a = _ . zzz ) a b 0 1 2 Note that this is equivalent to this select code: >>> df >> select ( _ . a == _ . zzz , _ . b ) a b 0 1 2 Source code in siuba/dply/verbs.py @singledispatch2 ( DataFrame ) def rename ( __data , ** kwargs ): \"\"\"Rename columns of a table. Parameters ---------- __data: The input table. **kwargs: Keyword arguments of the form new_name = _.old_name, or new_name = \"old_name\". Examples -------- >>> import pandas as pd >>> from siuba import _, rename, select >>> df = pd.DataFrame({\"zzz\": [1], \"b\": [2]}) >>> df >> rename(a = _.zzz) a b 0 1 2 Note that this is equivalent to this select code: >>> df >> select(_.a == _.zzz, _.b) a b 0 1 2 \"\"\" # TODO: allow names with spaces, etc.. col_names = { simple_varname ( v ): k for k , v in kwargs . items ()} if None in col_names : raise ValueError ( \"Rename needs column name (e.g. 'a' or _.a), but received %s \" % col_names [ None ]) return __data . rename ( columns = col_names )","title":"rename"},{"location":"verb-rename/#siuba.dply.verbs.rename","text":"Rename columns of a table. Parameters: Name Type Description Default __data The input table. required **kwargs Keyword arguments of the form new_name = _.old_name, or new_name = \"old_name\". {} Examples: >>> import pandas as pd >>> from siuba import _ , rename , select >>> df = pd . DataFrame ({ \"zzz\" : [ 1 ], \"b\" : [ 2 ]}) >>> df >> rename ( a = _ . zzz ) a b 0 1 2 Note that this is equivalent to this select code: >>> df >> select ( _ . a == _ . zzz , _ . b ) a b 0 1 2 Source code in siuba/dply/verbs.py @singledispatch2 ( DataFrame ) def rename ( __data , ** kwargs ): \"\"\"Rename columns of a table. Parameters ---------- __data: The input table. **kwargs: Keyword arguments of the form new_name = _.old_name, or new_name = \"old_name\". Examples -------- >>> import pandas as pd >>> from siuba import _, rename, select >>> df = pd.DataFrame({\"zzz\": [1], \"b\": [2]}) >>> df >> rename(a = _.zzz) a b 0 1 2 Note that this is equivalent to this select code: >>> df >> select(_.a == _.zzz, _.b) a b 0 1 2 \"\"\" # TODO: allow names with spaces, etc.. col_names = { simple_varname ( v ): k for k , v in kwargs . items ()} if None in col_names : raise ValueError ( \"Rename needs column name (e.g. 'a' or _.a), but received %s \" % col_names [ None ]) return __data . rename ( columns = col_names )","title":"rename()"},{"location":"verb-select/","text":"select ( __data , * args , ** kwargs ) Select columns of a table to keep or drop (and optionally rename). Parameters: Name Type Description Default __data The input table. required *args An expression specifying columns to keep or drop. () **kwargs Not implemented. {} Examples: >>> from siuba import _ , select >>> from siuba.data import cars >>> small_cars = cars . head ( 1 ) >>> small_cars cyl mpg hp 0 6 21.0 110 You can refer to columns by name or position. >>> small_cars >> select ( _ . cyl , _ [ 2 ]) cyl hp 0 6 110 Use a ~ sign to exclude a column. >>> small_cars >> select ( ~ _ . cyl ) mpg hp 0 21.0 110 You can use any methods you'd find on the .columns.str accessor: >>> small_cars . columns . str . contains ( \"p\" ) array ([ False , True , True ]) >>> small_cars >> select ( _ . contains ( \"p\" )) mpg hp 0 21.0 110 Use a slice to select a range of columns: >>> small_cars >> select ( _ [ 0 : 2 ]) cyl mpg 0 6 21.0 Multiple expressions can be combined using _[a, b, c] syntax. This is useful for dropping a complex set of matches. >>> small_cars >> select ( ~ _ [ _ . startswith ( \"c\" ), - 1 ]) mpg 0 21.0 Source code in siuba/dply/verbs.py @singledispatch2 ( DataFrame ) def select ( __data , * args , ** kwargs ): \"\"\"Select columns of a table to keep or drop (and optionally rename). Parameters ---------- __data: The input table. *args: An expression specifying columns to keep or drop. **kwargs: Not implemented. Examples -------- >>> from siuba import _, select >>> from siuba.data import cars >>> small_cars = cars.head(1) >>> small_cars cyl mpg hp 0 6 21.0 110 You can refer to columns by name or position. >>> small_cars >> select(_.cyl, _[2]) cyl hp 0 6 110 Use a `~` sign to exclude a column. >>> small_cars >> select(~_.cyl) mpg hp 0 21.0 110 You can use any methods you'd find on the .columns.str accessor: >>> small_cars.columns.str.contains(\"p\") array([False, True, True]) >>> small_cars >> select(_.contains(\"p\")) mpg hp 0 21.0 110 Use a slice to select a range of columns: >>> small_cars >> select(_[0:2]) cyl mpg 0 6 21.0 Multiple expressions can be combined using _[a, b, c] syntax. This is useful for dropping a complex set of matches. >>> small_cars >> select(~_[_.startswith(\"c\"), -1]) mpg 0 21.0 \"\"\" if kwargs : raise NotImplementedError ( \"Using kwargs in select not currently supported. \" \"Use _.newname == _.oldname instead\" ) var_list = var_create ( * args ) od = var_select ( __data . columns , * var_list ) to_rename = { k : v for k , v in od . items () if v is not None } return __data [ list ( od )] . rename ( columns = to_rename )","title":"select"},{"location":"verb-select/#siuba.dply.verbs.select","text":"Select columns of a table to keep or drop (and optionally rename). Parameters: Name Type Description Default __data The input table. required *args An expression specifying columns to keep or drop. () **kwargs Not implemented. {} Examples: >>> from siuba import _ , select >>> from siuba.data import cars >>> small_cars = cars . head ( 1 ) >>> small_cars cyl mpg hp 0 6 21.0 110 You can refer to columns by name or position. >>> small_cars >> select ( _ . cyl , _ [ 2 ]) cyl hp 0 6 110 Use a ~ sign to exclude a column. >>> small_cars >> select ( ~ _ . cyl ) mpg hp 0 21.0 110 You can use any methods you'd find on the .columns.str accessor: >>> small_cars . columns . str . contains ( \"p\" ) array ([ False , True , True ]) >>> small_cars >> select ( _ . contains ( \"p\" )) mpg hp 0 21.0 110 Use a slice to select a range of columns: >>> small_cars >> select ( _ [ 0 : 2 ]) cyl mpg 0 6 21.0 Multiple expressions can be combined using _[a, b, c] syntax. This is useful for dropping a complex set of matches. >>> small_cars >> select ( ~ _ [ _ . startswith ( \"c\" ), - 1 ]) mpg 0 21.0 Source code in siuba/dply/verbs.py @singledispatch2 ( DataFrame ) def select ( __data , * args , ** kwargs ): \"\"\"Select columns of a table to keep or drop (and optionally rename). Parameters ---------- __data: The input table. *args: An expression specifying columns to keep or drop. **kwargs: Not implemented. Examples -------- >>> from siuba import _, select >>> from siuba.data import cars >>> small_cars = cars.head(1) >>> small_cars cyl mpg hp 0 6 21.0 110 You can refer to columns by name or position. >>> small_cars >> select(_.cyl, _[2]) cyl hp 0 6 110 Use a `~` sign to exclude a column. >>> small_cars >> select(~_.cyl) mpg hp 0 21.0 110 You can use any methods you'd find on the .columns.str accessor: >>> small_cars.columns.str.contains(\"p\") array([False, True, True]) >>> small_cars >> select(_.contains(\"p\")) mpg hp 0 21.0 110 Use a slice to select a range of columns: >>> small_cars >> select(_[0:2]) cyl mpg 0 6 21.0 Multiple expressions can be combined using _[a, b, c] syntax. This is useful for dropping a complex set of matches. >>> small_cars >> select(~_[_.startswith(\"c\"), -1]) mpg 0 21.0 \"\"\" if kwargs : raise NotImplementedError ( \"Using kwargs in select not currently supported. \" \"Use _.newname == _.oldname instead\" ) var_list = var_create ( * args ) od = var_select ( __data . columns , * var_list ) to_rename = { k : v for k , v in od . items () if v is not None } return __data [ list ( od )] . rename ( columns = to_rename )","title":"select()"},{"location":"verb-semi_join/","text":"semi_join ( left , right = None , on = None ) Return the left table with every row that would be kept in an inner join. Parameters: Name Type Description Default left The left-hand table. required right The right-hand table. None on How to match them. By default it uses matches all columns with the same name across the two tables. None Examples: >>> import pandas as pd >>> from siuba import _ , semi_join , anti_join >>> df1 = pd . DataFrame ({ \"id\" : [ 1 , 2 , 3 ], \"x\" : [ \"a\" , \"b\" , \"c\" ]}) >>> df2 = pd . DataFrame ({ \"id\" : [ 2 , 3 , 3 ], \"y\" : [ \"l\" , \"m\" , \"n\" ]}) >>> df1 >> semi_join ( _ , df2 ) id x 1 2 b 2 3 c >>> df1 >> anti_join ( _ , df2 ) id x 0 1 a Generally, it's a good idea to explicitly specify the on argument. >>> df1 >> anti_join ( _ , df2 , on = \"id\" ) id x 0 1 a Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def semi_join ( left , right = None , on = None ): \"\"\"Return the left table with every row that would be kept in an inner join. Parameters ---------- left : The left-hand table. right : The right-hand table. on : How to match them. By default it uses matches all columns with the same name across the two tables. Examples -------- >>> import pandas as pd >>> from siuba import _, semi_join, anti_join >>> df1 = pd.DataFrame({\"id\": [1, 2, 3], \"x\": [\"a\", \"b\", \"c\"]}) >>> df2 = pd.DataFrame({\"id\": [2, 3, 3], \"y\": [\"l\", \"m\", \"n\"]}) >>> df1 >> semi_join(_, df2) id x 1 2 b 2 3 c >>> df1 >> anti_join(_, df2) id x 0 1 a Generally, it's a good idea to explicitly specify the on argument. >>> df1 >> anti_join(_, df2, on=\"id\") id x 0 1 a \"\"\" if isinstance ( on , Mapping ): # coerce colnames to list, to avoid indexing with tuples on_cols , right_on = map ( list , zip ( * on . items ())) right = right [ right_on ] . rename ( dict ( zip ( right_on , on_cols ))) elif on is None : warnings . warn ( \"No on column passed to join. \" \"Inferring join columns instead using shared column names.\" ) on_cols = list ( set ( left . columns ) . intersection ( set ( right . columns ))) if not len ( on_cols ): raise Exception ( \"No join column specified, and no shared column names\" ) warnings . warn ( \"Detected shared columns: %s \" % on_cols ) elif isinstance ( on , str ): on_cols = [ on ] else : on_cols = on # get our semi join on ---- if len ( on_cols ) == 1 : col_name = on_cols [ 0 ] indx = left [ col_name ] . isin ( right [ col_name ]) return left . loc [ indx ] # Not a super efficient approach. Effectively, an inner join with what would # be duplicate rows removed. merger = _MergeOperation ( left , right , left_on = on_cols , right_on = on_cols ) _ , l_indx , _ = merger . _get_join_info () range_indx = pd . RangeIndex ( len ( left )) return left . loc [ range_indx . isin ( l_indx )]","title":"Verb semi join"},{"location":"verb-semi_join/#siuba.dply.verbs.semi_join","text":"Return the left table with every row that would be kept in an inner join. Parameters: Name Type Description Default left The left-hand table. required right The right-hand table. None on How to match them. By default it uses matches all columns with the same name across the two tables. None Examples: >>> import pandas as pd >>> from siuba import _ , semi_join , anti_join >>> df1 = pd . DataFrame ({ \"id\" : [ 1 , 2 , 3 ], \"x\" : [ \"a\" , \"b\" , \"c\" ]}) >>> df2 = pd . DataFrame ({ \"id\" : [ 2 , 3 , 3 ], \"y\" : [ \"l\" , \"m\" , \"n\" ]}) >>> df1 >> semi_join ( _ , df2 ) id x 1 2 b 2 3 c >>> df1 >> anti_join ( _ , df2 ) id x 0 1 a Generally, it's a good idea to explicitly specify the on argument. >>> df1 >> anti_join ( _ , df2 , on = \"id\" ) id x 0 1 a Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def semi_join ( left , right = None , on = None ): \"\"\"Return the left table with every row that would be kept in an inner join. Parameters ---------- left : The left-hand table. right : The right-hand table. on : How to match them. By default it uses matches all columns with the same name across the two tables. Examples -------- >>> import pandas as pd >>> from siuba import _, semi_join, anti_join >>> df1 = pd.DataFrame({\"id\": [1, 2, 3], \"x\": [\"a\", \"b\", \"c\"]}) >>> df2 = pd.DataFrame({\"id\": [2, 3, 3], \"y\": [\"l\", \"m\", \"n\"]}) >>> df1 >> semi_join(_, df2) id x 1 2 b 2 3 c >>> df1 >> anti_join(_, df2) id x 0 1 a Generally, it's a good idea to explicitly specify the on argument. >>> df1 >> anti_join(_, df2, on=\"id\") id x 0 1 a \"\"\" if isinstance ( on , Mapping ): # coerce colnames to list, to avoid indexing with tuples on_cols , right_on = map ( list , zip ( * on . items ())) right = right [ right_on ] . rename ( dict ( zip ( right_on , on_cols ))) elif on is None : warnings . warn ( \"No on column passed to join. \" \"Inferring join columns instead using shared column names.\" ) on_cols = list ( set ( left . columns ) . intersection ( set ( right . columns ))) if not len ( on_cols ): raise Exception ( \"No join column specified, and no shared column names\" ) warnings . warn ( \"Detected shared columns: %s \" % on_cols ) elif isinstance ( on , str ): on_cols = [ on ] else : on_cols = on # get our semi join on ---- if len ( on_cols ) == 1 : col_name = on_cols [ 0 ] indx = left [ col_name ] . isin ( right [ col_name ]) return left . loc [ indx ] # Not a super efficient approach. Effectively, an inner join with what would # be duplicate rows removed. merger = _MergeOperation ( left , right , left_on = on_cols , right_on = on_cols ) _ , l_indx , _ = merger . _get_join_info () range_indx = pd . RangeIndex ( len ( left )) return left . loc [ range_indx . isin ( l_indx )]","title":"semi_join()"},{"location":"verb-separate/","text":"separate ( __data , col , into , sep = '[^a-zA-Z0-9]' , remove = True , convert = False , extra = 'warn' , fill = 'warn' ) Split col into len(into) piece. Return DataFrame with a column added for each piece. Parameters: Name Type Description Default __data a DataFrame. required col name of column to split (either string, or siu expression). required into names of resulting columns holding each entry in split. required sep regular expression used to split col. Passed to col.str.split method. '[^a-zA-Z0-9]' remove whether to remove col from the returned DataFrame. True convert whether to attempt to convert the split columns to numerics. False extra what to do when more splits than into names. One of (\"warn\", \"drop\" or \"merge\"). \"warn\" produces a warning; \"drop\" and \"merge\" currently not implemented. 'warn' fill what to do when fewer splits than into names. Currently not implemented. 'warn' Examples: >>> import pandas as pd >>> from siuba import separate >>> df = pd . DataFrame ({ \"label\" : [ \"S1-1\" , \"S2-2\" ]}) Split into two columns: >>> separate ( df , \"label\" , into = [ \"season\" , \"episode\" ]) season episode 0 S1 1 1 S2 2 Split, and try to convert columns to numerics: >>> separate ( df , \"label\" , into = [ \"season\" , \"episode\" ], convert = True ) season episode 0 S1 1 1 S2 2 Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def separate ( __data , col , into , sep = r \"[^a-zA-Z0-9]\" , remove = True , convert = False , extra = \"warn\" , fill = \"warn\" ): \"\"\"Split col into len(into) piece. Return DataFrame with a column added for each piece. Parameters ---------- __data: a DataFrame. col: name of column to split (either string, or siu expression). into: names of resulting columns holding each entry in split. sep: regular expression used to split col. Passed to col.str.split method. remove: whether to remove col from the returned DataFrame. convert: whether to attempt to convert the split columns to numerics. extra: what to do when more splits than into names. One of (\"warn\", \"drop\" or \"merge\"). \"warn\" produces a warning; \"drop\" and \"merge\" currently not implemented. fill: what to do when fewer splits than into names. Currently not implemented. Examples -------- >>> import pandas as pd >>> from siuba import separate >>> df = pd.DataFrame({\"label\": [\"S1-1\", \"S2-2\"]}) Split into two columns: >>> separate(df, \"label\", into = [\"season\", \"episode\"]) season episode 0 S1 1 1 S2 2 Split, and try to convert columns to numerics: >>> separate(df, \"label\", into = [\"season\", \"episode\"], convert = True) season episode 0 S1 1 1 S2 2 \"\"\" n_into = len ( into ) col_name = simple_varname ( col ) # splitting column ---- all_splits = __data [ col_name ] . str . split ( sep , expand = True ) n_split_cols = len ( all_splits . columns ) # handling too many or too few splits ---- if n_split_cols < n_into : # too few columns raise ValueError ( \"Expected %s split cols, found %s \" % ( n_into , n_split_cols )) elif n_split_cols > n_into : # Extra argument controls how we deal with too many splits if extra == \"warn\" : warnings . warn ( \"some warning about too many splits\" , UserWarning ) elif extra == \"drop\" : pass elif extra == \"merge\" : raise NotImplementedError ( \"TODO: separate extra = 'merge'\" ) else : raise ValueError ( \"Invalid extra argument: %s \" % extra ) # end up with only the into columns, correctly named ---- new_names = dict ( zip ( range ( n_into ), into )) keep_splits = all_splits . iloc [:, : n_into ] . rename ( columns = new_names ) out = pd . concat ([ __data , keep_splits ], axis = 1 ) # attempt to convert columns to numeric ---- if convert : # TODO: better strategy here? for k in into : try : out [ k ] = pd . to_numeric ( out [ k ]) except ValueError : pass if remove : return out . drop ( columns = col_name ) return out unite ( __data , col , * args , * , sep = '_' , remove = True ) Combine multiple columns into a single column. Return DataFrame that column included. Parameters: Name Type Description Default __data a DataFrame required col name of the to-be-created column (string). required *args names of each column to combine. () sep separator joining each column being combined. '_' remove whether to remove the combined columns from the returned DataFrame. True Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def unite ( __data , col , * args , sep = \"_\" , remove = True ): \"\"\"Combine multiple columns into a single column. Return DataFrame that column included. Parameters ---------- __data: a DataFrame col: name of the to-be-created column (string). *args: names of each column to combine. sep: separator joining each column being combined. remove: whether to remove the combined columns from the returned DataFrame. \"\"\" unite_col_names = list ( map ( simple_varname , args )) out_col_name = simple_varname ( col ) # validations ---- if None in unite_col_names : raise ValueError ( \"*args must be string, or simple column name, e.g. _.col_name\" ) missing_cols = set ( unite_col_names ) - set ( __data . columns ) if missing_cols : raise ValueError ( \"columns %s not in DataFrame.columns\" % missing_cols ) unite_cols = [ _coerce_to_str ( __data [ col_name ]) for col_name in unite_col_names ] if out_col_name in __data : raise ValueError ( \"col argument %s already a column in data\" % out_col_name ) # perform unite ---- # TODO: this is probably not very efficient. Maybe try with transform or apply? res = reduce ( lambda x , y : x + sep + y , unite_cols ) out_df = __data . copy () out_df [ out_col_name ] = res if remove : return out_df . drop ( columns = unite_col_names ) return out_df","title":"separate, unite"},{"location":"verb-separate/#siuba.dply.verbs.separate","text":"Split col into len(into) piece. Return DataFrame with a column added for each piece. Parameters: Name Type Description Default __data a DataFrame. required col name of column to split (either string, or siu expression). required into names of resulting columns holding each entry in split. required sep regular expression used to split col. Passed to col.str.split method. '[^a-zA-Z0-9]' remove whether to remove col from the returned DataFrame. True convert whether to attempt to convert the split columns to numerics. False extra what to do when more splits than into names. One of (\"warn\", \"drop\" or \"merge\"). \"warn\" produces a warning; \"drop\" and \"merge\" currently not implemented. 'warn' fill what to do when fewer splits than into names. Currently not implemented. 'warn' Examples: >>> import pandas as pd >>> from siuba import separate >>> df = pd . DataFrame ({ \"label\" : [ \"S1-1\" , \"S2-2\" ]}) Split into two columns: >>> separate ( df , \"label\" , into = [ \"season\" , \"episode\" ]) season episode 0 S1 1 1 S2 2 Split, and try to convert columns to numerics: >>> separate ( df , \"label\" , into = [ \"season\" , \"episode\" ], convert = True ) season episode 0 S1 1 1 S2 2 Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def separate ( __data , col , into , sep = r \"[^a-zA-Z0-9]\" , remove = True , convert = False , extra = \"warn\" , fill = \"warn\" ): \"\"\"Split col into len(into) piece. Return DataFrame with a column added for each piece. Parameters ---------- __data: a DataFrame. col: name of column to split (either string, or siu expression). into: names of resulting columns holding each entry in split. sep: regular expression used to split col. Passed to col.str.split method. remove: whether to remove col from the returned DataFrame. convert: whether to attempt to convert the split columns to numerics. extra: what to do when more splits than into names. One of (\"warn\", \"drop\" or \"merge\"). \"warn\" produces a warning; \"drop\" and \"merge\" currently not implemented. fill: what to do when fewer splits than into names. Currently not implemented. Examples -------- >>> import pandas as pd >>> from siuba import separate >>> df = pd.DataFrame({\"label\": [\"S1-1\", \"S2-2\"]}) Split into two columns: >>> separate(df, \"label\", into = [\"season\", \"episode\"]) season episode 0 S1 1 1 S2 2 Split, and try to convert columns to numerics: >>> separate(df, \"label\", into = [\"season\", \"episode\"], convert = True) season episode 0 S1 1 1 S2 2 \"\"\" n_into = len ( into ) col_name = simple_varname ( col ) # splitting column ---- all_splits = __data [ col_name ] . str . split ( sep , expand = True ) n_split_cols = len ( all_splits . columns ) # handling too many or too few splits ---- if n_split_cols < n_into : # too few columns raise ValueError ( \"Expected %s split cols, found %s \" % ( n_into , n_split_cols )) elif n_split_cols > n_into : # Extra argument controls how we deal with too many splits if extra == \"warn\" : warnings . warn ( \"some warning about too many splits\" , UserWarning ) elif extra == \"drop\" : pass elif extra == \"merge\" : raise NotImplementedError ( \"TODO: separate extra = 'merge'\" ) else : raise ValueError ( \"Invalid extra argument: %s \" % extra ) # end up with only the into columns, correctly named ---- new_names = dict ( zip ( range ( n_into ), into )) keep_splits = all_splits . iloc [:, : n_into ] . rename ( columns = new_names ) out = pd . concat ([ __data , keep_splits ], axis = 1 ) # attempt to convert columns to numeric ---- if convert : # TODO: better strategy here? for k in into : try : out [ k ] = pd . to_numeric ( out [ k ]) except ValueError : pass if remove : return out . drop ( columns = col_name ) return out","title":"separate()"},{"location":"verb-separate/#siuba.dply.verbs.unite","text":"Combine multiple columns into a single column. Return DataFrame that column included. Parameters: Name Type Description Default __data a DataFrame required col name of the to-be-created column (string). required *args names of each column to combine. () sep separator joining each column being combined. '_' remove whether to remove the combined columns from the returned DataFrame. True Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def unite ( __data , col , * args , sep = \"_\" , remove = True ): \"\"\"Combine multiple columns into a single column. Return DataFrame that column included. Parameters ---------- __data: a DataFrame col: name of the to-be-created column (string). *args: names of each column to combine. sep: separator joining each column being combined. remove: whether to remove the combined columns from the returned DataFrame. \"\"\" unite_col_names = list ( map ( simple_varname , args )) out_col_name = simple_varname ( col ) # validations ---- if None in unite_col_names : raise ValueError ( \"*args must be string, or simple column name, e.g. _.col_name\" ) missing_cols = set ( unite_col_names ) - set ( __data . columns ) if missing_cols : raise ValueError ( \"columns %s not in DataFrame.columns\" % missing_cols ) unite_cols = [ _coerce_to_str ( __data [ col_name ]) for col_name in unite_col_names ] if out_col_name in __data : raise ValueError ( \"col argument %s already a column in data\" % out_col_name ) # perform unite ---- # TODO: this is probably not very efficient. Maybe try with transform or apply? res = reduce ( lambda x , y : x + sep + y , unite_cols ) out_df = __data . copy () out_df [ out_col_name ] = res if remove : return out_df . drop ( columns = unite_col_names ) return out_df","title":"unite()"},{"location":"verb-show_query/","text":"show_query ( __data , simplify = False ) Print the details of a query. Parameters: Name Type Description Default __data A DataFrame of siuba.sql.LazyTbl. required simplify Whether to attempt to simplify the query. False **kwargs Additional arguments passed to specific implementations. required Source code in siuba/dply/verbs.py @pipe_no_args @singledispatch2 (( DataFrame , DataFrameGroupBy )) def show_query ( __data , simplify = False ): \"\"\"Print the details of a query. Parameters ---------- __data: A DataFrame of siuba.sql.LazyTbl. simplify: Whether to attempt to simplify the query. **kwargs: Additional arguments passed to specific implementations. \"\"\" print ( \"No query to show for a DataFrame\" ) return __data","title":"show_query"},{"location":"verb-show_query/#siuba.dply.verbs.show_query","text":"Print the details of a query. Parameters: Name Type Description Default __data A DataFrame of siuba.sql.LazyTbl. required simplify Whether to attempt to simplify the query. False **kwargs Additional arguments passed to specific implementations. required Source code in siuba/dply/verbs.py @pipe_no_args @singledispatch2 (( DataFrame , DataFrameGroupBy )) def show_query ( __data , simplify = False ): \"\"\"Print the details of a query. Parameters ---------- __data: A DataFrame of siuba.sql.LazyTbl. simplify: Whether to attempt to simplify the query. **kwargs: Additional arguments passed to specific implementations. \"\"\" print ( \"No query to show for a DataFrame\" ) return __data","title":"show_query()"},{"location":"verb-spread/","text":"spread ( __data , key , value , fill = None , reset_index = True ) Reshape table by spreading it out to wide format. Parameters: Name Type Description Default __data The input data. required key Column whose values will be used as new column names. required value Column whose values will fill the new column entries. required fill Value to set for any missing values. By default keeps them as missing values. None Examples: >>> import pandas as pd >>> from siuba import _ , gather >>> df = pd . DataFrame ({ \"id\" : [ \"a\" , \"b\" ], \"x\" : [ 1 , 2 ], \"y\" : [ 3 , None ]}) >>> long = gather ( df , \"key\" , \"value\" , - _ . id , drop_na = True ) >>> long id key value 0 a x 1.0 1 b x 2.0 2 a y 3.0 >>> spread ( long , \"key\" , \"value\" ) id x y 0 a 1.0 3.0 1 b 2.0 NaN Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def spread ( __data , key , value , fill = None , reset_index = True ): \"\"\"Reshape table by spreading it out to wide format. Parameters ---------- __data: The input data. key: Column whose values will be used as new column names. value: Column whose values will fill the new column entries. fill: Value to set for any missing values. By default keeps them as missing values. Examples -------- >>> import pandas as pd >>> from siuba import _, gather >>> df = pd.DataFrame({\"id\": [\"a\", \"b\"], \"x\": [1, 2], \"y\": [3, None]}) >>> long = gather(df, \"key\", \"value\", -_.id, drop_na=True) >>> long id key value 0 a x 1.0 1 b x 2.0 2 a y 3.0 >>> spread(long, \"key\", \"value\") id x y 0 a 1.0 3.0 1 b 2.0 NaN \"\"\" key_col = _get_single_var_select ( __data . columns , key ) val_col = _get_single_var_select ( __data . columns , value ) id_cols = [ col for col in __data . columns if col not in ( key_col , val_col )] wide = __data . set_index ( id_cols + [ key_col ]) . unstack ( level = - 1 ) if fill is not None : wide . fillna ( fill , inplace = True ) # remove multi-index from both rows and cols wide . columns = wide . columns . droplevel () . rename ( None ) if reset_index : wide . reset_index ( inplace = True ) return wide","title":"Verb spread"},{"location":"verb-spread/#siuba.dply.verbs.spread","text":"Reshape table by spreading it out to wide format. Parameters: Name Type Description Default __data The input data. required key Column whose values will be used as new column names. required value Column whose values will fill the new column entries. required fill Value to set for any missing values. By default keeps them as missing values. None Examples: >>> import pandas as pd >>> from siuba import _ , gather >>> df = pd . DataFrame ({ \"id\" : [ \"a\" , \"b\" ], \"x\" : [ 1 , 2 ], \"y\" : [ 3 , None ]}) >>> long = gather ( df , \"key\" , \"value\" , - _ . id , drop_na = True ) >>> long id key value 0 a x 1.0 1 b x 2.0 2 a y 3.0 >>> spread ( long , \"key\" , \"value\" ) id x y 0 a 1.0 3.0 1 b 2.0 NaN Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def spread ( __data , key , value , fill = None , reset_index = True ): \"\"\"Reshape table by spreading it out to wide format. Parameters ---------- __data: The input data. key: Column whose values will be used as new column names. value: Column whose values will fill the new column entries. fill: Value to set for any missing values. By default keeps them as missing values. Examples -------- >>> import pandas as pd >>> from siuba import _, gather >>> df = pd.DataFrame({\"id\": [\"a\", \"b\"], \"x\": [1, 2], \"y\": [3, None]}) >>> long = gather(df, \"key\", \"value\", -_.id, drop_na=True) >>> long id key value 0 a x 1.0 1 b x 2.0 2 a y 3.0 >>> spread(long, \"key\", \"value\") id x y 0 a 1.0 3.0 1 b 2.0 NaN \"\"\" key_col = _get_single_var_select ( __data . columns , key ) val_col = _get_single_var_select ( __data . columns , value ) id_cols = [ col for col in __data . columns if col not in ( key_col , val_col )] wide = __data . set_index ( id_cols + [ key_col ]) . unstack ( level = - 1 ) if fill is not None : wide . fillna ( fill , inplace = True ) # remove multi-index from both rows and cols wide . columns = wide . columns . droplevel () . rename ( None ) if reset_index : wide . reset_index ( inplace = True ) return wide","title":"spread()"},{"location":"verb-summarize/","text":"summarize ( __data , ** kwargs ) Assign variables that are single number summaries of a DataFrame. Grouped DataFrames will produce one row for each group. Otherwise, summarize produces a DataFrame with a single row. Parameters: Name Type Description Default __data a DataFrame The data being summarized. required **kwargs new_col_name=value pairs, where value can be a function taking a single argument for the data being operated on. {} Examples: >>> from siuba import _ , group_by , summarize >>> from siuba.data import cars >>> cars >> summarize ( avg = _ . mpg . mean (), n = _ . shape [ 0 ]) avg n 0 20.090625 32 >>> g_cyl = cars >> group_by ( _ . cyl ) >>> g_cyl >> summarize ( min = _ . mpg . min ()) cyl min 0 4 21.4 1 6 17.8 2 8 10.4 >>> g_cyl >> summarize ( mpg_std_err = _ . mpg . std () / _ . shape [ 0 ] ** .5 ) cyl mpg_std_err 0 4 1.359764 1 6 0.549397 2 8 0.684202 Source code in siuba/dply/verbs.py @singledispatch2 ( DataFrame ) def summarize ( __data , ** kwargs ): \"\"\"Assign variables that are single number summaries of a DataFrame. Grouped DataFrames will produce one row for each group. Otherwise, summarize produces a DataFrame with a single row. Parameters ---------- __data: a DataFrame The data being summarized. **kwargs: new_col_name=value pairs, where value can be a function taking a single argument for the data being operated on. Examples -------- >>> from siuba import _, group_by, summarize >>> from siuba.data import cars >>> cars >> summarize(avg = _.mpg.mean(), n = _.shape[0]) avg n 0 20.090625 32 >>> g_cyl = cars >> group_by(_.cyl) >>> g_cyl >> summarize(min = _.mpg.min()) cyl min 0 4 21.4 1 6 17.8 2 8 10.4 >>> g_cyl >> summarize(mpg_std_err = _.mpg.std() / _.shape[0]**.5) cyl mpg_std_err 0 4 1.359764 1 6 0.549397 2 8 0.684202 \"\"\" results = {} for k , v in kwargs . items (): res = v ( __data ) if callable ( v ) else v # validate operations returned single result if not is_scalar ( res ) and len ( res ) > 1 : raise ValueError ( \"Summarize argument, %s , must return result of length 1 or a scalar.\" % k ) # keep result, but use underlying array to avoid crazy index issues # on DataFrame construction (#138) results [ k ] = res . array if isinstance ( res , pd . Series ) else res # must pass index, or raises error when using all scalar values return DataFrame ( results , index = [ 0 ])","title":"summarize"},{"location":"verb-summarize/#siuba.dply.verbs.summarize","text":"Assign variables that are single number summaries of a DataFrame. Grouped DataFrames will produce one row for each group. Otherwise, summarize produces a DataFrame with a single row. Parameters: Name Type Description Default __data a DataFrame The data being summarized. required **kwargs new_col_name=value pairs, where value can be a function taking a single argument for the data being operated on. {} Examples: >>> from siuba import _ , group_by , summarize >>> from siuba.data import cars >>> cars >> summarize ( avg = _ . mpg . mean (), n = _ . shape [ 0 ]) avg n 0 20.090625 32 >>> g_cyl = cars >> group_by ( _ . cyl ) >>> g_cyl >> summarize ( min = _ . mpg . min ()) cyl min 0 4 21.4 1 6 17.8 2 8 10.4 >>> g_cyl >> summarize ( mpg_std_err = _ . mpg . std () / _ . shape [ 0 ] ** .5 ) cyl mpg_std_err 0 4 1.359764 1 6 0.549397 2 8 0.684202 Source code in siuba/dply/verbs.py @singledispatch2 ( DataFrame ) def summarize ( __data , ** kwargs ): \"\"\"Assign variables that are single number summaries of a DataFrame. Grouped DataFrames will produce one row for each group. Otherwise, summarize produces a DataFrame with a single row. Parameters ---------- __data: a DataFrame The data being summarized. **kwargs: new_col_name=value pairs, where value can be a function taking a single argument for the data being operated on. Examples -------- >>> from siuba import _, group_by, summarize >>> from siuba.data import cars >>> cars >> summarize(avg = _.mpg.mean(), n = _.shape[0]) avg n 0 20.090625 32 >>> g_cyl = cars >> group_by(_.cyl) >>> g_cyl >> summarize(min = _.mpg.min()) cyl min 0 4 21.4 1 6 17.8 2 8 10.4 >>> g_cyl >> summarize(mpg_std_err = _.mpg.std() / _.shape[0]**.5) cyl mpg_std_err 0 4 1.359764 1 6 0.549397 2 8 0.684202 \"\"\" results = {} for k , v in kwargs . items (): res = v ( __data ) if callable ( v ) else v # validate operations returned single result if not is_scalar ( res ) and len ( res ) > 1 : raise ValueError ( \"Summarize argument, %s , must return result of length 1 or a scalar.\" % k ) # keep result, but use underlying array to avoid crazy index issues # on DataFrame construction (#138) results [ k ] = res . array if isinstance ( res , pd . Series ) else res # must pass index, or raises error when using all scalar values return DataFrame ( results , index = [ 0 ])","title":"summarize()"},{"location":"verb-top_n/","text":"top_n ( __data , n , wt = None ) Filter to keep the top or bottom entries in each group. Parameters: Name Type Description Default ___data A DataFrame. required n The number of rows to keep in each group. required wt A column or expression that determines ordering (defaults to last column in data). None Examples: >>> from siuba import _ , top_n >>> df = pd . DataFrame ({ 'x' : [ 3 , 1 , 2 , 4 ], 'y' : [ 1 , 1 , 0 , 0 ]}) >>> top_n ( df , 2 , _ . x ) x y 0 3 1 3 4 0 >>> top_n ( df , - 2 , _ . x ) x y 1 1 1 2 2 0 >>> top_n ( df , 2 , _ . x * _ . y ) x y 0 3 1 1 1 1 Source code in siuba/dply/verbs.py @singledispatch2 (( pd . DataFrame , DataFrameGroupBy )) def top_n ( __data , n , wt = None ): \"\"\"Filter to keep the top or bottom entries in each group. Parameters ---------- ___data: A DataFrame. n: The number of rows to keep in each group. wt: A column or expression that determines ordering (defaults to last column in data). Examples -------- >>> from siuba import _, top_n >>> df = pd.DataFrame({'x': [3, 1, 2, 4], 'y': [1, 1, 0, 0]}) >>> top_n(df, 2, _.x) x y 0 3 1 3 4 0 >>> top_n(df, -2, _.x) x y 1 1 1 2 2 0 >>> top_n(df, 2, _.x*_.y) x y 0 3 1 1 1 1 \"\"\" # NOTE: using min_rank, since it can return a lazy expr for min_rank(ing) # but I would rather not have it imported in verbs. will be more # reasonable if each verb were its own file? need abstract verb / vector module. # vector imports experimental right now, so we need to invert deps # TODO: # * what if wt is a string? should remove all str -> expr in verbs like group_by etc.. # * verbs like filter allow lambdas, but this func breaks with that from .vector import min_rank if wt is None : sym_wt = getattr ( Symbolic ( MetaArg ( \"_\" )), __data . columns [ - 1 ]) elif isinstance ( wt , Call ): sym_wt = Symbolic ( wt ) else : raise TypeError ( \"wt must be a symbolic expression, eg. _.some_col\" ) if n > 0 : return filter ( __data , min_rank ( - sym_wt ) <= n ) else : return filter ( __data , min_rank ( sym_wt ) <= abs ( n ))","title":"Verb top n"},{"location":"verb-top_n/#siuba.dply.verbs.top_n","text":"Filter to keep the top or bottom entries in each group. Parameters: Name Type Description Default ___data A DataFrame. required n The number of rows to keep in each group. required wt A column or expression that determines ordering (defaults to last column in data). None Examples: >>> from siuba import _ , top_n >>> df = pd . DataFrame ({ 'x' : [ 3 , 1 , 2 , 4 ], 'y' : [ 1 , 1 , 0 , 0 ]}) >>> top_n ( df , 2 , _ . x ) x y 0 3 1 3 4 0 >>> top_n ( df , - 2 , _ . x ) x y 1 1 1 2 2 0 >>> top_n ( df , 2 , _ . x * _ . y ) x y 0 3 1 1 1 1 Source code in siuba/dply/verbs.py @singledispatch2 (( pd . DataFrame , DataFrameGroupBy )) def top_n ( __data , n , wt = None ): \"\"\"Filter to keep the top or bottom entries in each group. Parameters ---------- ___data: A DataFrame. n: The number of rows to keep in each group. wt: A column or expression that determines ordering (defaults to last column in data). Examples -------- >>> from siuba import _, top_n >>> df = pd.DataFrame({'x': [3, 1, 2, 4], 'y': [1, 1, 0, 0]}) >>> top_n(df, 2, _.x) x y 0 3 1 3 4 0 >>> top_n(df, -2, _.x) x y 1 1 1 2 2 0 >>> top_n(df, 2, _.x*_.y) x y 0 3 1 1 1 1 \"\"\" # NOTE: using min_rank, since it can return a lazy expr for min_rank(ing) # but I would rather not have it imported in verbs. will be more # reasonable if each verb were its own file? need abstract verb / vector module. # vector imports experimental right now, so we need to invert deps # TODO: # * what if wt is a string? should remove all str -> expr in verbs like group_by etc.. # * verbs like filter allow lambdas, but this func breaks with that from .vector import min_rank if wt is None : sym_wt = getattr ( Symbolic ( MetaArg ( \"_\" )), __data . columns [ - 1 ]) elif isinstance ( wt , Call ): sym_wt = Symbolic ( wt ) else : raise TypeError ( \"wt must be a symbolic expression, eg. _.some_col\" ) if n > 0 : return filter ( __data , min_rank ( - sym_wt ) <= n ) else : return filter ( __data , min_rank ( sym_wt ) <= abs ( n ))","title":"top_n()"},{"location":"verb-ungroup/","text":"ungroup ( __data ) Return an ungrouped DataFrame. Parameters: Name Type Description Default __data The data being ungrouped. required Examples: >>> from siuba import _ , group_by , ungroup >>> from siuba.data import cars >>> g_cyl = cars . groupby ( \"cyl\" ) >>> res1 = ungroup ( g_cyl ) >>> res2 = cars >> group_by ( _ . cyl ) >> ungroup () Source code in siuba/dply/verbs.py @singledispatch2 (( pd . DataFrame , DataFrameGroupBy )) def ungroup ( __data ): \"\"\"Return an ungrouped DataFrame. Parameters ---------- __data: The data being ungrouped. Examples -------- >>> from siuba import _, group_by, ungroup >>> from siuba.data import cars >>> g_cyl = cars.groupby(\"cyl\") >>> res1 = ungroup(g_cyl) >>> res2 = cars >> group_by(_.cyl) >> ungroup() \"\"\" # TODO: can we somehow just restore the original df used to construct # the groupby? if isinstance ( __data , pd . DataFrame ): return __data if isinstance ( __data , pd . Series ): return __data . reset_index () return __data . obj . reset_index ( drop = True )","title":"Verb ungroup"},{"location":"verb-ungroup/#siuba.dply.verbs.ungroup","text":"Return an ungrouped DataFrame. Parameters: Name Type Description Default __data The data being ungrouped. required Examples: >>> from siuba import _ , group_by , ungroup >>> from siuba.data import cars >>> g_cyl = cars . groupby ( \"cyl\" ) >>> res1 = ungroup ( g_cyl ) >>> res2 = cars >> group_by ( _ . cyl ) >> ungroup () Source code in siuba/dply/verbs.py @singledispatch2 (( pd . DataFrame , DataFrameGroupBy )) def ungroup ( __data ): \"\"\"Return an ungrouped DataFrame. Parameters ---------- __data: The data being ungrouped. Examples -------- >>> from siuba import _, group_by, ungroup >>> from siuba.data import cars >>> g_cyl = cars.groupby(\"cyl\") >>> res1 = ungroup(g_cyl) >>> res2 = cars >> group_by(_.cyl) >> ungroup() \"\"\" # TODO: can we somehow just restore the original df used to construct # the groupby? if isinstance ( __data , pd . DataFrame ): return __data if isinstance ( __data , pd . Series ): return __data . reset_index () return __data . obj . reset_index ( drop = True )","title":"ungroup()"},{"location":"verb-unite/","text":"unite ( __data , col , * args , * , sep = '_' , remove = True ) Combine multiple columns into a single column. Return DataFrame that column included. Parameters: Name Type Description Default __data a DataFrame required col name of the to-be-created column (string). required *args names of each column to combine. () sep separator joining each column being combined. '_' remove whether to remove the combined columns from the returned DataFrame. True Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def unite ( __data , col , * args , sep = \"_\" , remove = True ): \"\"\"Combine multiple columns into a single column. Return DataFrame that column included. Parameters ---------- __data: a DataFrame col: name of the to-be-created column (string). *args: names of each column to combine. sep: separator joining each column being combined. remove: whether to remove the combined columns from the returned DataFrame. \"\"\" unite_col_names = list ( map ( simple_varname , args )) out_col_name = simple_varname ( col ) # validations ---- if None in unite_col_names : raise ValueError ( \"*args must be string, or simple column name, e.g. _.col_name\" ) missing_cols = set ( unite_col_names ) - set ( __data . columns ) if missing_cols : raise ValueError ( \"columns %s not in DataFrame.columns\" % missing_cols ) unite_cols = [ _coerce_to_str ( __data [ col_name ]) for col_name in unite_col_names ] if out_col_name in __data : raise ValueError ( \"col argument %s already a column in data\" % out_col_name ) # perform unite ---- # TODO: this is probably not very efficient. Maybe try with transform or apply? res = reduce ( lambda x , y : x + sep + y , unite_cols ) out_df = __data . copy () out_df [ out_col_name ] = res if remove : return out_df . drop ( columns = unite_col_names ) return out_df","title":"Verb unite"},{"location":"verb-unite/#siuba.dply.verbs.unite","text":"Combine multiple columns into a single column. Return DataFrame that column included. Parameters: Name Type Description Default __data a DataFrame required col name of the to-be-created column (string). required *args names of each column to combine. () sep separator joining each column being combined. '_' remove whether to remove the combined columns from the returned DataFrame. True Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def unite ( __data , col , * args , sep = \"_\" , remove = True ): \"\"\"Combine multiple columns into a single column. Return DataFrame that column included. Parameters ---------- __data: a DataFrame col: name of the to-be-created column (string). *args: names of each column to combine. sep: separator joining each column being combined. remove: whether to remove the combined columns from the returned DataFrame. \"\"\" unite_col_names = list ( map ( simple_varname , args )) out_col_name = simple_varname ( col ) # validations ---- if None in unite_col_names : raise ValueError ( \"*args must be string, or simple column name, e.g. _.col_name\" ) missing_cols = set ( unite_col_names ) - set ( __data . columns ) if missing_cols : raise ValueError ( \"columns %s not in DataFrame.columns\" % missing_cols ) unite_cols = [ _coerce_to_str ( __data [ col_name ]) for col_name in unite_col_names ] if out_col_name in __data : raise ValueError ( \"col argument %s already a column in data\" % out_col_name ) # perform unite ---- # TODO: this is probably not very efficient. Maybe try with transform or apply? res = reduce ( lambda x , y : x + sep + y , unite_cols ) out_df = __data . copy () out_df [ out_col_name ] = res if remove : return out_df . drop ( columns = unite_col_names ) return out_df","title":"unite()"},{"location":"verb-unnest/","text":"unnest ( __data , key = 'data' ) Unnest a column holding nested data (e.g. Series of lists or DataFrames). Parameters: Name Type Description Default ___data A DataFrame. required key The name of the column to be unnested. 'data' Examples: >>> import pandas as pd >>> df = pd . DataFrame ({ 'id' : [ 1 , 2 ], 'data' : [[ 'a' , 'b' ], [ 'c' ]]}) >>> df >> unnest () id data 0 1 a 1 1 b 2 2 c Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def unnest ( __data , key = \"data\" ): \"\"\"Unnest a column holding nested data (e.g. Series of lists or DataFrames). Parameters ---------- ___data: A DataFrame. key: The name of the column to be unnested. Examples -------- >>> import pandas as pd >>> df = pd.DataFrame({'id': [1,2], 'data': [['a', 'b'], ['c']]}) >>> df >> unnest() id data 0 1 a 1 1 b 2 2 c \"\"\" # TODO: currently only takes key, not expressions nrows_nested = __data [ key ] . apply ( len , convert_dtype = True ) indx_nested = nrows_nested . index . repeat ( nrows_nested ) grp_keys = list ( __data . columns [ __data . columns != key ]) # flatten nested data data_entries = map ( _convert_nested_entry , __data [ key ]) long_data = pd . concat ( data_entries , ignore_index = True ) long_data . name = key # may be a better approach using a multi-index long_grp = __data . loc [ indx_nested , grp_keys ] . reset_index ( drop = True ) return long_grp . join ( long_data )","title":"Verb unnest"},{"location":"verb-unnest/#siuba.dply.verbs.unnest","text":"Unnest a column holding nested data (e.g. Series of lists or DataFrames). Parameters: Name Type Description Default ___data A DataFrame. required key The name of the column to be unnested. 'data' Examples: >>> import pandas as pd >>> df = pd . DataFrame ({ 'id' : [ 1 , 2 ], 'data' : [[ 'a' , 'b' ], [ 'c' ]]}) >>> df >> unnest () id data 0 1 a 1 1 b 2 2 c Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def unnest ( __data , key = \"data\" ): \"\"\"Unnest a column holding nested data (e.g. Series of lists or DataFrames). Parameters ---------- ___data: A DataFrame. key: The name of the column to be unnested. Examples -------- >>> import pandas as pd >>> df = pd.DataFrame({'id': [1,2], 'data': [['a', 'b'], ['c']]}) >>> df >> unnest() id data 0 1 a 1 1 b 2 2 c \"\"\" # TODO: currently only takes key, not expressions nrows_nested = __data [ key ] . apply ( len , convert_dtype = True ) indx_nested = nrows_nested . index . repeat ( nrows_nested ) grp_keys = list ( __data . columns [ __data . columns != key ]) # flatten nested data data_entries = map ( _convert_nested_entry , __data [ key ]) long_data = pd . concat ( data_entries , ignore_index = True ) long_data . name = key # may be a better approach using a multi-index long_grp = __data . loc [ indx_nested , grp_keys ] . reset_index ( drop = True ) return long_grp . join ( long_data )","title":"unnest()"},{"location":"verbs-filter-joins/","text":"anti_join anti_join ( left , right = None , on = None ) Return the left table with every row that would not be kept in an inner join. Parameters: Name Type Description Default left The left-hand table. required right The right-hand table. None on How to match them. By default it uses matches all columns with the same name across the two tables. None Examples: >>> import pandas as pd >>> from siuba import _ , semi_join , anti_join >>> df1 = pd . DataFrame ({ \"id\" : [ 1 , 2 , 3 ], \"x\" : [ \"a\" , \"b\" , \"c\" ]}) >>> df2 = pd . DataFrame ({ \"id\" : [ 2 , 3 , 3 ], \"y\" : [ \"l\" , \"m\" , \"n\" ]}) >>> df1 >> semi_join ( _ , df2 ) id x 1 2 b 2 3 c >>> df1 >> anti_join ( _ , df2 ) id x 0 1 a Generally, it's a good idea to explicitly specify the on argument. >>> df1 >> anti_join ( _ , df2 , on = \"id\" ) id x 0 1 a Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def anti_join ( left , right = None , on = None ): \"\"\"Return the left table with every row that would *not* be kept in an inner join. Parameters ---------- left : The left-hand table. right : The right-hand table. on : How to match them. By default it uses matches all columns with the same name across the two tables. Examples -------- >>> import pandas as pd >>> from siuba import _, semi_join, anti_join >>> df1 = pd.DataFrame({\"id\": [1, 2, 3], \"x\": [\"a\", \"b\", \"c\"]}) >>> df2 = pd.DataFrame({\"id\": [2, 3, 3], \"y\": [\"l\", \"m\", \"n\"]}) >>> df1 >> semi_join(_, df2) id x 1 2 b 2 3 c >>> df1 >> anti_join(_, df2) id x 0 1 a Generally, it's a good idea to explicitly specify the on argument. >>> df1 >> anti_join(_, df2, on=\"id\") id x 0 1 a \"\"\" # copied from semi_join if isinstance ( on , Mapping ): left_on , right_on = zip ( * on . items ()) else : left_on = right_on = on # manually perform merge, up to getting pieces need for indexing merger = _MergeOperation ( left , right , left_on = left_on , right_on = right_on ) _ , l_indx , _ = merger . _get_join_info () # use the left table's indexer to exclude those rows range_indx = pd . RangeIndex ( len ( left )) return left . iloc [ range_indx . difference ( l_indx ),:] semi_join semi_join ( left , right = None , on = None ) Return the left table with every row that would be kept in an inner join. Parameters: Name Type Description Default left The left-hand table. required right The right-hand table. None on How to match them. By default it uses matches all columns with the same name across the two tables. None Examples: >>> import pandas as pd >>> from siuba import _ , semi_join , anti_join >>> df1 = pd . DataFrame ({ \"id\" : [ 1 , 2 , 3 ], \"x\" : [ \"a\" , \"b\" , \"c\" ]}) >>> df2 = pd . DataFrame ({ \"id\" : [ 2 , 3 , 3 ], \"y\" : [ \"l\" , \"m\" , \"n\" ]}) >>> df1 >> semi_join ( _ , df2 ) id x 1 2 b 2 3 c >>> df1 >> anti_join ( _ , df2 ) id x 0 1 a Generally, it's a good idea to explicitly specify the on argument. >>> df1 >> anti_join ( _ , df2 , on = \"id\" ) id x 0 1 a Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def semi_join ( left , right = None , on = None ): \"\"\"Return the left table with every row that would be kept in an inner join. Parameters ---------- left : The left-hand table. right : The right-hand table. on : How to match them. By default it uses matches all columns with the same name across the two tables. Examples -------- >>> import pandas as pd >>> from siuba import _, semi_join, anti_join >>> df1 = pd.DataFrame({\"id\": [1, 2, 3], \"x\": [\"a\", \"b\", \"c\"]}) >>> df2 = pd.DataFrame({\"id\": [2, 3, 3], \"y\": [\"l\", \"m\", \"n\"]}) >>> df1 >> semi_join(_, df2) id x 1 2 b 2 3 c >>> df1 >> anti_join(_, df2) id x 0 1 a Generally, it's a good idea to explicitly specify the on argument. >>> df1 >> anti_join(_, df2, on=\"id\") id x 0 1 a \"\"\" if isinstance ( on , Mapping ): # coerce colnames to list, to avoid indexing with tuples on_cols , right_on = map ( list , zip ( * on . items ())) right = right [ right_on ] . rename ( dict ( zip ( right_on , on_cols ))) elif on is None : warnings . warn ( \"No on column passed to join. \" \"Inferring join columns instead using shared column names.\" ) on_cols = list ( set ( left . columns ) . intersection ( set ( right . columns ))) if not len ( on_cols ): raise Exception ( \"No join column specified, and no shared column names\" ) warnings . warn ( \"Detected shared columns: %s \" % on_cols ) elif isinstance ( on , str ): on_cols = [ on ] else : on_cols = on # get our semi join on ---- if len ( on_cols ) == 1 : col_name = on_cols [ 0 ] indx = left [ col_name ] . isin ( right [ col_name ]) return left . loc [ indx ] # Not a super efficient approach. Effectively, an inner join with what would # be duplicate rows removed. merger = _MergeOperation ( left , right , left_on = on_cols , right_on = on_cols ) _ , l_indx , _ = merger . _get_join_info () range_indx = pd . RangeIndex ( len ( left )) return left . loc [ range_indx . isin ( l_indx )]","title":"filter joins (anti, semi)"},{"location":"verbs-filter-joins/#anti_join","text":"","title":"anti_join"},{"location":"verbs-filter-joins/#siuba.dply.verbs.anti_join","text":"Return the left table with every row that would not be kept in an inner join. Parameters: Name Type Description Default left The left-hand table. required right The right-hand table. None on How to match them. By default it uses matches all columns with the same name across the two tables. None Examples: >>> import pandas as pd >>> from siuba import _ , semi_join , anti_join >>> df1 = pd . DataFrame ({ \"id\" : [ 1 , 2 , 3 ], \"x\" : [ \"a\" , \"b\" , \"c\" ]}) >>> df2 = pd . DataFrame ({ \"id\" : [ 2 , 3 , 3 ], \"y\" : [ \"l\" , \"m\" , \"n\" ]}) >>> df1 >> semi_join ( _ , df2 ) id x 1 2 b 2 3 c >>> df1 >> anti_join ( _ , df2 ) id x 0 1 a Generally, it's a good idea to explicitly specify the on argument. >>> df1 >> anti_join ( _ , df2 , on = \"id\" ) id x 0 1 a Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def anti_join ( left , right = None , on = None ): \"\"\"Return the left table with every row that would *not* be kept in an inner join. Parameters ---------- left : The left-hand table. right : The right-hand table. on : How to match them. By default it uses matches all columns with the same name across the two tables. Examples -------- >>> import pandas as pd >>> from siuba import _, semi_join, anti_join >>> df1 = pd.DataFrame({\"id\": [1, 2, 3], \"x\": [\"a\", \"b\", \"c\"]}) >>> df2 = pd.DataFrame({\"id\": [2, 3, 3], \"y\": [\"l\", \"m\", \"n\"]}) >>> df1 >> semi_join(_, df2) id x 1 2 b 2 3 c >>> df1 >> anti_join(_, df2) id x 0 1 a Generally, it's a good idea to explicitly specify the on argument. >>> df1 >> anti_join(_, df2, on=\"id\") id x 0 1 a \"\"\" # copied from semi_join if isinstance ( on , Mapping ): left_on , right_on = zip ( * on . items ()) else : left_on = right_on = on # manually perform merge, up to getting pieces need for indexing merger = _MergeOperation ( left , right , left_on = left_on , right_on = right_on ) _ , l_indx , _ = merger . _get_join_info () # use the left table's indexer to exclude those rows range_indx = pd . RangeIndex ( len ( left )) return left . iloc [ range_indx . difference ( l_indx ),:]","title":"anti_join()"},{"location":"verbs-filter-joins/#semi_join","text":"","title":"semi_join"},{"location":"verbs-filter-joins/#siuba.dply.verbs.semi_join","text":"Return the left table with every row that would be kept in an inner join. Parameters: Name Type Description Default left The left-hand table. required right The right-hand table. None on How to match them. By default it uses matches all columns with the same name across the two tables. None Examples: >>> import pandas as pd >>> from siuba import _ , semi_join , anti_join >>> df1 = pd . DataFrame ({ \"id\" : [ 1 , 2 , 3 ], \"x\" : [ \"a\" , \"b\" , \"c\" ]}) >>> df2 = pd . DataFrame ({ \"id\" : [ 2 , 3 , 3 ], \"y\" : [ \"l\" , \"m\" , \"n\" ]}) >>> df1 >> semi_join ( _ , df2 ) id x 1 2 b 2 3 c >>> df1 >> anti_join ( _ , df2 ) id x 0 1 a Generally, it's a good idea to explicitly specify the on argument. >>> df1 >> anti_join ( _ , df2 , on = \"id\" ) id x 0 1 a Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def semi_join ( left , right = None , on = None ): \"\"\"Return the left table with every row that would be kept in an inner join. Parameters ---------- left : The left-hand table. right : The right-hand table. on : How to match them. By default it uses matches all columns with the same name across the two tables. Examples -------- >>> import pandas as pd >>> from siuba import _, semi_join, anti_join >>> df1 = pd.DataFrame({\"id\": [1, 2, 3], \"x\": [\"a\", \"b\", \"c\"]}) >>> df2 = pd.DataFrame({\"id\": [2, 3, 3], \"y\": [\"l\", \"m\", \"n\"]}) >>> df1 >> semi_join(_, df2) id x 1 2 b 2 3 c >>> df1 >> anti_join(_, df2) id x 0 1 a Generally, it's a good idea to explicitly specify the on argument. >>> df1 >> anti_join(_, df2, on=\"id\") id x 0 1 a \"\"\" if isinstance ( on , Mapping ): # coerce colnames to list, to avoid indexing with tuples on_cols , right_on = map ( list , zip ( * on . items ())) right = right [ right_on ] . rename ( dict ( zip ( right_on , on_cols ))) elif on is None : warnings . warn ( \"No on column passed to join. \" \"Inferring join columns instead using shared column names.\" ) on_cols = list ( set ( left . columns ) . intersection ( set ( right . columns ))) if not len ( on_cols ): raise Exception ( \"No join column specified, and no shared column names\" ) warnings . warn ( \"Detected shared columns: %s \" % on_cols ) elif isinstance ( on , str ): on_cols = [ on ] else : on_cols = on # get our semi join on ---- if len ( on_cols ) == 1 : col_name = on_cols [ 0 ] indx = left [ col_name ] . isin ( right [ col_name ]) return left . loc [ indx ] # Not a super efficient approach. Effectively, an inner join with what would # be duplicate rows removed. merger = _MergeOperation ( left , right , left_on = on_cols , right_on = on_cols ) _ , l_indx , _ = merger . _get_join_info () range_indx = pd . RangeIndex ( len ( left )) return left . loc [ range_indx . isin ( l_indx )]","title":"semi_join()"},{"location":"verbs-filter/","text":"filter ( __data , * args ) Keep rows where conditions are true. Parameters: Name Type Description Default __data The data being filtered. required *args conditions that must be met to keep a column. () Examples: >>> from siuba import _ , filter >>> from siuba.data import cars Keep rows where cyl is 4 and mpg is less than 25. >>> cars >> filter ( _ . cyl == 4 , _ . mpg < 22 ) cyl mpg hp 20 4 21.5 97 31 4 21.4 109 Use | to represent an OR condition. For example, the code below keeps rows where hp is over 250 or mpg is over 32. >>> cars >> filter (( _ . hp > 300 ) | ( _ . mpg > 32 )) cyl mpg hp 17 4 32.4 66 19 4 33.9 65 30 8 15.0 335 Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def filter ( __data , * args ): \"\"\"Keep rows where conditions are true. Parameters ---------- __data: The data being filtered. *args: conditions that must be met to keep a column. Examples -------- >>> from siuba import _, filter >>> from siuba.data import cars Keep rows where cyl is 4 *and* mpg is less than 25. >>> cars >> filter(_.cyl == 4, _.mpg < 22) cyl mpg hp 20 4 21.5 97 31 4 21.4 109 Use `|` to represent an OR condition. For example, the code below keeps rows where hp is over 250 *or* mpg is over 32. >>> cars >> filter((_.hp > 300) | (_.mpg > 32)) cyl mpg hp 17 4 32.4 66 19 4 33.9 65 30 8 15.0 335 \"\"\" crnt_indx = True for arg in args : crnt_indx &= arg ( __data ) if callable ( arg ) else arg # use loc or iloc to subset, depending on crnt_indx ---- # the main issue here is that loc can't remove all rows using a slice # and iloc can't use a boolean series if isinstance ( crnt_indx , bool ) or isinstance ( crnt_indx , np . bool_ ): # iloc can do slice, but not a bool series result = __data . iloc [ slice ( None ) if crnt_indx else slice ( 0 ),:] else : result = __data . loc [ crnt_indx ,:] return result","title":"Verbs filter"},{"location":"verbs-filter/#siuba.dply.verbs.filter","text":"Keep rows where conditions are true. Parameters: Name Type Description Default __data The data being filtered. required *args conditions that must be met to keep a column. () Examples: >>> from siuba import _ , filter >>> from siuba.data import cars Keep rows where cyl is 4 and mpg is less than 25. >>> cars >> filter ( _ . cyl == 4 , _ . mpg < 22 ) cyl mpg hp 20 4 21.5 97 31 4 21.4 109 Use | to represent an OR condition. For example, the code below keeps rows where hp is over 250 or mpg is over 32. >>> cars >> filter (( _ . hp > 300 ) | ( _ . mpg > 32 )) cyl mpg hp 17 4 32.4 66 19 4 33.9 65 30 8 15.0 335 Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def filter ( __data , * args ): \"\"\"Keep rows where conditions are true. Parameters ---------- __data: The data being filtered. *args: conditions that must be met to keep a column. Examples -------- >>> from siuba import _, filter >>> from siuba.data import cars Keep rows where cyl is 4 *and* mpg is less than 25. >>> cars >> filter(_.cyl == 4, _.mpg < 22) cyl mpg hp 20 4 21.5 97 31 4 21.4 109 Use `|` to represent an OR condition. For example, the code below keeps rows where hp is over 250 *or* mpg is over 32. >>> cars >> filter((_.hp > 300) | (_.mpg > 32)) cyl mpg hp 17 4 32.4 66 19 4 33.9 65 30 8 15.0 335 \"\"\" crnt_indx = True for arg in args : crnt_indx &= arg ( __data ) if callable ( arg ) else arg # use loc or iloc to subset, depending on crnt_indx ---- # the main issue here is that loc can't remove all rows using a slice # and iloc can't use a boolean series if isinstance ( crnt_indx , bool ) or isinstance ( crnt_indx , np . bool_ ): # iloc can do slice, but not a bool series result = __data . iloc [ slice ( None ) if crnt_indx else slice ( 0 ),:] else : result = __data . loc [ crnt_indx ,:] return result","title":"filter()"},{"location":"verbs-joins/","text":"join ( left , right , on = None , how = None , * args , * , by = None , ** kwargs ) Join two tables together, by matching on specified columns. The functions inner_join, left_join, right_join, and full_join are provided as wrappers around join, and are used in the examples. Parameters: Name Type Description Default left The left-hand table. required right The right-hand table. required on How to match them. Note that the keyword \"by\" can also be used for this parameter, in order to support compatibility with dplyr. None how The type of join to perform (inner, full, left, right). None *args Additional postition arguments. Currently not supported. () **kwargs Additional keyword arguments. Currently not supported. {} Examples: >>> from siuba import _ , inner_join , left_join , full_join , right_join >>> from siuba.data import band_members , band_instruments , band_instruments2 >>> band_members name band 0 Mick Stones 1 John Beatles 2 Paul Beatles >>> band_instruments name plays 0 John guitar 1 Paul bass 2 Keith guitar Notice that above, only John and Paul have entries for band instruments. This means that they will be the only two rows in the inner_join result: >>> band_members >> inner_join ( _ , band_instruments ) name band plays 0 John Beatles guitar 1 Paul Beatles bass A left join ensures all original rows of the left hand data are included. >>> band_members >> left_join ( _ , band_instruments ) name band plays 0 Mick Stones NaN 1 John Beatles guitar 2 Paul Beatles bass A full join is similar, but ensures all rows of both data are included. >>> band_members >> full_join ( _ , band_instruments ) name band plays 0 Mick Stones NaN 1 John Beatles guitar 2 Paul Beatles bass 3 Keith NaN guitar You can explicilty specify columns to join on using the \"by\" argument: >>> band_members >> inner_join ( _ , band_instruments , by = \"name\" ) n ... Use a dictionary for the by argument, to match up columns with different names: >>> band_members >> full_join ( _ , band_instruments2 , { \"name\" : \"artist\" }) n ... Joins create a new row for each pair of matches. For example, the value 1 is in two rows on the left, and 2 rows on the right so 4 rows will be created. >>> df1 = pd . DataFrame ({ \"x\" : [ 1 , 1 , 3 ]}) >>> df2 = pd . DataFrame ({ \"x\" : [ 1 , 1 , 2 ], \"y\" : [ \"first\" , \"second\" , \"third\" ]}) >>> df1 >> left_join ( _ , df2 ) x y 0 1 first 1 1 second 2 1 first 3 1 second 4 3 NaN Missing values count as matches to eachother by default: >>> df3 = pd . DataFrame ({ \"x\" : [ 1 , None ], \"y\" : 2 }) >>> df4 = pd . DataFrame ({ \"x\" : [ 1 , None ], \"z\" : 3 }) >>> left_join ( df3 , df4 ) x y z 0 1.0 2 3 1 NaN 2 3 Returns: Type Description pd.DataFrame Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def join ( left , right , on = None , how = None , * args , by = None , ** kwargs ): \"\"\"Join two tables together, by matching on specified columns. The functions inner_join, left_join, right_join, and full_join are provided as wrappers around join, and are used in the examples. Parameters ---------- left : The left-hand table. right : The right-hand table. on : How to match them. Note that the keyword \"by\" can also be used for this parameter, in order to support compatibility with dplyr. how : The type of join to perform (inner, full, left, right). *args: Additional postition arguments. Currently not supported. **kwargs: Additional keyword arguments. Currently not supported. Returns ------- pd.DataFrame Examples -------- >>> from siuba import _, inner_join, left_join, full_join, right_join >>> from siuba.data import band_members, band_instruments, band_instruments2 >>> band_members name band 0 Mick Stones 1 John Beatles 2 Paul Beatles >>> band_instruments name plays 0 John guitar 1 Paul bass 2 Keith guitar Notice that above, only John and Paul have entries for band instruments. This means that they will be the only two rows in the inner_join result: >>> band_members >> inner_join(_, band_instruments) name band plays 0 John Beatles guitar 1 Paul Beatles bass A left join ensures all original rows of the left hand data are included. >>> band_members >> left_join(_, band_instruments) name band plays 0 Mick Stones NaN 1 John Beatles guitar 2 Paul Beatles bass A full join is similar, but ensures all rows of both data are included. >>> band_members >> full_join(_, band_instruments) name band plays 0 Mick Stones NaN 1 John Beatles guitar 2 Paul Beatles bass 3 Keith NaN guitar You can explicilty specify columns to join on using the \"by\" argument: >>> band_members >> inner_join(_, band_instruments, by = \"name\") n... Use a dictionary for the by argument, to match up columns with different names: >>> band_members >> full_join(_, band_instruments2, {\"name\": \"artist\"}) n... Joins create a new row for each pair of matches. For example, the value 1 is in two rows on the left, and 2 rows on the right so 4 rows will be created. >>> df1 = pd.DataFrame({\"x\": [1, 1, 3]}) >>> df2 = pd.DataFrame({\"x\": [1, 1, 2], \"y\": [\"first\", \"second\", \"third\"]}) >>> df1 >> left_join(_, df2) x y 0 1 first 1 1 second 2 1 first 3 1 second 4 3 NaN Missing values count as matches to eachother by default: >>> df3 = pd.DataFrame({\"x\": [1, None], \"y\": 2}) >>> df4 = pd.DataFrame({\"x\": [1, None], \"z\": 3}) >>> left_join(df3, df4) x y z 0 1.0 2 3 1 NaN 2 3 \"\"\" if not isinstance ( right , DataFrame ): raise Exception ( \"right hand table must be a DataFrame\" ) if how is None : raise Exception ( \"Must specify how argument\" ) if len ( args ) or len ( kwargs ): raise NotImplementedError ( \"extra arguments to pandas join not currently supported\" ) if on is None and by is not None : on = by # pandas uses outer, but dplyr uses term full if how == \"full\" : how = \"outer\" if isinstance ( on , Mapping ): left_on , right_on = zip ( * on . items ()) return left . merge ( right , how = how , left_on = left_on , right_on = right_on ) return left . merge ( right , how = how , on = on )","title":"mutate joins (inner, left, full)"},{"location":"verbs-joins/#siuba.dply.verbs.join","text":"Join two tables together, by matching on specified columns. The functions inner_join, left_join, right_join, and full_join are provided as wrappers around join, and are used in the examples. Parameters: Name Type Description Default left The left-hand table. required right The right-hand table. required on How to match them. Note that the keyword \"by\" can also be used for this parameter, in order to support compatibility with dplyr. None how The type of join to perform (inner, full, left, right). None *args Additional postition arguments. Currently not supported. () **kwargs Additional keyword arguments. Currently not supported. {} Examples: >>> from siuba import _ , inner_join , left_join , full_join , right_join >>> from siuba.data import band_members , band_instruments , band_instruments2 >>> band_members name band 0 Mick Stones 1 John Beatles 2 Paul Beatles >>> band_instruments name plays 0 John guitar 1 Paul bass 2 Keith guitar Notice that above, only John and Paul have entries for band instruments. This means that they will be the only two rows in the inner_join result: >>> band_members >> inner_join ( _ , band_instruments ) name band plays 0 John Beatles guitar 1 Paul Beatles bass A left join ensures all original rows of the left hand data are included. >>> band_members >> left_join ( _ , band_instruments ) name band plays 0 Mick Stones NaN 1 John Beatles guitar 2 Paul Beatles bass A full join is similar, but ensures all rows of both data are included. >>> band_members >> full_join ( _ , band_instruments ) name band plays 0 Mick Stones NaN 1 John Beatles guitar 2 Paul Beatles bass 3 Keith NaN guitar You can explicilty specify columns to join on using the \"by\" argument: >>> band_members >> inner_join ( _ , band_instruments , by = \"name\" ) n ... Use a dictionary for the by argument, to match up columns with different names: >>> band_members >> full_join ( _ , band_instruments2 , { \"name\" : \"artist\" }) n ... Joins create a new row for each pair of matches. For example, the value 1 is in two rows on the left, and 2 rows on the right so 4 rows will be created. >>> df1 = pd . DataFrame ({ \"x\" : [ 1 , 1 , 3 ]}) >>> df2 = pd . DataFrame ({ \"x\" : [ 1 , 1 , 2 ], \"y\" : [ \"first\" , \"second\" , \"third\" ]}) >>> df1 >> left_join ( _ , df2 ) x y 0 1 first 1 1 second 2 1 first 3 1 second 4 3 NaN Missing values count as matches to eachother by default: >>> df3 = pd . DataFrame ({ \"x\" : [ 1 , None ], \"y\" : 2 }) >>> df4 = pd . DataFrame ({ \"x\" : [ 1 , None ], \"z\" : 3 }) >>> left_join ( df3 , df4 ) x y z 0 1.0 2 3 1 NaN 2 3 Returns: Type Description pd.DataFrame Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def join ( left , right , on = None , how = None , * args , by = None , ** kwargs ): \"\"\"Join two tables together, by matching on specified columns. The functions inner_join, left_join, right_join, and full_join are provided as wrappers around join, and are used in the examples. Parameters ---------- left : The left-hand table. right : The right-hand table. on : How to match them. Note that the keyword \"by\" can also be used for this parameter, in order to support compatibility with dplyr. how : The type of join to perform (inner, full, left, right). *args: Additional postition arguments. Currently not supported. **kwargs: Additional keyword arguments. Currently not supported. Returns ------- pd.DataFrame Examples -------- >>> from siuba import _, inner_join, left_join, full_join, right_join >>> from siuba.data import band_members, band_instruments, band_instruments2 >>> band_members name band 0 Mick Stones 1 John Beatles 2 Paul Beatles >>> band_instruments name plays 0 John guitar 1 Paul bass 2 Keith guitar Notice that above, only John and Paul have entries for band instruments. This means that they will be the only two rows in the inner_join result: >>> band_members >> inner_join(_, band_instruments) name band plays 0 John Beatles guitar 1 Paul Beatles bass A left join ensures all original rows of the left hand data are included. >>> band_members >> left_join(_, band_instruments) name band plays 0 Mick Stones NaN 1 John Beatles guitar 2 Paul Beatles bass A full join is similar, but ensures all rows of both data are included. >>> band_members >> full_join(_, band_instruments) name band plays 0 Mick Stones NaN 1 John Beatles guitar 2 Paul Beatles bass 3 Keith NaN guitar You can explicilty specify columns to join on using the \"by\" argument: >>> band_members >> inner_join(_, band_instruments, by = \"name\") n... Use a dictionary for the by argument, to match up columns with different names: >>> band_members >> full_join(_, band_instruments2, {\"name\": \"artist\"}) n... Joins create a new row for each pair of matches. For example, the value 1 is in two rows on the left, and 2 rows on the right so 4 rows will be created. >>> df1 = pd.DataFrame({\"x\": [1, 1, 3]}) >>> df2 = pd.DataFrame({\"x\": [1, 1, 2], \"y\": [\"first\", \"second\", \"third\"]}) >>> df1 >> left_join(_, df2) x y 0 1 first 1 1 second 2 1 first 3 1 second 4 3 NaN Missing values count as matches to eachother by default: >>> df3 = pd.DataFrame({\"x\": [1, None], \"y\": 2}) >>> df4 = pd.DataFrame({\"x\": [1, None], \"z\": 3}) >>> left_join(df3, df4) x y z 0 1.0 2 3 1 NaN 2 3 \"\"\" if not isinstance ( right , DataFrame ): raise Exception ( \"right hand table must be a DataFrame\" ) if how is None : raise Exception ( \"Must specify how argument\" ) if len ( args ) or len ( kwargs ): raise NotImplementedError ( \"extra arguments to pandas join not currently supported\" ) if on is None and by is not None : on = by # pandas uses outer, but dplyr uses term full if how == \"full\" : how = \"outer\" if isinstance ( on , Mapping ): left_on , right_on = zip ( * on . items ()) return left . merge ( right , how = how , left_on = left_on , right_on = right_on ) return left . merge ( right , how = how , on = on )","title":"join()"},{"location":"verbs-mutate-transmute/","text":"mutate ( __data , ** kwargs ) Assign new variables to a DataFrame, while keeping existing ones. Parameters: Name Type Description Default __data pd.DataFrame required **kwargs new_col_name=value pairs, where value can be a function taking a singledispatch2 argument for the data being operated on. {} Examples: >>> from siuba import _ , mutate , head >>> from siuba.data import cars >>> cars >> mutate ( cyl2 = _ . cyl * 2 , cyl4 = _ . cyl2 * 2 ) >> head ( 2 ) cyl mpg hp cyl2 cyl4 0 6 21.0 110 12 24 1 6 21.0 110 12 24 Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def mutate ( __data , ** kwargs ): \"\"\"Assign new variables to a DataFrame, while keeping existing ones. Parameters ---------- __data: pd.DataFrame **kwargs: new_col_name=value pairs, where value can be a function taking a singledispatch2 argument for the data being operated on. See Also -------- transmute : Returns a DataFrame with only the newly created columns. Examples -------- >>> from siuba import _, mutate, head >>> from siuba.data import cars >>> cars >> mutate(cyl2 = _.cyl * 2, cyl4 = _.cyl2 * 2) >> head(2) cyl mpg hp cyl2 cyl4 0 6 21.0 110 12 24 1 6 21.0 110 12 24 \"\"\" orig_cols = __data . columns result = __data . assign ( ** kwargs ) new_cols = result . columns [ ~ result . columns . isin ( orig_cols )] return result . loc [:, [ * orig_cols , * new_cols ]] transmute ( __data , * args , ** kwargs ) Assign new columns to a DataFrame, while dropping previous columns. Parameters: Name Type Description Default __data The input data. required **kwargs Each keyword argument is the name of a new column, and an expression. {} Examples: >>> from siuba import _ , transmute , mutate , head >>> from siuba.data import cars Notice that transmute results in a table with only the new column: >>> cars >> transmute ( cyl2 = _ . cyl + 1 ) >> head ( 2 ) cyl2 0 7 1 7 By contrast, mutate adds the new column to the end of the table: >>> cars >> mutate ( cyl2 = _ . cyl + 1 ) >> head ( 2 ) cyl mpg hp cyl2 0 6 21.0 110 7 1 6 21.0 110 7 Source code in siuba/dply/verbs.py @singledispatch2 ( DataFrame ) def transmute ( __data , * args , ** kwargs ): \"\"\"Assign new columns to a DataFrame, while dropping previous columns. Parameters ---------- __data: The input data. **kwargs: Each keyword argument is the name of a new column, and an expression. See Also -------- mutate : Assign new columns, or modify existing ones. Examples -------- >>> from siuba import _, transmute, mutate, head >>> from siuba.data import cars Notice that transmute results in a table with only the new column: >>> cars >> transmute(cyl2 = _.cyl + 1) >> head(2) cyl2 0 7 1 7 By contrast, mutate adds the new column to the end of the table: >>> cars >> mutate(cyl2 = _.cyl + 1) >> head(2) cyl mpg hp cyl2 0 6 21.0 110 7 1 6 21.0 110 7 \"\"\" arg_vars = list ( map ( simple_varname , args )) for ii , name in enumerate ( arg_vars ): if name is None : raise Exception ( \"complex, unnamed expression at pos %s not supported\" % ii ) f_mutate = mutate . registry [ pd . DataFrame ] df = f_mutate ( __data , ** kwargs ) return df [[ * arg_vars , * kwargs . keys ()]]","title":"mutate, transmute"},{"location":"verbs-mutate-transmute/#siuba.dply.verbs.mutate","text":"Assign new variables to a DataFrame, while keeping existing ones. Parameters: Name Type Description Default __data pd.DataFrame required **kwargs new_col_name=value pairs, where value can be a function taking a singledispatch2 argument for the data being operated on. {} Examples: >>> from siuba import _ , mutate , head >>> from siuba.data import cars >>> cars >> mutate ( cyl2 = _ . cyl * 2 , cyl4 = _ . cyl2 * 2 ) >> head ( 2 ) cyl mpg hp cyl2 cyl4 0 6 21.0 110 12 24 1 6 21.0 110 12 24 Source code in siuba/dply/verbs.py @singledispatch2 ( pd . DataFrame ) def mutate ( __data , ** kwargs ): \"\"\"Assign new variables to a DataFrame, while keeping existing ones. Parameters ---------- __data: pd.DataFrame **kwargs: new_col_name=value pairs, where value can be a function taking a singledispatch2 argument for the data being operated on. See Also -------- transmute : Returns a DataFrame with only the newly created columns. Examples -------- >>> from siuba import _, mutate, head >>> from siuba.data import cars >>> cars >> mutate(cyl2 = _.cyl * 2, cyl4 = _.cyl2 * 2) >> head(2) cyl mpg hp cyl2 cyl4 0 6 21.0 110 12 24 1 6 21.0 110 12 24 \"\"\" orig_cols = __data . columns result = __data . assign ( ** kwargs ) new_cols = result . columns [ ~ result . columns . isin ( orig_cols )] return result . loc [:, [ * orig_cols , * new_cols ]]","title":"mutate()"},{"location":"verbs-mutate-transmute/#siuba.dply.verbs.transmute","text":"Assign new columns to a DataFrame, while dropping previous columns. Parameters: Name Type Description Default __data The input data. required **kwargs Each keyword argument is the name of a new column, and an expression. {} Examples: >>> from siuba import _ , transmute , mutate , head >>> from siuba.data import cars Notice that transmute results in a table with only the new column: >>> cars >> transmute ( cyl2 = _ . cyl + 1 ) >> head ( 2 ) cyl2 0 7 1 7 By contrast, mutate adds the new column to the end of the table: >>> cars >> mutate ( cyl2 = _ . cyl + 1 ) >> head ( 2 ) cyl mpg hp cyl2 0 6 21.0 110 7 1 6 21.0 110 7 Source code in siuba/dply/verbs.py @singledispatch2 ( DataFrame ) def transmute ( __data , * args , ** kwargs ): \"\"\"Assign new columns to a DataFrame, while dropping previous columns. Parameters ---------- __data: The input data. **kwargs: Each keyword argument is the name of a new column, and an expression. See Also -------- mutate : Assign new columns, or modify existing ones. Examples -------- >>> from siuba import _, transmute, mutate, head >>> from siuba.data import cars Notice that transmute results in a table with only the new column: >>> cars >> transmute(cyl2 = _.cyl + 1) >> head(2) cyl2 0 7 1 7 By contrast, mutate adds the new column to the end of the table: >>> cars >> mutate(cyl2 = _.cyl + 1) >> head(2) cyl mpg hp cyl2 0 6 21.0 110 7 1 6 21.0 110 7 \"\"\" arg_vars = list ( map ( simple_varname , args )) for ii , name in enumerate ( arg_vars ): if name is None : raise Exception ( \"complex, unnamed expression at pos %s not supported\" % ii ) f_mutate = mutate . registry [ pd . DataFrame ] df = f_mutate ( __data , ** kwargs ) return df [[ * arg_vars , * kwargs . keys ()]]","title":"transmute()"},{"location":"verbs-select/","text":"select ( __data , * args , ** kwargs ) Select columns of a table to keep or drop (and optionally rename). Parameters: Name Type Description Default __data The input table. required *args An expression specifying columns to keep or drop. () **kwargs Not implemented. {} Examples: >>> from siuba import _ , select >>> from siuba.data import cars >>> small_cars = cars . head ( 1 ) >>> small_cars cyl mpg hp 0 6 21.0 110 You can refer to columns by name or position. >>> small_cars >> select ( _ . cyl , _ [ 2 ]) cyl hp 0 6 110 Use a ~ sign to exclude a column. >>> small_cars >> select ( ~ _ . cyl ) mpg hp 0 21.0 110 You can use any methods you'd find on the .columns.str accessor: >>> small_cars . columns . str . contains ( \"p\" ) array ([ False , True , True ]) >>> small_cars >> select ( _ . contains ( \"p\" )) mpg hp 0 21.0 110 Use a slice to select a range of columns: >>> small_cars >> select ( _ [ 0 : 2 ]) cyl mpg 0 6 21.0 Multiple expressions can be combined using _[a, b, c] syntax. This is useful for dropping a complex set of matches. >>> small_cars >> select ( ~ _ [ _ . startswith ( \"c\" ), - 1 ]) mpg 0 21.0 Source code in siuba/dply/verbs.py @singledispatch2 ( DataFrame ) def select ( __data , * args , ** kwargs ): \"\"\"Select columns of a table to keep or drop (and optionally rename). Parameters ---------- __data: The input table. *args: An expression specifying columns to keep or drop. **kwargs: Not implemented. Examples -------- >>> from siuba import _, select >>> from siuba.data import cars >>> small_cars = cars.head(1) >>> small_cars cyl mpg hp 0 6 21.0 110 You can refer to columns by name or position. >>> small_cars >> select(_.cyl, _[2]) cyl hp 0 6 110 Use a `~` sign to exclude a column. >>> small_cars >> select(~_.cyl) mpg hp 0 21.0 110 You can use any methods you'd find on the .columns.str accessor: >>> small_cars.columns.str.contains(\"p\") array([False, True, True]) >>> small_cars >> select(_.contains(\"p\")) mpg hp 0 21.0 110 Use a slice to select a range of columns: >>> small_cars >> select(_[0:2]) cyl mpg 0 6 21.0 Multiple expressions can be combined using _[a, b, c] syntax. This is useful for dropping a complex set of matches. >>> small_cars >> select(~_[_.startswith(\"c\"), -1]) mpg 0 21.0 \"\"\" if kwargs : raise NotImplementedError ( \"Using kwargs in select not currently supported. \" \"Use _.newname == _.oldname instead\" ) var_list = var_create ( * args ) od = var_select ( __data . columns , * var_list ) to_rename = { k : v for k , v in od . items () if v is not None } return __data [ list ( od )] . rename ( columns = to_rename )","title":"Verbs select"},{"location":"verbs-select/#siuba.dply.verbs.select","text":"Select columns of a table to keep or drop (and optionally rename). Parameters: Name Type Description Default __data The input table. required *args An expression specifying columns to keep or drop. () **kwargs Not implemented. {} Examples: >>> from siuba import _ , select >>> from siuba.data import cars >>> small_cars = cars . head ( 1 ) >>> small_cars cyl mpg hp 0 6 21.0 110 You can refer to columns by name or position. >>> small_cars >> select ( _ . cyl , _ [ 2 ]) cyl hp 0 6 110 Use a ~ sign to exclude a column. >>> small_cars >> select ( ~ _ . cyl ) mpg hp 0 21.0 110 You can use any methods you'd find on the .columns.str accessor: >>> small_cars . columns . str . contains ( \"p\" ) array ([ False , True , True ]) >>> small_cars >> select ( _ . contains ( \"p\" )) mpg hp 0 21.0 110 Use a slice to select a range of columns: >>> small_cars >> select ( _ [ 0 : 2 ]) cyl mpg 0 6 21.0 Multiple expressions can be combined using _[a, b, c] syntax. This is useful for dropping a complex set of matches. >>> small_cars >> select ( ~ _ [ _ . startswith ( \"c\" ), - 1 ]) mpg 0 21.0 Source code in siuba/dply/verbs.py @singledispatch2 ( DataFrame ) def select ( __data , * args , ** kwargs ): \"\"\"Select columns of a table to keep or drop (and optionally rename). Parameters ---------- __data: The input table. *args: An expression specifying columns to keep or drop. **kwargs: Not implemented. Examples -------- >>> from siuba import _, select >>> from siuba.data import cars >>> small_cars = cars.head(1) >>> small_cars cyl mpg hp 0 6 21.0 110 You can refer to columns by name or position. >>> small_cars >> select(_.cyl, _[2]) cyl hp 0 6 110 Use a `~` sign to exclude a column. >>> small_cars >> select(~_.cyl) mpg hp 0 21.0 110 You can use any methods you'd find on the .columns.str accessor: >>> small_cars.columns.str.contains(\"p\") array([False, True, True]) >>> small_cars >> select(_.contains(\"p\")) mpg hp 0 21.0 110 Use a slice to select a range of columns: >>> small_cars >> select(_[0:2]) cyl mpg 0 6 21.0 Multiple expressions can be combined using _[a, b, c] syntax. This is useful for dropping a complex set of matches. >>> small_cars >> select(~_[_.startswith(\"c\"), -1]) mpg 0 21.0 \"\"\" if kwargs : raise NotImplementedError ( \"Using kwargs in select not currently supported. \" \"Use _.newname == _.oldname instead\" ) var_list = var_create ( * args ) od = var_select ( __data . columns , * var_list ) to_rename = { k : v for k , v in od . items () if v is not None } return __data [ list ( od )] . rename ( columns = to_rename )","title":"select()"}]}